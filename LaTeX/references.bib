@article{babaeva_cnn,
    title={Optimal hyperparameter tuning of convolutional neural networks based on the parameter-setting-free harmony search algorithm},
    author={Lee, Woo-Young and Park, Seung-Min and Sim, Kwee-Bo},
    journal={Optik},
    volume={172},
    pages={359--367},
    year={2018},
    publisher={Elsevier}
}

@inproceedings{Dhar_2023,
   title={Differential Evolution Algorithm Based Hyper-Parameters Selection of Convolutional Neural Network for Speech Command Recognition},
   url={http://dx.doi.org/10.5220/0012251500003595},
   DOI={10.5220/0012251500003595},
   booktitle={Proceedings of the 15th International Joint Conference on Computational Intelligence},
   publisher={SCITEPRESS - Science and Technology Publications},
   author={Dhar, Sandipan and Sen, Anuvab and Bandyopadhyay, Aritra and Jana, Nanda and Ghosh, Arjun and Sarayloo, Zahra},
   year={2023},
   pages={315–322}
}

@inproceedings{2013SHADE,
  author    = {Tanabe, Ryoji and Fukunaga, Alex},
  title     = {Success-history based parameter adaptation for Differential Evolution},
  booktitle = {2013 IEEE Congress on Evolutionary Computation},
  year      = {2013},
  pages     = {71--78},
  publisher = {IEEE},
  doi       = {10.1109/CEC.2013.6557555}
}
@inproceedings{2014SHADE,
  author    = {Tanabe, Ryoji and Fukunaga, Alex S.},
  title     = {Improving the search performance of {SHADE} using linear population size reduction},
  booktitle = {2014 IEEE Congress on Evolutionary Computation (CEC)},
  year      = {2014},
  pages     = {1658--1665},
  publisher = {IEEE},
  doi       = {10.1109/CEC.2014.6900380}
}

@article{sciencedirect_emotion,
title = {Pre-trained models: Past, present and future},
journal = {AI Open},
volume = {2},
pages = {225-250},
year = {2021},
issn = {2666-6510},
doi = {https://doi.org/10.1016/j.aiopen.2021.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S2666651021000231},
author = {Xu Han and Zhengyan Zhang and Ning Ding and Yuxian Gu and Xiao Liu and Yuqi Huo and Jiezhong Qiu and Yuan Yao and Ao Zhang and Liang Zhang and Wentao Han and Minlie Huang and Qin Jin and Yanyan Lan and Yang Liu and Zhiyuan Liu and Zhiwu Lu and Xipeng Qiu and Ruihua Song and Jie Tang and Ji-Rong Wen and Jinhui Yuan and Wayne Xin Zhao and Jun Zhu},
keywords = {Pre-trained models, Language models, Transfer learning, Self-supervised learning, Natural language processing, Multimodal processing, Artificial intelligence},
abstract = {Large-scale pre-trained models (PTMs) such as BERT and GPT have recently achieved great success and become a milestone in the field of artificial intelligence (AI). Owing to sophisticated pre-training objectives and huge model parameters, large-scale PTMs can effectively capture knowledge from massive labeled and unlabeled data. By storing knowledge into huge parameters and fine-tuning on specific tasks, the rich knowledge implicitly encoded in huge parameters can benefit a variety of downstream tasks, which has been extensively demonstrated via experimental verification and empirical analysis. It is now the consensus of the AI community to adopt PTMs as backbone for downstream tasks rather than learning models from scratch. In this paper, we take a deep look into the history of pre-training, especially its special relation with transfer learning and self-supervised learning, to reveal the crucial position of PTMs in the AI development spectrum. Further, we comprehensively review the latest breakthroughs of PTMs. These breakthroughs are driven by the surge of computational power and the increasing availability of data, towards four important directions: designing effective architectures, utilizing rich contexts, improving computational efficiency, and conducting interpretation and theoretical analysis. Finally, we discuss a series of open problems and research directions of PTMs, and hope our view can inspire and advance the future study of PTMs.}
}

@misc{ibm_watson,
  title = {Low-rank adaptation ({LoRA}) fine tuning},
  author = {{IBM}},
  year = {2025},
  url = {https://www.ibm.com/docs/en/watsonx/w-and-w/2.1.0?topic=tuning-lora-fine},
  note = {Accessed: 2025-12-01}
}

@misc{lora_xs,
      title={LoRA-XS: Low-Rank Adaptation with Extremely Small Number of Parameters}, 
      author={Klaudia Bałazy and Mohammadreza Banaei and Karl Aberer and Jacek Tabor},
      year={2025},
      eprint={2405.17604},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.17604},
}

@misc{gpt3,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.14165}, 
}

@misc{adapter_tuning,
      title={Parameter-Efficient Transfer Learning for NLP}, 
      author={Neil Houlsby and Andrei Giurgiu and Stanislaw Jastrzebski and Bruna Morrone and Quentin de Laroussilhe and Andrea Gesmundo and Mona Attariyan and Sylvain Gelly},
      year={2019},
      eprint={1902.00751},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1902.00751}, 
}

@misc{bitfit,
      title={BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models}, 
      author={Elad Ben Zaken and Shauli Ravfogel and Yoav Goldberg},
      year={2022},
      eprint={2106.10199},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2106.10199}, 
}

@misc{lora_paper,
      title={LoRA: Low-Rank Adaptation of Large Language Models}, 
      author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
      year={2021},
      eprint={2106.09685},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2106.09685}, 
}

@Inbook{curse_dim,
    author="Keogh, Eamonn
    and Mueen, Abdullah",
    editor="Sammut, Claude
    and Webb, Geoffrey I.",
    title="Curse of Dimensionality",
    bookTitle="Encyclopedia of Machine Learning and Data Mining",
    year="2017",
    publisher="Springer US",
    address="Boston, MA",
    pages="314--315",
    isbn="978-1-4899-7687-1",
    doi="10.1007/978-1-4899-7687-1_192",
    url="https://doi.org/10.1007/978-1-4899-7687-1_192"
}

@book{holland_ga,
  title={Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control, and Artificial Intelligence},
  author={Holland, John H.},
  year={1992},
  publisher={The MIT Press},
  address={Cambridge, MA, USA}
}

@article{das_de,
  title={Differential evolution: A survey of the state-of-the-art},
  author={Das, Swagatam and Suganthan, Ponnuthurai Nagaratnam},
  journal={IEEE Transactions on Evolutionary Computation},
  volume={15},
  number={1},
  pages={4--31},
  year={2011},
  publisher={IEEE}
}

@misc{bayes_opt,
      title={Practical Bayesian Optimization of Machine Learning Algorithms}, 
      author={Jasper Snoek and Hugo Larochelle and Ryan P. Adams},
      year={2012},
      eprint={1206.2944},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1206.2944},
}

@article{distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.01108}
}

@ARTICLE{nsga2,
  author={Deb, K. and Pratap, A. and Agarwal, S. and Meyarivan, T.},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={A fast and elitist multiobjective genetic algorithm: NSGA-II}, 
  year={2002},
  volume={6},
  number={2},
  pages={182-197},
  keywords={Genetic algorithms;Sorting;Computational complexity;Evolutionary computation;Computational modeling;Testing;Decision making;Associate members;Diversity reception;Constraint optimization},
  doi={10.1109/4235.996017}
}

@misc{pareto_front,
  author       = {Cenaero},
  title        = {Page Title},
  howpublished = {Cenaero.be (via Internet Archive)},
  year         = {2020},
  month        = feb,
  day          = {26},
  note         = {[Online]. Available: \url{https://web.archive.org/web/20200226003108/https://www.cenaero.be/Page.asp?docid=27103}}
}

@article{spea2,
author = {Zitzler, Eckart and Laumanns, Marco and Thiele, Lothar},
year = {2001},
month = {07},
pages = {},
title = {SPEA2: Improving the Strength Pareto Evolutionary Algorithm},
volume = {103},
journal = {TIK-Report}
}

@article{wang_pso,
  title={Particle swarm optimization algorithm: an overview},
  author={Wang, Dong and Tan, Dapei and Liu, Lei},
  journal={Soft Computing},
  volume={22},
  number={2},
  pages={387--408},
  year={2018},
  url={https://doi.org/10.1007/s00500-016-2474-6},
  publisher={Springer}
}