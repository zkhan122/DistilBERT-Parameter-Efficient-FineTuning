{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"widgets":{"application/vnd.jupyter.widget-state+json":{"01e799f84bc3439db6cf304f25702b0b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"063b64c4d1e7438287388b6a07992e7d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"06af3150a7314a259d247dda4b878f70":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2afb929be5f443dbaabfe39bdb71c74f","max":127466,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2467ba26ff114a718b96306269420308","value":127466}},"06ed37e0be254c0189af075e6e943c0e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"07a2d020c1b24bfb84a9505034b11c97":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_77efa2266ac64ccbac74f9072f0f6628","max":1030740,"min":0,"orientation":"horizontal","style":"IPY_MODEL_063b64c4d1e7438287388b6a07992e7d","value":1030740}},"07b52e5be6f74c09a305e4e17f403f24":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6e65f4b01200429bbe8c010d8b267015","IPY_MODEL_3880332f4227484fa6421611d48ef719","IPY_MODEL_445bb174bfc44c6db6256aba46364967"],"layout":"IPY_MODEL_b1606dad0963400cb8c0473e59c9237a"}},"07bc19ab1dfe4247bfa380903a78cb80":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0814abbe10e342f3bed435e821e8e1d1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"08709503aef84033be1ed38a52c5e699":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a1d8f71c066e4071ba3e1108edb5d30b","max":2000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_37f4360caa0f4f5388eb03c693e70b6a","value":2000}},"0898ea20ddf946acbc754ca6decbaf0c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b993e16b7504a7b9e1a67352c1d987f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0cb0e55cc2dd4d1591715fa1e92d9eeb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0e74182d4a204fefbe73affba0cc43ed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0eb6358df9224c41b98890b8ef12d179":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b059ae6e0f2b4a57913df840b5963267","IPY_MODEL_db73a479e8814d03b51e0d1ab767bb87","IPY_MODEL_33298c79df194c97b2fee5315781881d"],"layout":"IPY_MODEL_96e2f4cbb1104355a29813f7b7757bd9"}},"12028dbde537431990be9d20233fe8c8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1230770bfb384fb3b32f9bcf94b8e09e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_96a20b625667406a8143481c708f3100","IPY_MODEL_28441b71a49d4dd6b587021b6e664ab0","IPY_MODEL_2d2cbb1fec3d4ce29a8b47106b1ba49a"],"layout":"IPY_MODEL_6fbb3f23036646d284b48f7ce5c085f7"}},"12b4d4ffc36545c1a07855ea78c28395":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"12dd060dfe67444182954afb162233c6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7db60b3d04ed4302be036431f8492d6f","placeholder":"​","style":"IPY_MODEL_53cb5d415a514317890bd667ae8252e0","value":" 3000/3000 [00:00&lt;00:00, 4072.40 examples/s]"}},"158fcc41bcb842abbd0319c8a12afcea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9a5a93cd9a645a0837ec5cd5643b013","placeholder":"​","style":"IPY_MODEL_6e385ddfcdea4beb893f6d2e460964bf","value":"Generating train split: 100%"}},"159eb7b4b5f044449a6200b2f3d705ab":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16c30a0fbbae409db96fedfe9f003cd0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"179b05cba9e64bb4bcad2d27400b0328":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b8860dbbce04406be1519ba32faa845":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1c5f32f73fc9448183d4ea79d0eb57ea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_996b49e8d8c34a6fbee52f7b5879ab93","placeholder":"​","style":"IPY_MODEL_415b1a367225474aa637e888bd69cd97","value":" 268M/268M [00:02&lt;00:00, 160MB/s]"}},"1c6f0e5be2514066a7e89129b220872d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e883cdefc0a240b68d1dcf5b2c599293","placeholder":"​","style":"IPY_MODEL_0814abbe10e342f3bed435e821e8e1d1","value":"tokenizer.json: 100%"}},"1cdb7122235a4c778c063657381c5d74":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2467ba26ff114a718b96306269420308":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"24dbd724412445178395ec466526f43a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8560bee85b904b1d8f598f61457370c8","placeholder":"​","style":"IPY_MODEL_5d364ac20ca5477c846f45bed79c45b9","value":" 232k/232k [00:00&lt;00:00, 1.88MB/s]"}},"28034f6e8ae345c3a0d37354a74addd1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2816733a86a24a42abcd0069dfc04a45":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28441b71a49d4dd6b587021b6e664ab0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e844fceae052449d97b63b39ed2b9a58","max":483,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0cb0e55cc2dd4d1591715fa1e92d9eeb","value":483}},"297c57771105417fb267192c398e6228":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_582af8bfa8f64bd1ac2826729c10adcc","placeholder":"​","style":"IPY_MODEL_81db8ca32a57451b825a6c0c1b9eb732","value":"vocab.txt: 100%"}},"29b7d157f3f245eca4791f37a2fbc873":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a2bc9c7374b427aa36809e1b4b2e2c5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2afb929be5f443dbaabfe39bdb71c74f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b8c007f616d4a30bca8b0c46f32235c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_158fcc41bcb842abbd0319c8a12afcea","IPY_MODEL_3ec2cecfbc724850b37975410e9698b3","IPY_MODEL_4d573bc4d84749839a8ffb56c5f21ca5"],"layout":"IPY_MODEL_c2e4eaabb8234e84a179ed755f0dcb79"}},"2d2cbb1fec3d4ce29a8b47106b1ba49a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e913b671df6492383acefc1acbf5ba2","placeholder":"​","style":"IPY_MODEL_07bc19ab1dfe4247bfa380903a78cb80","value":" 483/483 [00:00&lt;00:00, 25.7kB/s]"}},"324291ee005648ceb01e72bd0533c404":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"33298c79df194c97b2fee5315781881d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_88e5a73c6a7b4362be1fc8da7011b430","placeholder":"​","style":"IPY_MODEL_c561d5fab6904adb9db1d92e3f8baa73","value":" 2000/2000 [00:00&lt;00:00, 92745.09 examples/s]"}},"34ec4963ffac4bc8b2d18dc084c9e98f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37f4360caa0f4f5388eb03c693e70b6a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3880332f4227484fa6421611d48ef719":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a7b298d3caf34dceab2931479b83c2ca","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_47b3b3394e614bf39d825d54d398d26d","value":48}},"3a94252119b14f31adfdc54f92067933":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b3febbecfad420db7dd08bca8e05e3a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9dbdeee9f7c44314b1579b7d48ebab16","placeholder":"​","style":"IPY_MODEL_0e74182d4a204fefbe73affba0cc43ed","value":"Downloading builder script: "}},"3d12a9209ea14b2694063e0ff8a4663b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3ec2cecfbc724850b37975410e9698b3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c8ad2bd72a2b4030a47d2acebf67f4e7","max":16000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7c92f3e1bb0f47d993da0be07cda85a0","value":16000}},"3fbda3319fe34005a5251eb90567d96b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed95141e71b648eda10476c71bf90e08","placeholder":"​","style":"IPY_MODEL_3d12a9209ea14b2694063e0ff8a4663b","value":"Map: 100%"}},"4032ae6a852745ec9532288f13ea3652":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cbc4693292bc460a81dfb36ab29227a2","IPY_MODEL_67ece0d9acf5466a9d040fe6c92197b2","IPY_MODEL_e46cf2250ea74824b9507a35fdea8686"],"layout":"IPY_MODEL_b2c0760361b0421da2f9e11927f09961"}},"415b1a367225474aa637e888bd69cd97":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"41ea4b64b50d4265a931732e773f1b0d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_65f39a6db5fb449283971d90852d91ec","placeholder":"​","style":"IPY_MODEL_54b11055fb6a49f987354949c1a8cf46","value":" 9.05k/? [00:00&lt;00:00, 339kB/s]"}},"445bb174bfc44c6db6256aba46364967":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c844aa6328cc44f9b5d80a265c390abf","placeholder":"​","style":"IPY_MODEL_77adb596b0e8486b80f674a2b0ae3998","value":" 48.0/48.0 [00:00&lt;00:00, 3.20kB/s]"}},"454f27f6c02f41928434c6073e2739ef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_179b05cba9e64bb4bcad2d27400b0328","placeholder":"​","style":"IPY_MODEL_1b8860dbbce04406be1519ba32faa845","value":" 1.03M/1.03M [00:01&lt;00:00, 904kB/s]"}},"4553ff3aa4c94601b5bd5557bb18012b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc4d64e7a7084064a1ed92dcb3b80b38","placeholder":"​","style":"IPY_MODEL_dd7eafcdef634698a63dccf812ebe293","value":"model.safetensors: 100%"}},"47b3b3394e614bf39d825d54d398d26d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4aefd248c1f04d1fa5114e4a658e66fe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c4553e8d6d174ec199da4f40ca6c6213","placeholder":"​","style":"IPY_MODEL_d53150fb5b594fedb37d0e7cfa98f4b1","value":" 2000/2000 [00:00&lt;00:00, 3853.86 examples/s]"}},"4d573bc4d84749839a8ffb56c5f21ca5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_34ec4963ffac4bc8b2d18dc084c9e98f","placeholder":"​","style":"IPY_MODEL_818a17048f394809aace5d7000887752","value":" 16000/16000 [00:00&lt;00:00, 160750.19 examples/s]"}},"4d998081ed04452c91ca7cf72034aefd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c26dad5f2484e8cae8d02c5a137a3e9","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_867cf3f6144e496b89748bc2bd7b074b","value":1}},"50a5a0d019534a779d8228e477efb919":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51f2bd3d567f45dba347df0879656300":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_297c57771105417fb267192c398e6228","IPY_MODEL_82d3fe5bec4a4dd68372339324eb95eb","IPY_MODEL_24dbd724412445178395ec466526f43a"],"layout":"IPY_MODEL_2816733a86a24a42abcd0069dfc04a45"}},"53cb5d415a514317890bd667ae8252e0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"54b11055fb6a49f987354949c1a8cf46":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"553f67fcb6044edf8365b6e773baf8a3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"582af8bfa8f64bd1ac2826729c10adcc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5886ae8327044265ba840f04ca93064e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5928e138001749e8a1bf2846cc35c1bc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5d364ac20ca5477c846f45bed79c45b9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5e29fdf7446444d4b1f850c077be3d53":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_16c30a0fbbae409db96fedfe9f003cd0","placeholder":"​","style":"IPY_MODEL_a29cbb282062498699c6e70101c0aa40","value":"split/train-00000-of-00001.parquet: 100%"}},"5f56c1ed9b2142ac99fe9c93cf7dfc62":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e9d9c3a693964d9bb6f1572bc03c770a","placeholder":"​","style":"IPY_MODEL_7bd44f1a972a4e858abdd2181e6d7142","value":"README.md: "}},"60b04b567e2b49e8bc94e87eb6af67a5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3fbda3319fe34005a5251eb90567d96b","IPY_MODEL_08709503aef84033be1ed38a52c5e699","IPY_MODEL_4aefd248c1f04d1fa5114e4a658e66fe"],"layout":"IPY_MODEL_01e799f84bc3439db6cf304f25702b0b"}},"635b42b4c7964febb4a664cc1e0c012c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6ff67757e95d4a6e8a7c2835c1add4d1","IPY_MODEL_cd3e423a32a242c5b33b5ccf13e69567","IPY_MODEL_12dd060dfe67444182954afb162233c6"],"layout":"IPY_MODEL_c81b239d3a3e4640b3de9729bd398c9b"}},"647299c5e28846ea9ff20b417b26ef67":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65e6952fd45e4c4f9d688a2e51973b0a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_91a47c5a2d8542febf579b3f11fe4b9a","max":2000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c89f3099e5ba473e8dd9678678c1f45c","value":2000}},"65f39a6db5fb449283971d90852d91ec":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67ece0d9acf5466a9d040fe6c92197b2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_12028dbde537431990be9d20233fe8c8","max":128987,"min":0,"orientation":"horizontal","style":"IPY_MODEL_902747af7bf640bc9a312b4252a469fa","value":128987}},"689168d3b6e540aba2df17e2b7d833dc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e385ddfcdea4beb893f6d2e460964bf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6e65f4b01200429bbe8c010d8b267015":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a94252119b14f31adfdc54f92067933","placeholder":"​","style":"IPY_MODEL_8c50d564862043bcbbfb2ff92b876c14","value":"tokenizer_config.json: 100%"}},"6f6ab1c34c014e64914ce51118952459":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_647299c5e28846ea9ff20b417b26ef67","placeholder":"​","style":"IPY_MODEL_96bfe5d6c1c7483689e9144d5d2509db","value":" 4.20k/? [00:00&lt;00:00, 157kB/s]"}},"6fbb3f23036646d284b48f7ce5c085f7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ff67757e95d4a6e8a7c2835c1add4d1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_689168d3b6e540aba2df17e2b7d833dc","placeholder":"​","style":"IPY_MODEL_ce191357312f4804b51cc602d086894d","value":"Map: 100%"}},"728fbe0c1c694361a9a3f206916fc94c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"76c72810e7124c1c8c48cd9df89a972b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_945ee263eb264b67971c754c344672f6","placeholder":"​","style":"IPY_MODEL_9af8b6cd14a8485d9852d78d14000a1d","value":"split/validation-00000-of-00001.parquet: 100%"}},"77adb596b0e8486b80f674a2b0ae3998":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"77efa2266ac64ccbac74f9072f0f6628":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7bd44f1a972a4e858abdd2181e6d7142":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7c92f3e1bb0f47d993da0be07cda85a0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7d9fd1fe88e24eb3815237713cc07f5b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7db60b3d04ed4302be036431f8492d6f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e6b457062bf43e0b6184ae09ae1f2fe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d72c1bf3cef44abe81ea30aa88ef7cbb","max":2000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fa675313c2cc4898801c46339ff8eda6","value":2000}},"7e913b671df6492383acefc1acbf5ba2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f8f790bded14024b1988c06de897614":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d773e854a9ba4af4ac2a6d15346fa5f5","placeholder":"​","style":"IPY_MODEL_82a78cbcedde447ea2022aeaddb5bc28","value":"Generating test split: 100%"}},"818a17048f394809aace5d7000887752":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"81db8ca32a57451b825a6c0c1b9eb732":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"828208fba310434ea6156500d98c47d2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"82a78cbcedde447ea2022aeaddb5bc28":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"82d3fe5bec4a4dd68372339324eb95eb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2a2bc9c7374b427aa36809e1b4b2e2c5","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f5e6a8d8a1a84a14ac645e2f985b0c3a","value":231508}},"8560bee85b904b1d8f598f61457370c8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"867cf3f6144e496b89748bc2bd7b074b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"88e5a73c6a7b4362be1fc8da7011b430":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ac8343898934417b6c18290f940e556":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4553ff3aa4c94601b5bd5557bb18012b","IPY_MODEL_bce2dca539a34582a7164f2de8221fe4","IPY_MODEL_1c5f32f73fc9448183d4ea79d0eb57ea"],"layout":"IPY_MODEL_c167c6c9d4044c94838123e21d36d854"}},"8c50d564862043bcbbfb2ff92b876c14":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"902747af7bf640bc9a312b4252a469fa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"91a47c5a2d8542febf579b3f11fe4b9a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"945ee263eb264b67971c754c344672f6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"96a20b625667406a8143481c708f3100":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c7e17ce4e57a496ab60856daf362adc3","placeholder":"​","style":"IPY_MODEL_06ed37e0be254c0189af075e6e943c0e","value":"config.json: 100%"}},"96bfe5d6c1c7483689e9144d5d2509db":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"96e2f4cbb1104355a29813f7b7757bd9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9785e6f541874963bd668717cfcc64b7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e5030fbf9fbd4266a3697bdc5e13425b","placeholder":"​","style":"IPY_MODEL_553f67fcb6044edf8365b6e773baf8a3","value":" 2000/2000 [00:00&lt;00:00, 3579.80 examples/s]"}},"996b49e8d8c34a6fbee52f7b5879ab93":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99fc506a10cb4c20bd57a5af81075af0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9af8b6cd14a8485d9852d78d14000a1d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9c26dad5f2484e8cae8d02c5a137a3e9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"9dbdeee9f7c44314b1579b7d48ebab16":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1d8f71c066e4071ba3e1108edb5d30b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a29cbb282062498699c6e70101c0aa40":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a67ac0e01d754329b14494b0a0d66c68":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_eb07045c3f1f45eb9e2af6e415e00e4c","IPY_MODEL_7e6b457062bf43e0b6184ae09ae1f2fe","IPY_MODEL_9785e6f541874963bd668717cfcc64b7"],"layout":"IPY_MODEL_e1a2b74d86c04f02ab8151811c9cafec"}},"a7419b777291448fa661386430065915":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7b298d3caf34dceab2931479b83c2ca":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad6ab71f36294ef2b05beeb2d8944726":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b059ae6e0f2b4a57913df840b5963267":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b83e1b9363984881a8a40aa176cb02b9","placeholder":"​","style":"IPY_MODEL_d0031feb222641a38b258f5797d9c300","value":"Generating validation split: 100%"}},"b1606dad0963400cb8c0473e59c9237a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2c0760361b0421da2f9e11927f09961":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b6a72d9f634b4da181eba2864317c29b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7f8f790bded14024b1988c06de897614","IPY_MODEL_65e6952fd45e4c4f9d688a2e51973b0a","IPY_MODEL_d92e500243cb4ecbbc2914816231f62e"],"layout":"IPY_MODEL_29b7d157f3f245eca4791f37a2fbc873"}},"b831373e115c4d218a8575d96110632e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b83e1b9363984881a8a40aa176cb02b9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9aaea30007445d1bb9130c697dc9206":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5f56c1ed9b2142ac99fe9c93cf7dfc62","IPY_MODEL_4d998081ed04452c91ca7cf72034aefd","IPY_MODEL_41ea4b64b50d4265a931732e773f1b0d"],"layout":"IPY_MODEL_a7419b777291448fa661386430065915"}},"ba75544ff26143b7aa0d06009b40eac9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bce2dca539a34582a7164f2de8221fe4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e53372319eb14ca4aa20034bcc724fa4","max":267954768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d999860389b74db59ba4a5f902ab7653","value":267954768}},"bf4e5535dc19423fb46e54c6db1bc2e9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1c6f0e5be2514066a7e89129b220872d","IPY_MODEL_cfad5f4486a241cf9b33f788cfcc85b4","IPY_MODEL_e7466c056232410b8491fc481e10d5a5"],"layout":"IPY_MODEL_c0a25c1bd6fb4a37ad04ddd47878b0fb"}},"c0a25c1bd6fb4a37ad04ddd47878b0fb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c167c6c9d4044c94838123e21d36d854":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2e4eaabb8234e84a179ed755f0dcb79":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4553e8d6d174ec199da4f40ca6c6213":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c561d5fab6904adb9db1d92e3f8baa73":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c76619126ff14ba3b42a4dc0b38976e9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3b3febbecfad420db7dd08bca8e05e3a","IPY_MODEL_ed3152d235ec4d4b974ee394d17c73d0","IPY_MODEL_6f6ab1c34c014e64914ce51118952459"],"layout":"IPY_MODEL_1cdb7122235a4c778c063657381c5d74"}},"c7e17ce4e57a496ab60856daf362adc3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c81b239d3a3e4640b3de9729bd398c9b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c844aa6328cc44f9b5d80a265c390abf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c89f3099e5ba473e8dd9678678c1f45c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c8ad2bd72a2b4030a47d2acebf67f4e7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cbc4693292bc460a81dfb36ab29227a2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_12b4d4ffc36545c1a07855ea78c28395","placeholder":"​","style":"IPY_MODEL_324291ee005648ceb01e72bd0533c404","value":"split/test-00000-of-00001.parquet: 100%"}},"cc2f29133a8d4924a27d0ef798831e9c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"cc4d64e7a7084064a1ed92dcb3b80b38":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd3e423a32a242c5b33b5ccf13e69567":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ad6ab71f36294ef2b05beeb2d8944726","max":3000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_728fbe0c1c694361a9a3f206916fc94c","value":3000}},"ce191357312f4804b51cc602d086894d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cfad5f4486a241cf9b33f788cfcc85b4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_159eb7b4b5f044449a6200b2f3d705ab","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7d9fd1fe88e24eb3815237713cc07f5b","value":466062}},"cff260a27080450e956dd5a025aa7ff7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0031feb222641a38b258f5797d9c300":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d11bf26044134d699ab7d4bd6e05f888":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_99fc506a10cb4c20bd57a5af81075af0","placeholder":"​","style":"IPY_MODEL_df7a3babb5704b13b089f90585a5cdc5","value":" 127k/127k [00:01&lt;00:00, 80.5kB/s]"}},"d2cf5de4bba54b9cb2d286b7b7843cd5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d53150fb5b594fedb37d0e7cfa98f4b1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d72c1bf3cef44abe81ea30aa88ef7cbb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d773e854a9ba4af4ac2a6d15346fa5f5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d90ac0ef69f342439fe7fa7e929d7164":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d92e500243cb4ecbbc2914816231f62e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d2cf5de4bba54b9cb2d286b7b7843cd5","placeholder":"​","style":"IPY_MODEL_828208fba310434ea6156500d98c47d2","value":" 2000/2000 [00:00&lt;00:00, 83544.38 examples/s]"}},"d999860389b74db59ba4a5f902ab7653":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"db73a479e8814d03b51e0d1ab767bb87":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cff260a27080450e956dd5a025aa7ff7","max":2000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d90ac0ef69f342439fe7fa7e929d7164","value":2000}},"dd7eafcdef634698a63dccf812ebe293":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"df7a3babb5704b13b089f90585a5cdc5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e1a2b74d86c04f02ab8151811c9cafec":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e46cf2250ea74824b9507a35fdea8686":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0898ea20ddf946acbc754ca6decbaf0c","placeholder":"​","style":"IPY_MODEL_5928e138001749e8a1bf2846cc35c1bc","value":" 129k/129k [00:00&lt;00:00, 476kB/s]"}},"e4cd61e0491c4388aaebef029432315f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5e29fdf7446444d4b1f850c077be3d53","IPY_MODEL_07a2d020c1b24bfb84a9505034b11c97","IPY_MODEL_454f27f6c02f41928434c6073e2739ef"],"layout":"IPY_MODEL_b831373e115c4d218a8575d96110632e"}},"e5030fbf9fbd4266a3697bdc5e13425b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e53372319eb14ca4aa20034bcc724fa4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7466c056232410b8491fc481e10d5a5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_50a5a0d019534a779d8228e477efb919","placeholder":"​","style":"IPY_MODEL_ed97f5c92bf240ef8dd23d65ad16fd87","value":" 466k/466k [00:00&lt;00:00, 7.30MB/s]"}},"e844fceae052449d97b63b39ed2b9a58":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e883cdefc0a240b68d1dcf5b2c599293":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9d9c3a693964d9bb6f1572bc03c770a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb07045c3f1f45eb9e2af6e415e00e4c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba75544ff26143b7aa0d06009b40eac9","placeholder":"​","style":"IPY_MODEL_5886ae8327044265ba840f04ca93064e","value":"Map: 100%"}},"ed3152d235ec4d4b974ee394d17c73d0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc2f29133a8d4924a27d0ef798831e9c","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0b993e16b7504a7b9e1a67352c1d987f","value":1}},"ed95141e71b648eda10476c71bf90e08":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed97f5c92bf240ef8dd23d65ad16fd87":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f5e6a8d8a1a84a14ac645e2f985b0c3a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f7a33881311243389b35ca2aa42f7e27":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_76c72810e7124c1c8c48cd9df89a972b","IPY_MODEL_06af3150a7314a259d247dda4b878f70","IPY_MODEL_d11bf26044134d699ab7d4bd6e05f888"],"layout":"IPY_MODEL_28034f6e8ae345c3a0d37354a74addd1"}},"f9a5a93cd9a645a0837ec5cd5643b013":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa675313c2cc4898801c46339ff8eda6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!pip install --upgrade protobuf==3.20.0\n!pip install -U 'tensorflow[and-cuda]'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T15:30:02.353491Z","iopub.execute_input":"2025-11-26T15:30:02.354059Z","iopub.status.idle":"2025-11-26T15:31:13.687388Z","shell.execute_reply.started":"2025-11-26T15:30:02.354032Z","shell.execute_reply":"2025-11-26T15:31:13.686427Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow[and-cuda] in /usr/local/lib/python3.11/dist-packages (2.18.0)\nCollecting tensorflow[and-cuda]\n  Downloading tensorflow-2.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (25.2.10)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (0.6.0)\nRequirement already satisfied: google_pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (18.1.1)\nRequirement already satisfied: opt_einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (3.4.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (25.0)\nRequirement already satisfied: protobuf>=5.28.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (6.33.0)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (2.32.5)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (75.2.0)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (1.17.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (3.1.0)\nRequirement already satisfied: typing_extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (4.15.0)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (1.17.2)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (1.74.0)\nCollecting tensorboard~=2.20.0 (from tensorflow[and-cuda])\n  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\nCollecting keras>=3.10.0 (from tensorflow[and-cuda])\n  Downloading keras-3.12.0-py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (1.26.4)\nRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (3.14.0)\nCollecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow[and-cuda])\n  Downloading ml_dtypes-0.5.4-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.9 kB)\nRequirement already satisfied: nvidia-cublas-cu12<13.0,>=12.5.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (12.5.3.2)\nRequirement already satisfied: nvidia-cuda-cupti-cu12<13.0,>=12.5.82 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (12.5.82)\nRequirement already satisfied: nvidia-cuda-nvcc-cu12<13.0,>=12.5.82 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (12.5.82)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12<13.0,>=12.5.82 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (12.5.82)\nRequirement already satisfied: nvidia-cuda-runtime-cu12<13.0,>=12.5.82 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (12.5.82)\nRequirement already satisfied: nvidia-cudnn-cu12<10.0,>=9.3.0.75 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (9.3.0.75)\nRequirement already satisfied: nvidia-cufft-cu12<12.0,>=11.2.3.61 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (11.2.3.61)\nRequirement already satisfied: nvidia-curand-cu12<11.0,>=10.3.6.82 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (10.3.6.82)\nRequirement already satisfied: nvidia-cusolver-cu12<12.0,>=11.6.3.83 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (11.6.3.83)\nRequirement already satisfied: nvidia-cusparse-cu12<13.0,>=12.5.1.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (12.5.1.3)\nCollecting nvidia-nccl-cu12<3.0,>=2.25.1 (from tensorflow[and-cuda])\n  Downloading nvidia_nccl_cu12-2.28.9-py3-none-manylinux_2_18_x86_64.whl.metadata (2.0 kB)\nRequirement already satisfied: nvidia-nvjitlink-cu12<13.0,>=12.5.82 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (12.5.82)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow[and-cuda]) (0.45.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.10.0->tensorflow[and-cuda]) (14.2.0)\nRequirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.10.0->tensorflow[and-cuda]) (0.1.0)\nRequirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.10.0->tensorflow[and-cuda]) (0.16.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.0->tensorflow[and-cuda]) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.0->tensorflow[and-cuda]) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.0->tensorflow[and-cuda]) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.0->tensorflow[and-cuda]) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.0->tensorflow[and-cuda]) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.0->tensorflow[and-cuda]) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (2025.10.5)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.20.0->tensorflow[and-cuda]) (3.8.2)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.20.0->tensorflow[and-cuda]) (11.3.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.20.0->tensorflow[and-cuda]) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.20.0->tensorflow[and-cuda]) (3.1.3)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow[and-cuda]) (3.0.3)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.0->tensorflow[and-cuda]) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.0->tensorflow[and-cuda]) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.0->tensorflow[and-cuda]) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.26.0->tensorflow[and-cuda]) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.26.0->tensorflow[and-cuda]) (2024.2.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.10.0->tensorflow[and-cuda]) (4.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.10.0->tensorflow[and-cuda]) (2.19.2)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.26.0->tensorflow[and-cuda]) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow[and-cuda]) (0.1.2)\nDownloading keras-3.12.0-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading ml_dtypes-0.5.4-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.28.9-py3-none-manylinux_2_18_x86_64.whl (296.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.8/296.8 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading tensorflow-2.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.6/620.6 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nccl-cu12, ml_dtypes, tensorboard, keras, tensorflow\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.21.5\n    Uninstalling nvidia-nccl-cu12-2.21.5:\n      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n  Attempting uninstall: ml_dtypes\n    Found existing installation: ml-dtypes 0.4.1\n    Uninstalling ml-dtypes-0.4.1:\n      Successfully uninstalled ml-dtypes-0.4.1\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.18.0\n    Uninstalling tensorboard-2.18.0:\n      Successfully uninstalled tensorboard-2.18.0\n  Attempting uninstall: keras\n    Found existing installation: keras 3.8.0\n    Uninstalling keras-3.8.0:\n      Successfully uninstalled keras-3.8.0\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.18.0\n    Uninstalling tensorflow-2.18.0:\n      Successfully uninstalled tensorflow-2.18.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nccl-cu12==2.21.5; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nccl-cu12 2.28.9 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\ntf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.20.0 which is incompatible.\ntensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.20.0 which is incompatible.\ntensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.20.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed keras-3.12.0 ml_dtypes-0.5.4 nvidia-nccl-cu12-2.28.9 tensorboard-2.20.0 tensorflow-2.20.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install evaluate","metadata":{"id":"QBTSK7foyxo_","outputId":"c2d0d0a5-f25d-43e0-96e1-54a4210b726f","trusted":true,"execution":{"iopub.status.busy":"2025-11-26T15:31:13.689064Z","iopub.execute_input":"2025-11-26T15:31:13.689336Z","iopub.status.idle":"2025-11-26T15:31:20.784964Z","shell.execute_reply.started":"2025-11-26T15:31:13.689312Z","shell.execute_reply":"2025-11-26T15:31:20.784307Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.4.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.5)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.18)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.10.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.36.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\nCollecting pyarrow>=21.0.0 (from datasets>=2.0.0->evaluate)\n  Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (0.28.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.10.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.11.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1.0.0->datasets>=2.0.0->evaluate) (1.3.1)\nDownloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (47.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pyarrow, evaluate\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 19.0.1\n    Uninstalling pyarrow-19.0.1:\n      Successfully uninstalled pyarrow-19.0.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed evaluate-0.4.6 pyarrow-22.0.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# **1.   Imports**","metadata":{"id":"G0uMRFlq4QSo"}},{"cell_type":"code","source":"import torch\nimport gc\nimport random\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport evaluate\nimport time\nimport glob\nfrom typing import Dict, Any, List, Optional, Tuple, cast\nfrom dataclasses import dataclass, asdict\n\n# Hugging Face Libraries\nfrom datasets import load_dataset, DatasetDict\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n    DataCollatorWithPadding,\n    EvalPrediction\n)\nfrom peft import LoraConfig, TaskType, get_peft_model","metadata":{"id":"FaO5xsPcyQBa","trusted":true,"execution":{"iopub.status.busy":"2025-11-26T15:31:20.785860Z","iopub.execute_input":"2025-11-26T15:31:20.786073Z","iopub.status.idle":"2025-11-26T15:31:44.583484Z","shell.execute_reply.started":"2025-11-26T15:31:20.786052Z","shell.execute_reply":"2025-11-26T15:31:44.582845Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Hardware & Reproducibility\nSEED = 42\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"id":"tWOPO2zb0b-0","trusted":true,"execution":{"iopub.status.busy":"2025-11-26T15:31:44.584901Z","iopub.execute_input":"2025-11-26T15:31:44.585412Z","iopub.status.idle":"2025-11-26T15:31:44.589041Z","shell.execute_reply.started":"2025-11-26T15:31:44.585392Z","shell.execute_reply":"2025-11-26T15:31:44.588404Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Data & Model Settings\nMODEL_NAME = \"distilbert-base-uncased\"\nTRAIN_SAMPLE_SIZE = 3000\nNUM_LABELS = 6\nMAX_LENGTH = 128","metadata":{"id":"Saoe4Phv0jHE","trusted":true,"execution":{"iopub.status.busy":"2025-11-26T15:31:44.589812Z","iopub.execute_input":"2025-11-26T15:31:44.590078Z","iopub.status.idle":"2025-11-26T15:31:44.635954Z","shell.execute_reply.started":"2025-11-26T15:31:44.590054Z","shell.execute_reply":"2025-11-26T15:31:44.635432Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# PSO Settings\nPSO_POPULATION_SIZE = 20   # Number of particles\nPSO_EPOCHS = 5            # Number of iterations\nINERTIA_W = 0.7           # Inertia weight\nCOGNITIVE_C1 = 1.5        # Personal best weight\nSOCIAL_C2 = 1.5           # Global best weight","metadata":{"id":"5tBfZqXm0lDC","trusted":true,"execution":{"iopub.status.busy":"2025-11-26T15:31:44.636567Z","iopub.execute_input":"2025-11-26T15:31:44.636727Z","iopub.status.idle":"2025-11-26T15:31:44.652556Z","shell.execute_reply.started":"2025-11-26T15:31:44.636714Z","shell.execute_reply":"2025-11-26T15:31:44.651840Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Search Space (Discrete Options)\nWARMUP_OPTIONS = [0.0, 0.06, 0.1]\nRANK_OPTIONS = [2, 4, 8, 16, 24]\nALPHA_OPTIONS = [8, 16, 32, 64, 96]\nDROPOUT_OPTIONS = [0.0, 0.05, 0.1, 0.2]\nTARGET_MODULE_OPTIONS = [\n    [\"q_lin\", \"v_lin\"],                           # Index 0\n    [\"q_lin\", \"v_lin\", \"ffn.lin1\", \"ffn.lin2\"]    # Index 1\n]","metadata":{"id":"8mgKBVUk0nuD","trusted":true,"execution":{"iopub.status.busy":"2025-11-26T15:31:44.653149Z","iopub.execute_input":"2025-11-26T15:31:44.653712Z","iopub.status.idle":"2025-11-26T15:31:44.666673Z","shell.execute_reply.started":"2025-11-26T15:31:44.653695Z","shell.execute_reply":"2025-11-26T15:31:44.666114Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# **2.   Data Structure Definitions**","metadata":{"id":"6Ux8mVfc5M6U"}},{"cell_type":"code","source":"@dataclass(frozen=True)\nclass LoraHyperparameters:\n    \"\"\"Structure to hold a specific set of hyperparameters\"\"\"\n    learning_rate: float\n    warmup_ratio: float\n    rank: int\n    alpha: int\n    dropout: float\n    target_modules: List[str]\n\nclass Particle:\n    \"\"\"Represents a single particle in the swarm\"\"\"\n    def __init__(self, dim, min_bound, max_bound):\n        self.position = np.random.uniform(min_bound, max_bound, dim)\n        self.velocity = np.random.uniform(-1, 1, dim)\n        self.best_position = np.copy(self.position)\n        self.best_score = -float('inf') # Maximizing accuracy","metadata":{"id":"JXtyXE7V0p0C","trusted":true,"execution":{"iopub.status.busy":"2025-11-26T15:31:44.667461Z","iopub.execute_input":"2025-11-26T15:31:44.667689Z","iopub.status.idle":"2025-11-26T15:31:44.682530Z","shell.execute_reply.started":"2025-11-26T15:31:44.667662Z","shell.execute_reply":"2025-11-26T15:31:44.681904Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# **3.   Helper Functions**","metadata":{"id":"DmociEDp5RZl"}},{"cell_type":"code","source":"def set_global_seed(seed: int):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n\ndef cleanup_memory():\n    \"\"\"Forcefully releases GPU memory\"\"\"\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"id":"IgvJdUZm0uN5","trusted":true,"execution":{"iopub.status.busy":"2025-11-26T15:31:44.683189Z","iopub.execute_input":"2025-11-26T15:31:44.683451Z","iopub.status.idle":"2025-11-26T15:31:44.699835Z","shell.execute_reply.started":"2025-11-26T15:31:44.683430Z","shell.execute_reply":"2025-11-26T15:31:44.699189Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# **4.   Data Management Loss**","metadata":{"id":"GAft_wnH666f"}},{"cell_type":"code","source":"class DataManager:\n    def __init__(self, model_name: str = MODEL_NAME):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.dataset: Optional[Dict[str, Any]] = None\n\n    def prepare_data(self) -> Dict[str, Any]:\n        if self.dataset is not None:\n            return self.dataset\n\n        print(\"Loading and processing data...\")\n        full_dataset = cast(DatasetDict, load_dataset(\"dair-ai/emotion\"))\n\n        # Consistent subset selection\n        train_subset = full_dataset[\"train\"].shuffle(seed=SEED).select(range(TRAIN_SAMPLE_SIZE))\n\n        def _tokenize(examples):\n            return self.tokenizer(\n                examples[\"text\"],\n                truncation=True,\n                padding=\"max_length\",\n                max_length=MAX_LENGTH\n            )\n\n        tokenized_train = train_subset.map(_tokenize, batched=True)\n        tokenized_val = full_dataset[\"validation\"].map(_tokenize, batched=True)\n        tokenized_test = full_dataset[\"test\"].map(_tokenize, batched=True)\n\n        self.dataset = {\n            \"train\": tokenized_train,\n            \"validation\": tokenized_val,\n            \"test\": tokenized_test,\n            \"tokenizer\": self.tokenizer,\n            \"num_labels\": NUM_LABELS\n        }\n        print(\"Data preparation complete.\")\n        return self.dataset","metadata":{"id":"MGMn1seI0wwM","trusted":true,"execution":{"iopub.status.busy":"2025-11-26T15:31:44.701521Z","iopub.execute_input":"2025-11-26T15:31:44.701714Z","iopub.status.idle":"2025-11-26T15:31:44.714704Z","shell.execute_reply.started":"2025-11-26T15:31:44.701700Z","shell.execute_reply":"2025-11-26T15:31:44.714147Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# **5.   PSO Optimiser Engine**","metadata":{"id":"rtyP7dZd7Atx"}},{"cell_type":"code","source":"class PSO_HyperparameterOptimizer:\n    def __init__(self, data_bundle: Dict[str, Any]):\n        self.data = data_bundle\n        self.results: List[Dict[str, Any]] = []\n        self.metric = evaluate.load(\"accuracy\")\n\n        # 6 Dimensions: [LR, Warmup, Rank, Alpha, Dropout, Modules]\n        # We use indices for discrete lists, continuous for LR\n        self.min_bounds = np.array([1e-5, 0, 0, 0, 0, 0], dtype=float)\n        self.max_bounds = np.array([\n            2e-4,\n            len(WARMUP_OPTIONS) - 0.01,\n            len(RANK_OPTIONS) - 0.01,\n            len(ALPHA_OPTIONS) - 0.01,\n            len(DROPOUT_OPTIONS) - 0.01,\n            len(TARGET_MODULE_OPTIONS) - 0.01\n        ], dtype=float)\n        self.dim = 6\n        self.best_params = None\n        self.best_val_accuracy = -1.0\n\n    def _map_position_to_params(self, position: np.ndarray) -> LoraHyperparameters:\n        \"\"\"Maps continuous particle position to discrete hyperparameters\"\"\"\n        # Clip to ensure bounds\n        pos = np.clip(position, self.min_bounds, self.max_bounds)\n\n        return LoraHyperparameters(\n            learning_rate=float(pos[0]),\n            warmup_ratio=WARMUP_OPTIONS[int(pos[1])],\n            rank=RANK_OPTIONS[int(pos[2])],\n            alpha=ALPHA_OPTIONS[int(pos[3])],\n            dropout=DROPOUT_OPTIONS[int(pos[4])],\n            target_modules=TARGET_MODULE_OPTIONS[int(pos[5])]\n        )\n\n    def _compute_metrics(self, eval_pred: EvalPrediction):\n        preds, labels = eval_pred\n        preds = np.argmax(preds, axis=1)\n        return self.metric.compute(predictions=preds, references=labels)\n\n    def train_model(self, trial_id: int, params: LoraHyperparameters) -> float:\n        \"\"\"Runs a single training trial\"\"\"\n        print(f\"   > Params: LR={params.learning_rate:.2e}, Rank={params.rank}, Alpha={params.alpha}\")\n        \n        model = AutoModelForSequenceClassification.from_pretrained(\n            MODEL_NAME, num_labels=self.data[\"num_labels\"]\n        )\n\n        peft_config = LoraConfig(\n            task_type=TaskType.SEQ_CLS,\n            r=params.rank,\n            lora_alpha=params.alpha,\n            lora_dropout=params.dropout,\n            target_modules=params.target_modules\n        )\n        model = get_peft_model(model, peft_config)\n\n        current_seed = SEED + trial_id\n        \n        args = TrainingArguments(\n            output_dir=f\"./results/trial_{trial_id}\",\n            learning_rate=params.learning_rate,\n            per_device_train_batch_size=16,\n            per_device_eval_batch_size=16,\n            num_train_epochs=3,\n            warmup_ratio=params.warmup_ratio,\n            weight_decay=0.01,\n            eval_strategy=\"epoch\",\n            save_strategy=\"no\",\n            logging_strategy=\"epoch\",\n            seed=current_seed,\n            report_to=\"none\",\n            load_best_model_at_end=False\n        )\n\n        data_collator = DataCollatorWithPadding(tokenizer=self.data[\"tokenizer\"])\n\n        trainer = Trainer(\n            model=model,\n            args=args,\n            train_dataset=self.data[\"train\"],\n            eval_dataset=self.data[\"validation\"],\n            data_collator=data_collator,\n            compute_metrics=self._compute_metrics\n        )\n\n        trainer.train()\n        eval_results = trainer.evaluate()\n\n        # Clean up immediately\n        del model\n        del trainer\n        cleanup_memory()\n\n        return eval_results[\"eval_accuracy\"]\n\n    def evaluate_on_test(self, params: LoraHyperparameters, trial_id: int) -> float:\n        \"\"\"Evaluates best hyperparameters on the test set\"\"\"\n        print(f\"\\n   > Evaluating on TEST set with params: LR={params.learning_rate:.2e}, Rank={params.rank}\")\n\n        model = AutoModelForSequenceClassification.from_pretrained(\n            MODEL_NAME, num_labels=self.data[\"num_labels\"]\n        )\n\n        peft_config = LoraConfig(\n            task_type=TaskType.SEQ_CLS,\n            r=params.rank,\n            lora_alpha=params.alpha,\n            lora_dropout=params.dropout,\n            target_modules=params.target_modules\n        )\n        model = get_peft_model(model, peft_config)\n\n        args = TrainingArguments(\n            output_dir=f\"./results/test_eval_{trial_id}\",\n            learning_rate=params.learning_rate,\n            per_device_train_batch_size=16,\n            per_device_eval_batch_size=16,\n            num_train_epochs=3,\n            warmup_ratio=params.warmup_ratio,\n            weight_decay=0.01,\n            eval_strategy=\"no\",\n            save_strategy=\"no\",\n            logging_strategy=\"no\",\n            seed=SEED + trial_id,\n            report_to=\"none\",\n        )\n\n        data_collator = DataCollatorWithPadding(tokenizer=self.data[\"tokenizer\"])\n\n        trainer = Trainer(\n            model=model,\n            args=args,\n            train_dataset=self.data[\"train\"],\n            eval_dataset=self.data[\"test\"],\n            data_collator=data_collator,\n            compute_metrics=self._compute_metrics\n        )\n\n        # Train on training data\n        trainer.train()\n\n        # Evaluate on test data\n        test_results = trainer.evaluate()\n\n        del model\n        del trainer\n        cleanup_memory()\n\n        return test_results[\"eval_accuracy\"]\n\n    def evaluate_top_solutions_with_seeds(self, run_id=1, num_top=5, num_seeds=3):\n        \"\"\"\n        Retrains the top 5 unique configs found in PSO 3 times each with NEW seeds.\n        Saves the results to 'robustness_results_runX.csv'.\n        \"\"\"\n        print(f\"\\n{'='*60}\")\n        print(f\"ROBUSTNESS CHECK: TOP {num_top} CONFIGS x {num_seeds} NEW SEEDS\")\n        print(f\"{'='*60}\")\n\n        # 1. Filter Top Unique Candidates\n        sorted_results = sorted(self.results, key=lambda x: x['val_accuracy'], reverse=True)\n        unique_candidates = []\n        seen_configs = set()\n\n        for res in sorted_results:\n            config_key = (\n                res['learning_rate'], res['warmup_ratio'], res['rank'], \n                res['alpha'], res['dropout'], tuple(res['target_modules'])\n            )\n            if config_key not in seen_configs:\n                seen_configs.add(config_key)\n                unique_candidates.append(res)\n            if len(unique_candidates) >= num_top:\n                break\n\n        final_robust_results = []\n        \n        # 2. Retrain Candidates\n        for rank, candidate in enumerate(unique_candidates, 1):\n            params = LoraHyperparameters(\n                learning_rate=candidate['learning_rate'],\n                warmup_ratio=candidate['warmup_ratio'],\n                rank=candidate['rank'],\n                alpha=candidate['alpha'],\n                dropout=candidate['dropout'],\n                target_modules=candidate['target_modules']\n            )\n\n            print(f\"\\n--- Rank {rank} (Original Acc: {candidate['val_accuracy']:.4%}) ---\")\n            seed_accuracies = []\n\n            for seed_i in range(num_seeds):\n                robust_trial_id = 10000 + (run_id * 100) + seed_i\n\n                display_seed = SEED + robust_trial_id\n                \n                print(f\"   > Retraining Seed {seed_i + 1}/{num_seeds} (Seed: {display_seed})...\", end=\" \", flush=True)\n                \n                acc = self.train_model(robust_trial_id, params)\n                \n                seed_accuracies.append(acc)\n                print(f\"Acc: {acc:.4%}\")\n\n            mean_acc = np.mean(seed_accuracies)\n            std_acc = np.std(seed_accuracies)\n\n            # Prepare Record for CSV\n            record = asdict(params)\n            record.update({\n                'rank': rank,\n                'original_pso_accuracy': candidate['val_accuracy'],\n                'mean_robust_accuracy': mean_acc,\n                'std_robust_accuracy': std_acc,\n                'seed_accuracies': str(seed_accuracies)\n            })\n            final_robust_results.append(record)\n\n        # 3. Save Robustness Results to CSV\n        robust_df = pd.DataFrame(final_robust_results)\n        robust_filename = f\"robustness_results_run{run_id+1}.csv\"\n        robust_df.to_csv(robust_filename, index=False)\n        print(f\"\\nRobustness detailed results saved to {robust_filename}\")\n\n        # 4. Update Best Params based on MEAN accuracy (Stability wins)\n        final_robust_results.sort(key=lambda x: x['mean_robust_accuracy'], reverse=True)\n        if final_robust_results:\n            winner = final_robust_results[0]\n            print(f\"\\nUpdated Best Params to Rank {winner['rank']} (Mean: {winner['mean_robust_accuracy']:.4%})\")\n            \n            self.best_params = LoraHyperparameters(\n                learning_rate=winner['learning_rate'],\n                warmup_ratio=winner['warmup_ratio'],\n                rank=winner['rank'],\n                alpha=winner['alpha'],\n                dropout=winner['dropout'],\n                target_modules=winner['target_modules']\n            )\n            self.best_val_accuracy = winner['mean_robust_accuracy']\n            \n    def run_optimization(self):\n        print(f\"Starting PSO: {PSO_POPULATION_SIZE} particles, {PSO_EPOCHS} epochs.\")\n        swarm = [Particle(self.dim, self.min_bounds, self.max_bounds) for _ in range(PSO_POPULATION_SIZE)]\n        global_best_pos = np.zeros(self.dim)\n        global_best_score = -float('inf')\n        trial_count = 0\n\n        for epoch in range(PSO_EPOCHS):\n            print(f\"\\n=== PSO EPOCH {epoch + 1}/{PSO_EPOCHS} ===\")\n            for i, particle in enumerate(swarm):\n                trial_count += 1\n                # print(f\"\\n[Trial {trial_count}] Particle {i+1}\")\n                try:\n                    params = self._map_position_to_params(particle.position)\n                    start_time = time.time()\n                    accuracy = self.train_model(trial_count, params)\n                    duration = time.time() - start_time\n                    \n                    print(f\"   > [Trial {trial_count}] Accuracy: {accuracy:.4%}, Time: {duration:.2f}s\")\n\n                    if accuracy > particle.best_score:\n                        particle.best_score = accuracy\n                        particle.best_position = np.copy(particle.position)\n\n                    if accuracy > global_best_score:\n                        global_best_score = accuracy\n                        global_best_pos = np.copy(particle.position)\n                        print(f\"   >>> New Swarm Best: {accuracy:.4%}\")\n\n                    record = asdict(params)\n                    record.update({\n                        \"trial_id\": trial_count, \n                        \"val_accuracy\": accuracy, \n                        \"pso_epoch\": epoch+1,\n                        \"training_time_sec\": duration\n                    })\n                    self.results.append(record)\n\n                except Exception as e:\n                    print(f\"!!! ERROR in Trial {trial_count}: {e}\")\n                    cleanup_memory()\n\n            # Update Particles\n            for particle in swarm:\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                vel_cognitive = COGNITIVE_C1 * r1 * (particle.best_position - particle.position)\n                vel_social = SOCIAL_C2 * r2 * (global_best_pos - particle.position)\n                particle.velocity = (INERTIA_W * particle.velocity) + vel_cognitive + vel_social\n                particle.position += particle.velocity\n                particle.position = np.clip(particle.position, self.min_bounds, self.max_bounds)\n\n    def save_results(self, filename: str):\n        if not self.results:\n            print(\"No results to save.\")\n            return\n        df = pd.DataFrame(self.results)\n        df.to_csv(filename, index=False)\n        print(f\"Results saved to {filename}\")\n        \n        best_run = df.loc[df['val_accuracy'].idxmax()]\n        print(\"\\nBEST VALIDATION RESULT:\")\n        print(f\"Accuracy: {best_run['val_accuracy']:.4%}\")\n        \n        print(f\"Rank: {best_run['rank']}, Alpha: {best_run['alpha']}, LR: {best_run['learning_rate']:.2e}\")","metadata":{"id":"cqprVCju0zhN","trusted":true,"execution":{"iopub.status.busy":"2025-11-26T15:31:44.715533Z","iopub.execute_input":"2025-11-26T15:31:44.715807Z","iopub.status.idle":"2025-11-26T15:31:44.742653Z","shell.execute_reply.started":"2025-11-26T15:31:44.715785Z","shell.execute_reply":"2025-11-26T15:31:44.741968Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# **6.   Main Execution Pipeline**","metadata":{"id":"rq-nEUIT7HL_"}},{"cell_type":"code","source":"def run_single_optimization(run_id: int, data_bundle: Dict[str, Any]) -> Tuple[float, float, LoraHyperparameters]:\n    print(f\"\\n{'='*60}\\nRUN {run_id + 1}\\n{'='*60}\")\n\n    optimizer = PSO_HyperparameterOptimizer(data_bundle)\n\n    # 1. Broad Search (20 particles)\n    optimizer.run_optimization()\n\n    optimizer.evaluate_top_solutions_with_seeds(run_id=run_id, num_top=5, num_seeds=3)\n    \n    # 3. Save PSO History\n    optimizer.save_results(f\"initial_results.csv\")\n    \n    # 4. Final Test (Using the Robust Best Params)\n    print(\"\\n\" + \"=\"*50 + \"\\nEVALUATING ROBUST BEST CONFIG ON TEST SET\\n\" + \"=\"*50)\n    test_accuracy = optimizer.evaluate_on_test(optimizer.best_params, trial_id=run_id)\n    print(f\"\\nRun {run_id + 1} - Robust Val: {optimizer.best_val_accuracy:.4%}, Test: {test_accuracy:.4%}\")\n\n    return optimizer.best_val_accuracy, test_accuracy, optimizer.best_params\n\nif __name__ == \"__main__\":\n    NUM_RUNS = 1  # Number of independent optimization runs\n    STOCHASTICITY_SEED_BASE = SEED\n\n    print(\"=\"*60)\n    print(f\"STOCHASTICITY REPORTING: {NUM_RUNS} FULL OPTIMIZATION RUN\")\n    print(\"=\"*60)\n\n    all_results = []\n    val_accuracies = []\n    test_accuracies = []\n\n    try:\n        # 1. Prepare Data (once, reused for all runs)\n        data_mgr = DataManager(MODEL_NAME)\n        data_bundle = data_mgr.prepare_data()\n\n        for run in range(NUM_RUNS):\n            set_global_seed(STOCHASTICITY_SEED_BASE + run)\n\n            try:\n                val_acc, test_acc, best_params = run_single_optimization(run, data_bundle)\n\n                all_results.append({\n                    \"run_id\": run + 1,\n                    \"seed\": STOCHASTICITY_SEED_BASE + run,\n                    \"val_accuracy\": val_acc,\n                    \"test_accuracy\": test_acc,\n                    \"learning_rate\": best_params.learning_rate,\n                    \"warmup_ratio\": best_params.warmup_ratio,\n                    \"rank\": best_params.rank,\n                    \"alpha\": best_params.alpha,\n                    \"dropout\": best_params.dropout,\n                    \"target_modules\": str(best_params.target_modules)\n                })\n\n                val_accuracies.append(val_acc)\n                test_accuracies.append(test_acc)\n\n            except Exception as e:\n                print(f\"!!! ERROR in Run {run + 1}: {e}\")\n                import traceback\n                traceback.print_exc()\n                cleanup_memory()\n\n        # 3. Print Summary Statistics\n        print(\"\\n\" + \"=\"*60)\n        print(\"STOCHASTICITY SUMMARY\")\n        print(\"=\"*60)\n\n        if val_accuracies:\n            val_mean = np.mean(val_accuracies)\n            val_std = np.std(val_accuracies)\n            val_min = np.min(val_accuracies)\n            val_max = np.max(val_accuracies)\n\n            print(f\"\\nVALIDATION ACCURACY (from PSO search):\")\n            print(f\"  Mean: {val_mean:.4%}\")\n            print(f\"  Std:  {val_std:.4%}\")\n            print(f\"  Min:  {val_min:.4%}\")\n            print(f\"  Max:  {val_max:.4%}\")\n\n            test_mean = np.mean(test_accuracies)\n            test_std = np.std(test_accuracies)\n            test_min = np.min(test_accuracies)\n            test_max = np.max(test_accuracies)\n\n            print(f\"\\nTEST ACCURACY (generalization):\")\n            print(f\"  Mean: {test_mean:.4%}\")\n            print(f\"  Std:  {test_std:.4%}\")\n            print(f\"  Min:  {test_min:.4%}\")\n            print(f\"  Max:  {test_max:.4%}\")\n\n            print(f\"\\nIndividual Results:\")\n            for result in all_results:\n                print(f\"  Run {result['run_id']}: Val={result['val_accuracy']:.4%}, Test={result['test_accuracy']:.4%}\")\n\n        # 4. Save stochasticity results\n        results_df = pd.DataFrame(all_results)\n        results_df.to_csv(\"pso_stochasticity_results.csv\", index=False)\n        print(f\"\\nStochasticity results saved to pso_stochasticity_results.csv\")\n\n        # Print Final Summary\n        print(\"\\n\" + \"=\"*60)\n        print(\"FINAL SINGLE RUN SUMMARY\")\n        print(\"=\"*60)\n        \n        if all_results:\n            res = all_results[0]\n            print(f\"Run 1 Results:\")\n            print(f\"  Robust Validation Accuracy: {res['val_accuracy']:.4%}\")\n            print(f\"  Final Test Accuracy:        {res['test_accuracy']:.4%}\")\n            print(f\"  Best Parameters Found:\")\n            print(f\"    LR: {res['learning_rate']:.2e}, Rank: {res['rank']}, Alpha: {res['alpha']}\")\n\n        # 4. Save results\n        results_df = pd.DataFrame(all_results)\n        results_df.to_csv(\"pso_final_results.csv\", index=False)\n        print(f\"\\nFinal results saved to pso_final_results.csv\")\n    \n    except KeyboardInterrupt:\n        print(\"\\nOptimization interrupted by user.\")\n    except Exception as e:\n        print(f\"\\nCritical failure: {e}\")\n        import traceback\n        traceback.print_exc()\n    finally:\n        cleanup_memory()\n        print(\"Process Complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T15:31:44.743431Z","iopub.execute_input":"2025-11-26T15:31:44.743679Z","iopub.status.idle":"2025-11-26T18:08:03.182101Z","shell.execute_reply.started":"2025-11-26T15:31:44.743663Z","shell.execute_reply":"2025-11-26T18:08:03.181315Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"============================================================\nSTOCHASTICITY REPORTING: 1 FULL OPTIMIZATION RUN\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a2d10309ced4d23a54c9dba0c1c73bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"109e1bc90c8d4eb08b1959648d18dac9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06526b0713e349dfaa54e16266417704"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"878a5f74db764124b180cbe207fa8e1a"}},"metadata":{}},{"name":"stdout","text":"Loading and processing data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9db78e9d9f734088a13c1fd5c7f8f05f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"split/train-00000-of-00001.parquet:   0%|          | 0.00/1.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"552668d3f4344fe8a474ba745f7cba9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"split/validation-00000-of-00001.parquet:   0%|          | 0.00/127k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"299d3a5f964142dc9ac3dc0c777196d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"split/test-00000-of-00001.parquet:   0%|          | 0.00/129k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"015975fba8b34c9f902c7385232c74fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/16000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"796063ca6ddf4873b31f1b4b9524fae3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7de0ccbfdf654862ae5fc00c9989c559"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d145d1d4ce184ccd8b20cd1291cc1a71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22b4081ac314465b8dee93f019894492"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0070e5e83cc84eaba0abe4c37825f68e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e85cc238513b4d548b9cca1e9bfba43f"}},"metadata":{}},{"name":"stdout","text":"Data preparation complete.\n\n============================================================\nRUN 1\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0d0c0be87e5441ca99a3b18b267242b"}},"metadata":{}},{"name":"stdout","text":"Starting PSO: 20 particles, 5 epochs.\n\n=== PSO EPOCH 1/5 ===\n   > Params: LR=8.12e-05, Rank=16, Alpha=32\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc14834cd40c45f481e51a94220e58da"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nNo label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:01, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.575000</td>\n      <td>1.338386</td>\n      <td>0.527500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.162500</td>\n      <td>1.052251</td>\n      <td>0.613000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.995900</td>\n      <td>0.987069</td>\n      <td>0.648000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:04]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 1] Accuracy: 64.8000%, Time: 70.58s\n   >>> New Swarm Best: 64.8000%\n   > Params: LR=1.68e-04, Rank=2, Alpha=8\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.375300</td>\n      <td>1.090121</td>\n      <td>0.606500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.918400</td>\n      <td>0.801216</td>\n      <td>0.692000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.739600</td>\n      <td>0.732495</td>\n      <td>0.720000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 2] Accuracy: 72.0000%, Time: 82.48s\n   >>> New Swarm Best: 72.0000%\n   > Params: LR=9.67e-05, Rank=2, Alpha=32\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:07, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.519500</td>\n      <td>1.231409</td>\n      <td>0.535000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.102200</td>\n      <td>1.008371</td>\n      <td>0.649000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.953800</td>\n      <td>0.945258</td>\n      <td>0.662000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 3] Accuracy: 66.2000%, Time: 73.56s\n   > Params: LR=6.79e-05, Rank=16, Alpha=32\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:07, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.532700</td>\n      <td>1.321954</td>\n      <td>0.532000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.194200</td>\n      <td>1.115488</td>\n      <td>0.578500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.072100</td>\n      <td>1.064407</td>\n      <td>0.611500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 4] Accuracy: 61.1500%, Time: 74.09s\n   > Params: LR=1.14e-04, Rank=24, Alpha=64\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:17, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.290800</td>\n      <td>0.956644</td>\n      <td>0.663000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.786500</td>\n      <td>0.671591</td>\n      <td>0.750000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.581500</td>\n      <td>0.597693</td>\n      <td>0.772000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 5] Accuracy: 77.2000%, Time: 84.12s\n   >>> New Swarm Best: 77.2000%\n   > Params: LR=8.38e-05, Rank=24, Alpha=16\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:16, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.442700</td>\n      <td>1.186525</td>\n      <td>0.548000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.045000</td>\n      <td>0.950681</td>\n      <td>0.666000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.877400</td>\n      <td>0.881120</td>\n      <td>0.677000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 6] Accuracy: 67.7000%, Time: 83.97s\n   > Params: LR=1.10e-05, Rank=16, Alpha=64\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:07, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.709300</td>\n      <td>1.634418</td>\n      <td>0.352000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.591900</td>\n      <td>1.568919</td>\n      <td>0.352000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.553100</td>\n      <td>1.556630</td>\n      <td>0.362000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 7] Accuracy: 36.2000%, Time: 74.19s\n   > Params: LR=6.91e-05, Rank=16, Alpha=64\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:07, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.502300</td>\n      <td>1.248165</td>\n      <td>0.535000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.119100</td>\n      <td>1.028327</td>\n      <td>0.640500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.978200</td>\n      <td>0.971254</td>\n      <td>0.672000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 8] Accuracy: 67.2000%, Time: 74.16s\n   > Params: LR=1.09e-04, Rank=2, Alpha=8\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:14, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.469700</td>\n      <td>1.192565</td>\n      <td>0.548500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.053400</td>\n      <td>0.961636</td>\n      <td>0.647000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.907800</td>\n      <td>0.899548</td>\n      <td>0.669500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 9] Accuracy: 66.9500%, Time: 81.58s\n   > Params: LR=5.35e-05, Rank=4, Alpha=8\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.572200</td>\n      <td>1.492051</td>\n      <td>0.485500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.336500</td>\n      <td>1.230149</td>\n      <td>0.538000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.188700</td>\n      <td>1.182029</td>\n      <td>0.546500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 10] Accuracy: 54.6500%, Time: 83.01s\n   > Params: LR=1.63e-04, Rank=4, Alpha=8\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:07, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.510200</td>\n      <td>1.209556</td>\n      <td>0.537000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.067100</td>\n      <td>0.960310</td>\n      <td>0.647000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.882700</td>\n      <td>0.879092</td>\n      <td>0.677000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 11] Accuracy: 67.7000%, Time: 73.74s\n   > Params: LR=3.28e-05, Rank=24, Alpha=16\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:16, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.633400</td>\n      <td>1.536654</td>\n      <td>0.457500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.410800</td>\n      <td>1.299820</td>\n      <td>0.532000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.251200</td>\n      <td>1.240348</td>\n      <td>0.541500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 12] Accuracy: 54.1500%, Time: 83.99s\n   > Params: LR=6.41e-05, Rank=16, Alpha=32\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:07, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.543800</td>\n      <td>1.366597</td>\n      <td>0.530000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.221700</td>\n      <td>1.146325</td>\n      <td>0.562500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.095800</td>\n      <td>1.089794</td>\n      <td>0.592500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 13] Accuracy: 59.2500%, Time: 73.61s\n   > Params: LR=1.38e-04, Rank=4, Alpha=64\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.348100</td>\n      <td>0.943903</td>\n      <td>0.670000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.729000</td>\n      <td>0.616760</td>\n      <td>0.780500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.520400</td>\n      <td>0.541078</td>\n      <td>0.813500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 14] Accuracy: 81.3500%, Time: 82.92s\n   >>> New Swarm Best: 81.3500%\n   > Params: LR=1.77e-05, Rank=16, Alpha=8\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:07, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.674700</td>\n      <td>1.600427</td>\n      <td>0.353000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.572200</td>\n      <td>1.566851</td>\n      <td>0.364000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.551700</td>\n      <td>1.560521</td>\n      <td>0.382000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 15] Accuracy: 38.2000%, Time: 74.12s\n   > Params: LR=7.48e-05, Rank=24, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:16, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.342300</td>\n      <td>1.027967</td>\n      <td>0.623500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.863000</td>\n      <td>0.768422</td>\n      <td>0.709000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.681100</td>\n      <td>0.694071</td>\n      <td>0.739500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 16] Accuracy: 73.9500%, Time: 83.90s\n   > Params: LR=1.81e-04, Rank=4, Alpha=16\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.380200</td>\n      <td>0.998509</td>\n      <td>0.623000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.825900</td>\n      <td>0.695090</td>\n      <td>0.742500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.612700</td>\n      <td>0.619060</td>\n      <td>0.775500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 17] Accuracy: 77.5500%, Time: 83.01s\n   > Params: LR=1.25e-04, Rank=2, Alpha=64\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:06, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.382000</td>\n      <td>1.078089</td>\n      <td>0.612500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.942000</td>\n      <td>0.847255</td>\n      <td>0.686500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.796900</td>\n      <td>0.793684</td>\n      <td>0.698500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 18] Accuracy: 69.8500%, Time: 73.72s\n   > Params: LR=7.18e-05, Rank=16, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:16, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.426800</td>\n      <td>1.094262</td>\n      <td>0.581000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.911800</td>\n      <td>0.806599</td>\n      <td>0.712000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.715100</td>\n      <td>0.718443</td>\n      <td>0.727500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 19] Accuracy: 72.7500%, Time: 83.55s\n   > Params: LR=1.79e-04, Rank=16, Alpha=32\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:07, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.393200</td>\n      <td>1.042379</td>\n      <td>0.612500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.843800</td>\n      <td>0.738429</td>\n      <td>0.730000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.673600</td>\n      <td>0.685032</td>\n      <td>0.744500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 20] Accuracy: 74.4500%, Time: 74.10s\n\n=== PSO EPOCH 2/5 ===\n   > Params: LR=1.00e-05, Rank=8, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:07, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.705500</td>\n      <td>1.630801</td>\n      <td>0.352000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.589300</td>\n      <td>1.566154</td>\n      <td>0.356500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.549000</td>\n      <td>1.552235</td>\n      <td>0.399000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 21] Accuracy: 39.9000%, Time: 73.60s\n   > Params: LR=1.00e-05, Rank=4, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.656500</td>\n      <td>1.576713</td>\n      <td>0.352000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.544600</td>\n      <td>1.538633</td>\n      <td>0.441000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.514200</td>\n      <td>1.519666</td>\n      <td>0.473000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 22] Accuracy: 47.3000%, Time: 82.91s\n   > Params: LR=2.00e-04, Rank=2, Alpha=64\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:07, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.322500</td>\n      <td>0.947831</td>\n      <td>0.648500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.797100</td>\n      <td>0.705727</td>\n      <td>0.746500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.627300</td>\n      <td>0.642561</td>\n      <td>0.773500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 23] Accuracy: 77.3500%, Time: 73.69s\n   > Params: LR=1.00e-05, Rank=4, Alpha=64\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:14, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.714100</td>\n      <td>1.612666</td>\n      <td>0.403000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.553500</td>\n      <td>1.529808</td>\n      <td>0.468500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.507000</td>\n      <td>1.505273</td>\n      <td>0.493500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 24] Accuracy: 49.3500%, Time: 81.83s\n   > Params: LR=2.00e-04, Rank=2, Alpha=64\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:14, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.276800</td>\n      <td>0.838011</td>\n      <td>0.693000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.647200</td>\n      <td>0.530665</td>\n      <td>0.802500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.419300</td>\n      <td>0.450634</td>\n      <td>0.848000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 25] Accuracy: 84.8000%, Time: 81.91s\n   >>> New Swarm Best: 84.8000%\n   > Params: LR=1.00e-05, Rank=8, Alpha=64\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:07, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.731500</td>\n      <td>1.656200</td>\n      <td>0.354000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.609900</td>\n      <td>1.581550</td>\n      <td>0.372000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.564800</td>\n      <td>1.566674</td>\n      <td>0.387000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 26] Accuracy: 38.7000%, Time: 73.85s\n   > Params: LR=1.00e-05, Rank=8, Alpha=64\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:07, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.720600</td>\n      <td>1.648090</td>\n      <td>0.352000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.602800</td>\n      <td>1.574893</td>\n      <td>0.352500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.562000</td>\n      <td>1.561499</td>\n      <td>0.366000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 27] Accuracy: 36.6000%, Time: 73.84s\n   > Params: LR=1.00e-05, Rank=2, Alpha=64\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.713900</td>\n      <td>1.607824</td>\n      <td>0.352000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.570500</td>\n      <td>1.553720</td>\n      <td>0.387000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.536800</td>\n      <td>1.540887</td>\n      <td>0.417500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 28] Accuracy: 41.7500%, Time: 82.91s\n   > Params: LR=1.00e-05, Rank=8, Alpha=16\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.703900</td>\n      <td>1.644331</td>\n      <td>0.345000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.604700</td>\n      <td>1.582324</td>\n      <td>0.432500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.565800</td>\n      <td>1.569565</td>\n      <td>0.431500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 29] Accuracy: 43.1500%, Time: 82.01s\n   > Params: LR=2.00e-04, Rank=4, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.181900</td>\n      <td>0.710922</td>\n      <td>0.743000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.534900</td>\n      <td>0.433776</td>\n      <td>0.855000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.321100</td>\n      <td>0.373175</td>\n      <td>0.880000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 30] Accuracy: 88.0000%, Time: 82.91s\n   >>> New Swarm Best: 88.0000%\n   > Params: LR=2.00e-04, Rank=2, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:07, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.355400</td>\n      <td>0.944342</td>\n      <td>0.654500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.773300</td>\n      <td>0.679023</td>\n      <td>0.752500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.610900</td>\n      <td>0.614183</td>\n      <td>0.777500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 31] Accuracy: 77.7500%, Time: 73.75s\n   > Params: LR=1.00e-05, Rank=16, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:07, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.690300</td>\n      <td>1.629910</td>\n      <td>0.358000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.589300</td>\n      <td>1.570135</td>\n      <td>0.381000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.551800</td>\n      <td>1.557746</td>\n      <td>0.399000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 32] Accuracy: 39.9000%, Time: 74.10s\n   > Params: LR=2.00e-04, Rank=4, Alpha=32\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.206300</td>\n      <td>0.840870</td>\n      <td>0.696500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.670400</td>\n      <td>0.577845</td>\n      <td>0.782000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.479800</td>\n      <td>0.503468</td>\n      <td>0.823000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 33] Accuracy: 82.3000%, Time: 82.84s\n   > Params: LR=2.00e-04, Rank=2, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:07, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.325400</td>\n      <td>0.901083</td>\n      <td>0.686000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.758000</td>\n      <td>0.665216</td>\n      <td>0.761000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.600700</td>\n      <td>0.612550</td>\n      <td>0.784500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 34] Accuracy: 78.4500%, Time: 73.79s\n   > Params: LR=2.00e-04, Rank=8, Alpha=64\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:07, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.330000</td>\n      <td>0.967366</td>\n      <td>0.664000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.766000</td>\n      <td>0.679416</td>\n      <td>0.749000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.596600</td>\n      <td>0.624789</td>\n      <td>0.778500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 35] Accuracy: 77.8500%, Time: 73.81s\n   > Params: LR=2.00e-04, Rank=2, Alpha=64\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.203800</td>\n      <td>0.822132</td>\n      <td>0.698500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.630200</td>\n      <td>0.535338</td>\n      <td>0.803500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.435400</td>\n      <td>0.474961</td>\n      <td>0.833000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 36] Accuracy: 83.3000%, Time: 82.85s\n   > Params: LR=2.00e-04, Rank=4, Alpha=32\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.338300</td>\n      <td>0.915650</td>\n      <td>0.671000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.692300</td>\n      <td>0.573207</td>\n      <td>0.797500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.476700</td>\n      <td>0.505692</td>\n      <td>0.821000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 37] Accuracy: 82.1000%, Time: 82.89s\n   > Params: LR=2.00e-04, Rank=4, Alpha=64\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:14, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.156700</td>\n      <td>0.766810</td>\n      <td>0.711000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.585900</td>\n      <td>0.493104</td>\n      <td>0.827500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.392200</td>\n      <td>0.439829</td>\n      <td>0.850000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 38] Accuracy: 85.0000%, Time: 81.53s\n   > Params: LR=1.00e-05, Rank=2, Alpha=64\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.708500</td>\n      <td>1.605210</td>\n      <td>0.445500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.552700</td>\n      <td>1.534610</td>\n      <td>0.451500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.509500</td>\n      <td>1.509079</td>\n      <td>0.486000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 39] Accuracy: 48.6000%, Time: 82.82s\n   > Params: LR=1.00e-05, Rank=4, Alpha=32\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:07, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.779400</td>\n      <td>1.695394</td>\n      <td>0.352500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.650700</td>\n      <td>1.611753</td>\n      <td>0.352500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.595200</td>\n      <td>1.592683</td>\n      <td>0.353500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 40] Accuracy: 35.3500%, Time: 73.93s\n\n=== PSO EPOCH 3/5 ===\n   > Params: LR=1.00e-05, Rank=8, Alpha=64\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:16, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.679800</td>\n      <td>1.588294</td>\n      <td>0.359000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.550200</td>\n      <td>1.534699</td>\n      <td>0.433500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.511000</td>\n      <td>1.515227</td>\n      <td>0.480500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 41] Accuracy: 48.0500%, Time: 83.09s\n   > Params: LR=1.00e-05, Rank=8, Alpha=32\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:16, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.745400</td>\n      <td>1.648346</td>\n      <td>0.358500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.593600</td>\n      <td>1.564226</td>\n      <td>0.371500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.547100</td>\n      <td>1.549127</td>\n      <td>0.407000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 42] Accuracy: 40.7000%, Time: 83.17s\n   > Params: LR=2.00e-04, Rank=4, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.184900</td>\n      <td>0.748135</td>\n      <td>0.712500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.588300</td>\n      <td>0.510830</td>\n      <td>0.829000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.380700</td>\n      <td>0.439295</td>\n      <td>0.851500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 43] Accuracy: 85.1500%, Time: 83.02s\n   > Params: LR=1.00e-05, Rank=8, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:16, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.678800</td>\n      <td>1.577633</td>\n      <td>0.352000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.544800</td>\n      <td>1.524731</td>\n      <td>0.464000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.502500</td>\n      <td>1.498232</td>\n      <td>0.491500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"   > [Trial 44] Accuracy: 49.1500%, Time: 83.10s\n   > Params: LR=2.00e-04, Rank=2, Alpha=64\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nNo label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:07, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.366400</td>\n      <td>0.939889</td>\n      <td>0.664000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.787300</td>\n      <td>0.688419</td>\n      <td>0.753000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.614400</td>\n      <td>0.623036</td>\n      <td>0.780000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 45] Accuracy: 78.0000%, Time: 73.89s\n   > Params: LR=1.00e-05, Rank=4, Alpha=64\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.717900</td>\n      <td>1.610011</td>\n      <td>0.367000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.563500</td>\n      <td>1.549160</td>\n      <td>0.422000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.524300</td>\n      <td>1.531053</td>\n      <td>0.468000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 46] Accuracy: 46.8000%, Time: 82.93s\n   > Params: LR=1.00e-05, Rank=4, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:14, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.703000</td>\n      <td>1.589687</td>\n      <td>0.355500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.553600</td>\n      <td>1.537678</td>\n      <td>0.441500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.517300</td>\n      <td>1.517373</td>\n      <td>0.489500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 47] Accuracy: 48.9500%, Time: 81.61s\n   > Params: LR=1.00e-05, Rank=16, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.690500</td>\n      <td>1.582181</td>\n      <td>0.352500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.521500</td>\n      <td>1.476439</td>\n      <td>0.515500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.438900</td>\n      <td>1.426607</td>\n      <td>0.530500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 48] Accuracy: 53.0500%, Time: 82.26s\n   > Params: LR=1.00e-05, Rank=4, Alpha=32\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:14, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.723700</td>\n      <td>1.648685</td>\n      <td>0.275000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.596200</td>\n      <td>1.567072</td>\n      <td>0.428500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.549900</td>\n      <td>1.554662</td>\n      <td>0.422000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 49] Accuracy: 42.2000%, Time: 81.65s\n   > Params: LR=2.00e-04, Rank=8, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:14, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.225000</td>\n      <td>0.743402</td>\n      <td>0.728500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.546000</td>\n      <td>0.457624</td>\n      <td>0.846500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.329400</td>\n      <td>0.394390</td>\n      <td>0.862500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 50] Accuracy: 86.2500%, Time: 81.85s\n   > Params: LR=2.00e-04, Rank=4, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:16, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.260300</td>\n      <td>0.766024</td>\n      <td>0.720500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.582500</td>\n      <td>0.475804</td>\n      <td>0.829000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.368900</td>\n      <td>0.399349</td>\n      <td>0.865500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 51] Accuracy: 86.5500%, Time: 83.05s\n   > Params: LR=1.00e-05, Rank=8, Alpha=32\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:16, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.737700</td>\n      <td>1.644763</td>\n      <td>0.367500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.591000</td>\n      <td>1.566497</td>\n      <td>0.362500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.547600</td>\n      <td>1.556336</td>\n      <td>0.392000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 52] Accuracy: 39.2000%, Time: 83.31s\n   > Params: LR=2.00e-04, Rank=2, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.222600</td>\n      <td>0.771206</td>\n      <td>0.736500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.596000</td>\n      <td>0.481628</td>\n      <td>0.837000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.383900</td>\n      <td>0.427575</td>\n      <td>0.858500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 53] Accuracy: 85.8500%, Time: 82.93s\n   > Params: LR=2.00e-04, Rank=8, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:16, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.098400</td>\n      <td>0.705145</td>\n      <td>0.752000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.521900</td>\n      <td>0.499445</td>\n      <td>0.842500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.349600</td>\n      <td>0.420203</td>\n      <td>0.868000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 54] Accuracy: 86.8000%, Time: 83.08s\n   > Params: LR=2.00e-04, Rank=4, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:14, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.306100</td>\n      <td>0.822980</td>\n      <td>0.704000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.588500</td>\n      <td>0.483789</td>\n      <td>0.838500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.361200</td>\n      <td>0.401517</td>\n      <td>0.865000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 55] Accuracy: 86.5000%, Time: 81.45s\n   > Params: LR=2.00e-04, Rank=2, Alpha=64\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.273200</td>\n      <td>0.865366</td>\n      <td>0.705500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.661600</td>\n      <td>0.549102</td>\n      <td>0.815000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.437300</td>\n      <td>0.462808</td>\n      <td>0.841000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 56] Accuracy: 84.1000%, Time: 83.02s\n   > Params: LR=2.00e-04, Rank=4, Alpha=64\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:14, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.274700</td>\n      <td>0.839399</td>\n      <td>0.705500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.618900</td>\n      <td>0.502957</td>\n      <td>0.822000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.403800</td>\n      <td>0.445043</td>\n      <td>0.851500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 57] Accuracy: 85.1500%, Time: 81.52s\n   > Params: LR=2.00e-04, Rank=4, Alpha=64\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:16, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.257900</td>\n      <td>0.791484</td>\n      <td>0.710000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.621900</td>\n      <td>0.514806</td>\n      <td>0.828000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.394200</td>\n      <td>0.432301</td>\n      <td>0.856000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 58] Accuracy: 85.6000%, Time: 82.98s\n   > Params: LR=1.00e-05, Rank=16, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:16, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.694500</td>\n      <td>1.586475</td>\n      <td>0.352500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.540500</td>\n      <td>1.500666</td>\n      <td>0.500500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.467400</td>\n      <td>1.457904</td>\n      <td>0.516500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 59] Accuracy: 51.6500%, Time: 83.42s\n   > Params: LR=1.00e-05, Rank=4, Alpha=64\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.696900</td>\n      <td>1.608840</td>\n      <td>0.352000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.568100</td>\n      <td>1.548027</td>\n      <td>0.370500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.529000</td>\n      <td>1.533515</td>\n      <td>0.451500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 60] Accuracy: 45.1500%, Time: 82.98s\n\n=== PSO EPOCH 4/5 ===\n   > Params: LR=1.00e-05, Rank=8, Alpha=32\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:07, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.692900</td>\n      <td>1.635620</td>\n      <td>0.352500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.605600</td>\n      <td>1.587419</td>\n      <td>0.352500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.573400</td>\n      <td>1.576563</td>\n      <td>0.354000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 61] Accuracy: 35.4000%, Time: 73.41s\n   > Params: LR=1.00e-05, Rank=2, Alpha=8\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:16, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.727100</td>\n      <td>1.662677</td>\n      <td>0.352500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.629700</td>\n      <td>1.605953</td>\n      <td>0.352000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.593100</td>\n      <td>1.593126</td>\n      <td>0.352500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 62] Accuracy: 35.2500%, Time: 83.06s\n   > Params: LR=2.00e-04, Rank=8, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:16, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.135800</td>\n      <td>0.782466</td>\n      <td>0.708500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.547200</td>\n      <td>0.485791</td>\n      <td>0.837000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.352500</td>\n      <td>0.421169</td>\n      <td>0.863500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 63] Accuracy: 86.3500%, Time: 83.20s\n   > Params: LR=1.00e-05, Rank=16, Alpha=64\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:07, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.701500</td>\n      <td>1.635734</td>\n      <td>0.352000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.601600</td>\n      <td>1.580202</td>\n      <td>0.352000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.566700</td>\n      <td>1.568069</td>\n      <td>0.352000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 64] Accuracy: 35.2000%, Time: 74.52s\n   > Params: LR=2.00e-04, Rank=4, Alpha=64\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.115500</td>\n      <td>0.710598</td>\n      <td>0.731000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.542600</td>\n      <td>0.499731</td>\n      <td>0.820500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.371600</td>\n      <td>0.436282</td>\n      <td>0.853000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 65] Accuracy: 85.3000%, Time: 82.79s\n   > Params: LR=1.00e-05, Rank=24, Alpha=16\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:16, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.711500</td>\n      <td>1.642043</td>\n      <td>0.352000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.606400</td>\n      <td>1.582792</td>\n      <td>0.352000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.570900</td>\n      <td>1.571056</td>\n      <td>0.356000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 66] Accuracy: 35.6000%, Time: 84.21s\n   > Params: LR=1.00e-05, Rank=2, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.679000</td>\n      <td>1.583213</td>\n      <td>0.352000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.547400</td>\n      <td>1.528485</td>\n      <td>0.451000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.503400</td>\n      <td>1.501188</td>\n      <td>0.494500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 67] Accuracy: 49.4500%, Time: 83.11s\n   > Params: LR=1.00e-05, Rank=24, Alpha=64\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:17, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.718000</td>\n      <td>1.610453</td>\n      <td>0.360500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.569200</td>\n      <td>1.553504</td>\n      <td>0.449000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.531500</td>\n      <td>1.538035</td>\n      <td>0.458500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 68] Accuracy: 45.8500%, Time: 85.20s\n   > Params: LR=1.00e-05, Rank=2, Alpha=64\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.704500</td>\n      <td>1.607551</td>\n      <td>0.358000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.562700</td>\n      <td>1.551212</td>\n      <td>0.443000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.529800</td>\n      <td>1.536286</td>\n      <td>0.467500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 69] Accuracy: 46.7500%, Time: 82.66s\n   > Params: LR=2.00e-04, Rank=4, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.178800</td>\n      <td>0.810523</td>\n      <td>0.696000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.591300</td>\n      <td>0.490917</td>\n      <td>0.847000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.371800</td>\n      <td>0.435252</td>\n      <td>0.858000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 70] Accuracy: 85.8000%, Time: 82.99s\n   > Params: LR=2.00e-04, Rank=4, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:16, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.126400</td>\n      <td>0.720782</td>\n      <td>0.726500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.547100</td>\n      <td>0.497714</td>\n      <td>0.836500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.343700</td>\n      <td>0.423057</td>\n      <td>0.856500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 71] Accuracy: 85.6500%, Time: 83.82s\n   > Params: LR=1.00e-05, Rank=8, Alpha=16\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:17, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.717700</td>\n      <td>1.648393</td>\n      <td>0.392000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.609700</td>\n      <td>1.581429</td>\n      <td>0.360500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.566500</td>\n      <td>1.567403</td>\n      <td>0.365500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 72] Accuracy: 36.5500%, Time: 84.19s\n   > Params: LR=2.00e-04, Rank=4, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:16, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.218200</td>\n      <td>0.738324</td>\n      <td>0.720000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.568900</td>\n      <td>0.458341</td>\n      <td>0.844000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.358000</td>\n      <td>0.411995</td>\n      <td>0.862500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 73] Accuracy: 86.2500%, Time: 83.87s\n   > Params: LR=2.00e-04, Rank=16, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:17, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.117500</td>\n      <td>0.695138</td>\n      <td>0.742000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.510800</td>\n      <td>0.456127</td>\n      <td>0.854500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.321900</td>\n      <td>0.396509</td>\n      <td>0.877500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 74] Accuracy: 87.7500%, Time: 84.82s\n   > Params: LR=2.00e-04, Rank=4, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:14, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.228900</td>\n      <td>0.780857</td>\n      <td>0.709000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.578800</td>\n      <td>0.471699</td>\n      <td>0.829500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.350700</td>\n      <td>0.405166</td>\n      <td>0.866500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 75] Accuracy: 86.6500%, Time: 81.79s\n   > Params: LR=2.00e-04, Rank=2, Alpha=64\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:16, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.262100</td>\n      <td>0.816155</td>\n      <td>0.707000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.621100</td>\n      <td>0.503001</td>\n      <td>0.836500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.405800</td>\n      <td>0.445113</td>\n      <td>0.849000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 76] Accuracy: 84.9000%, Time: 83.41s\n   > Params: LR=2.00e-04, Rank=4, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.204800</td>\n      <td>0.741222</td>\n      <td>0.725000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.547900</td>\n      <td>0.475576</td>\n      <td>0.843500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.351100</td>\n      <td>0.403998</td>\n      <td>0.870000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 77] Accuracy: 87.0000%, Time: 82.23s\n   > Params: LR=2.00e-04, Rank=8, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:16, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.197600</td>\n      <td>0.714743</td>\n      <td>0.728500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.528800</td>\n      <td>0.453184</td>\n      <td>0.855000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.328800</td>\n      <td>0.400334</td>\n      <td>0.865500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 78] Accuracy: 86.5500%, Time: 83.62s\n   > Params: LR=1.00e-05, Rank=16, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:17, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.687000</td>\n      <td>1.581741</td>\n      <td>0.352000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.548500</td>\n      <td>1.529080</td>\n      <td>0.426500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.507300</td>\n      <td>1.504563</td>\n      <td>0.495000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 79] Accuracy: 49.5000%, Time: 84.52s\n   > Params: LR=1.00e-05, Rank=16, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:17, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.668500</td>\n      <td>1.588376</td>\n      <td>0.352000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.551100</td>\n      <td>1.538774</td>\n      <td>0.472500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.510000</td>\n      <td>1.513247</td>\n      <td>0.500000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 80] Accuracy: 50.0000%, Time: 84.39s\n\n=== PSO EPOCH 5/5 ===\n   > Params: LR=1.00e-05, Rank=8, Alpha=64\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:07, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.732700</td>\n      <td>1.661410</td>\n      <td>0.408500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.616400</td>\n      <td>1.586318</td>\n      <td>0.415000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.572200</td>\n      <td>1.572183</td>\n      <td>0.406500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 81] Accuracy: 40.6500%, Time: 74.51s\n   > Params: LR=1.00e-05, Rank=4, Alpha=16\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:16, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.703900</td>\n      <td>1.641205</td>\n      <td>0.367500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.604300</td>\n      <td>1.583378</td>\n      <td>0.352500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.569600</td>\n      <td>1.571464</td>\n      <td>0.352000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 82] Accuracy: 35.2000%, Time: 83.63s\n   > Params: LR=2.00e-04, Rank=8, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.147800</td>\n      <td>0.764769</td>\n      <td>0.722000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.551200</td>\n      <td>0.465559</td>\n      <td>0.841500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.337800</td>\n      <td>0.413239</td>\n      <td>0.862000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 83] Accuracy: 86.2000%, Time: 82.28s\n   > Params: LR=1.00e-05, Rank=4, Alpha=64\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:07, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.736800</td>\n      <td>1.659814</td>\n      <td>0.411000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.612700</td>\n      <td>1.585468</td>\n      <td>0.420000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.570500</td>\n      <td>1.570574</td>\n      <td>0.415000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 84] Accuracy: 41.5000%, Time: 73.75s\n   > Params: LR=2.00e-04, Rank=8, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:14, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.146600</td>\n      <td>0.690313</td>\n      <td>0.743000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.523100</td>\n      <td>0.470440</td>\n      <td>0.841500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.340500</td>\n      <td>0.408340</td>\n      <td>0.868500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 85] Accuracy: 86.8500%, Time: 81.80s\n   > Params: LR=1.00e-05, Rank=24, Alpha=8\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.692600</td>\n      <td>1.637773</td>\n      <td>0.400500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.608400</td>\n      <td>1.592976</td>\n      <td>0.384000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.579700</td>\n      <td>1.582275</td>\n      <td>0.382500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 86] Accuracy: 38.2500%, Time: 82.54s\n   > Params: LR=1.00e-05, Rank=2, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.684800</td>\n      <td>1.582283</td>\n      <td>0.351500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.546900</td>\n      <td>1.532481</td>\n      <td>0.407000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.510200</td>\n      <td>1.510385</td>\n      <td>0.464500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 87] Accuracy: 46.4500%, Time: 82.90s\n   > Params: LR=1.00e-05, Rank=24, Alpha=64\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:07, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.730200</td>\n      <td>1.652471</td>\n      <td>0.352000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.612000</td>\n      <td>1.583009</td>\n      <td>0.355500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.571500</td>\n      <td>1.570367</td>\n      <td>0.367000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 88] Accuracy: 36.7000%, Time: 74.31s\n   > Params: LR=1.00e-05, Rank=2, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:16, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.662400</td>\n      <td>1.578768</td>\n      <td>0.432500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.533300</td>\n      <td>1.500046</td>\n      <td>0.496000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.462800</td>\n      <td>1.457917</td>\n      <td>0.524500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 89] Accuracy: 52.4500%, Time: 83.06s\n   > Params: LR=2.00e-04, Rank=4, Alpha=64\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.199600</td>\n      <td>0.801193</td>\n      <td>0.710500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.622900</td>\n      <td>0.522705</td>\n      <td>0.818500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.414800</td>\n      <td>0.451030</td>\n      <td>0.848500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 90] Accuracy: 84.8500%, Time: 82.89s\n   > Params: LR=2.00e-04, Rank=8, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.161700</td>\n      <td>0.729093</td>\n      <td>0.720000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.539500</td>\n      <td>0.459784</td>\n      <td>0.841500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.337600</td>\n      <td>0.395705</td>\n      <td>0.866000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 91] Accuracy: 86.6000%, Time: 83.03s\n   > Params: LR=1.00e-05, Rank=24, Alpha=64\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:17, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.680500</td>\n      <td>1.602285</td>\n      <td>0.352000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.560800</td>\n      <td>1.544583</td>\n      <td>0.432500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.523300</td>\n      <td>1.526002</td>\n      <td>0.462000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 92] Accuracy: 46.2000%, Time: 84.18s\n   > Params: LR=2.00e-04, Rank=8, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:16, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.240300</td>\n      <td>0.814449</td>\n      <td>0.706500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.594300</td>\n      <td>0.485716</td>\n      <td>0.824500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.390700</td>\n      <td>0.411887</td>\n      <td>0.861500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 93] Accuracy: 86.1500%, Time: 83.85s\n   > Params: LR=2.00e-04, Rank=8, Alpha=64\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:16, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.196800</td>\n      <td>0.769476</td>\n      <td>0.718500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.596000</td>\n      <td>0.510428</td>\n      <td>0.815000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.396000</td>\n      <td>0.435986</td>\n      <td>0.854500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 94] Accuracy: 85.4500%, Time: 84.13s\n   > Params: LR=2.00e-04, Rank=4, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.147400</td>\n      <td>0.771883</td>\n      <td>0.722000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.589800</td>\n      <td>0.479628</td>\n      <td>0.839000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.355200</td>\n      <td>0.400388</td>\n      <td>0.870000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 95] Accuracy: 87.0000%, Time: 82.41s\n   > Params: LR=2.00e-04, Rank=4, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:16, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.196000</td>\n      <td>0.784769</td>\n      <td>0.711500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.576200</td>\n      <td>0.489628</td>\n      <td>0.837500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.375400</td>\n      <td>0.420551</td>\n      <td>0.860500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 96] Accuracy: 86.0500%, Time: 83.97s\n   > Params: LR=2.00e-04, Rank=4, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.238000</td>\n      <td>0.781706</td>\n      <td>0.716500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.587000</td>\n      <td>0.492352</td>\n      <td>0.836000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.366100</td>\n      <td>0.429198</td>\n      <td>0.861500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 97] Accuracy: 86.1500%, Time: 82.35s\n   > Params: LR=2.00e-04, Rank=8, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:16, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.170100</td>\n      <td>0.740792</td>\n      <td>0.730000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.556300</td>\n      <td>0.462302</td>\n      <td>0.847500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.336100</td>\n      <td>0.409965</td>\n      <td>0.871500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 98] Accuracy: 87.1500%, Time: 83.80s\n   > Params: LR=1.00e-05, Rank=8, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:16, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.702900</td>\n      <td>1.596826</td>\n      <td>0.364000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.554400</td>\n      <td>1.528893</td>\n      <td>0.478500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.501100</td>\n      <td>1.496667</td>\n      <td>0.506500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > [Trial 99] Accuracy: 50.6500%, Time: 83.58s\n   > Params: LR=1.00e-05, Rank=8, Alpha=64\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:07, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.684900</td>\n      <td>1.625676</td>\n      <td>0.421000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.591600</td>\n      <td>1.578496</td>\n      <td>0.362000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.567000</td>\n      <td>1.568950</td>\n      <td>0.367000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"   > [Trial 100] Accuracy: 36.7000%, Time: 74.03s\n\n============================================================\nROBUSTNESS CHECK: TOP 5 CONFIGS x 3 NEW SEEDS\n============================================================\n\n--- Rank 1 (Original Acc: 88.0000%) ---\n   > Retraining Seed 1/3 (Seed: 10042)... ","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > Params: LR=2.00e-04, Rank=4, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:16, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.218300</td>\n      <td>0.743180</td>\n      <td>0.735000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.558700</td>\n      <td>0.472059</td>\n      <td>0.835500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.342800</td>\n      <td>0.405637</td>\n      <td>0.862000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Acc: 86.2000%\n   > Retraining Seed 2/3 (Seed: 10043)... ","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > Params: LR=2.00e-04, Rank=4, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:16, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.223800</td>\n      <td>0.739265</td>\n      <td>0.719000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.566800</td>\n      <td>0.459341</td>\n      <td>0.846500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.366200</td>\n      <td>0.398877</td>\n      <td>0.869000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Acc: 86.9000%\n   > Retraining Seed 3/3 (Seed: 10044)... ","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > Params: LR=2.00e-04, Rank=4, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:16, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.288700</td>\n      <td>0.804347</td>\n      <td>0.715000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.603600</td>\n      <td>0.482212</td>\n      <td>0.844000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.379500</td>\n      <td>0.422215</td>\n      <td>0.853500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Acc: 85.3500%\n\n--- Rank 2 (Original Acc: 87.7500%) ---\n   > Retraining Seed 1/3 (Seed: 10042)... ","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > Params: LR=2.00e-04, Rank=16, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:16, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.088600</td>\n      <td>0.687695</td>\n      <td>0.740000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.508500</td>\n      <td>0.446843</td>\n      <td>0.851500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.321500</td>\n      <td>0.387442</td>\n      <td>0.867500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Acc: 86.7500%\n   > Retraining Seed 2/3 (Seed: 10043)... ","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > Params: LR=2.00e-04, Rank=16, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:16, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.156100</td>\n      <td>0.704726</td>\n      <td>0.727000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.516100</td>\n      <td>0.459260</td>\n      <td>0.849000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.322600</td>\n      <td>0.386370</td>\n      <td>0.875500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Acc: 87.5500%\n   > Retraining Seed 3/3 (Seed: 10044)... ","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > Params: LR=2.00e-04, Rank=16, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:16, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.156900</td>\n      <td>0.718903</td>\n      <td>0.747500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.523200</td>\n      <td>0.426775</td>\n      <td>0.856000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.321100</td>\n      <td>0.385319</td>\n      <td>0.876000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Acc: 87.6000%\n\n--- Rank 3 (Original Acc: 87.1500%) ---\n   > Retraining Seed 1/3 (Seed: 10042)... ","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > Params: LR=2.00e-04, Rank=8, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:16, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.147000</td>\n      <td>0.677777</td>\n      <td>0.750000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.514800</td>\n      <td>0.444444</td>\n      <td>0.850000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.325600</td>\n      <td>0.382854</td>\n      <td>0.866000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Acc: 86.6000%\n   > Retraining Seed 2/3 (Seed: 10043)... ","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > Params: LR=2.00e-04, Rank=8, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:16, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.203900</td>\n      <td>0.713815</td>\n      <td>0.718500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.516300</td>\n      <td>0.462157</td>\n      <td>0.851500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.327600</td>\n      <td>0.398256</td>\n      <td>0.878000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Acc: 87.8000%\n   > Retraining Seed 3/3 (Seed: 10044)... ","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > Params: LR=2.00e-04, Rank=8, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:16, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.259700</td>\n      <td>0.774785</td>\n      <td>0.715500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.581800</td>\n      <td>0.458548</td>\n      <td>0.852000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.357100</td>\n      <td>0.402007</td>\n      <td>0.867000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Acc: 86.7000%\n\n--- Rank 4 (Original Acc: 87.0000%) ---\n   > Retraining Seed 1/3 (Seed: 10042)... ","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > Params: LR=2.00e-04, Rank=4, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:14, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.230800</td>\n      <td>0.710588</td>\n      <td>0.741500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.541600</td>\n      <td>0.470241</td>\n      <td>0.837500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.343900</td>\n      <td>0.416019</td>\n      <td>0.867000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Acc: 86.7000%\n   > Retraining Seed 2/3 (Seed: 10043)... ","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > Params: LR=2.00e-04, Rank=4, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:14, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.253400</td>\n      <td>0.735869</td>\n      <td>0.719500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.561100</td>\n      <td>0.470293</td>\n      <td>0.844500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.345100</td>\n      <td>0.393522</td>\n      <td>0.869500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Acc: 86.9500%\n   > Retraining Seed 3/3 (Seed: 10044)... ","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > Params: LR=2.00e-04, Rank=4, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.316400</td>\n      <td>0.805595</td>\n      <td>0.712500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.591300</td>\n      <td>0.461113</td>\n      <td>0.846500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.351100</td>\n      <td>0.398446</td>\n      <td>0.863500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Acc: 86.3500%\n\n--- Rank 5 (Original Acc: 87.0000%) ---\n   > Retraining Seed 1/3 (Seed: 10042)... ","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > Params: LR=2.00e-04, Rank=4, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:14, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.117200</td>\n      <td>0.709689</td>\n      <td>0.741500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.521300</td>\n      <td>0.472132</td>\n      <td>0.838500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.355700</td>\n      <td>0.420935</td>\n      <td>0.856000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Acc: 85.6000%\n   > Retraining Seed 2/3 (Seed: 10043)... ","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > Params: LR=2.00e-04, Rank=4, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.142900</td>\n      <td>0.683425</td>\n      <td>0.741500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.525300</td>\n      <td>0.455247</td>\n      <td>0.849000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.332100</td>\n      <td>0.400781</td>\n      <td>0.862500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Acc: 86.2500%\n   > Retraining Seed 3/3 (Seed: 10044)... ","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   > Params: LR=2.00e-04, Rank=4, Alpha=96\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.170700</td>\n      <td>0.758034</td>\n      <td>0.721500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.558200</td>\n      <td>0.466857</td>\n      <td>0.842500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.352000</td>\n      <td>0.409323</td>\n      <td>0.865500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:06]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Acc: 86.5500%\n\nRobustness detailed results saved to robustness_results_run1.csv\n\nUpdated Best Params to Rank 2 (Mean: 87.3000%)\nResults saved to initial_results.csv\n\nBEST VALIDATION RESULT:\nAccuracy: 88.0000%\nRank: 4, Alpha: 96, LR: 2.00e-04\n\n==================================================\nEVALUATING ROBUST BEST CONFIG ON TEST SET\n==================================================\n\n   > Evaluating on TEST set with params: LR=2.00e-04, Rank=2\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 00:59, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"\nRun 1 - Robust Val: 87.3000%, Test: 86.3000%\n\n============================================================\nSTOCHASTICITY SUMMARY\n============================================================\n\nVALIDATION ACCURACY (from PSO search):\n  Mean: 87.3000%\n  Std:  0.0000%\n  Min:  87.3000%\n  Max:  87.3000%\n\nTEST ACCURACY (generalization):\n  Mean: 86.3000%\n  Std:  0.0000%\n  Min:  86.3000%\n  Max:  86.3000%\n\nIndividual Results:\n  Run 1: Val=87.3000%, Test=86.3000%\n\nStochasticity results saved to pso_stochasticity_results.csv\n\n============================================================\nFINAL SINGLE RUN SUMMARY\n============================================================\nRun 1 Results:\n  Robust Validation Accuracy: 87.3000%\n  Final Test Accuracy:        86.3000%\n  Best Parameters Found:\n    LR: 2.00e-04, Rank: 2, Alpha: 96\n\nFinal results saved to pso_final_results.csv\nProcess Complete.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# **7.   Plot Generation**","metadata":{}},{"cell_type":"code","source":"def visualize_results():\n    # Target specifically Run 1 (The first full execution)\n    file_path = \"initial_results.csv\"\n    \n    # if not os.path.exists(file_path):\n    #     print(f\"Results file '{file_path}' not found.\")\n    #     return\n    \n    # Load data\n    df = pd.read_csv(file_path)\n    \n    # Ensure we are only looking at the Search Phase (Phase 1)\n    # Filter out any robustness trials if they accidentally got mixed in (Trial IDs > 9000)\n    if 'trial_id' in df.columns:\n        df = df[df['trial_id'] < 9000].copy()\n        \n    print(f\"Plotting Phase 1 Search Results: {len(df)} trials (20 particles x 4 epochs)\")\n    \n    plt.style.use('seaborn-v0_8-whitegrid')\n    fig, axes = plt.subplots(1, 3, figsize=(20,6))\n    \n    color = 'tab:blue'\n    run_name = \"Run 1 (Search Phase)\"\n    \n    # --- Plot 1: Convergence Trend ---\n    # Shows how the swarm found better solutions over the 80 trials\n    df['best_so_far'] = df['val_accuracy'].cummax()\n    \n    axes[0].plot(df['trial_id'], df['best_so_far'], label='Best So Far', color='red', linewidth=2)\n    axes[0].scatter(df['trial_id'], df['val_accuracy'], label='Individual Trials', color=color, alpha=0.5, s=15)\n    \n    # --- Plot 2: Accuracy Distribution ---\n    # Shows the spread of performance among the 20 individuals over 4 epochs\n    axes[1].hist(df['val_accuracy'], bins=15, alpha=0.7, label=run_name, color=color, edgecolor='black')\n    \n    # --- Plot 3: Training Time vs Accuracy ---\n    if 'training_time_sec' in df.columns:\n        axes[2].scatter(df['training_time_sec'], df['val_accuracy'], \n                        label=run_name, color=color, alpha=0.6, edgecolors='w', s=40)\n    else:\n        axes[2].text(0.5, 0.5, \"Time data missing\", ha='center')\n\n    # --- Formatting ---\n    axes[0].set_title(\"Search Convergence (Run 1)\", fontsize=14, fontweight='bold')\n    axes[0].set_xlabel(\"Trial ID (1-80)\", fontsize=12)\n    axes[0].set_ylabel(\"Validation Accuracy\", fontsize=12)\n    axes[0].legend()\n    axes[0].grid(True, linestyle='--', alpha=0.7)\n\n    axes[1].set_title(\"Accuracy Distribution (Run 1)\", fontsize=14, fontweight='bold')\n    axes[1].set_xlabel(\"Validation Accuracy\", fontsize=12)\n    axes[1].set_ylabel(\"Frequency\", fontsize=12)\n    \n    axes[2].set_title(\"Training Cost vs. Performance (Run 1)\", fontsize=14, fontweight='bold')\n    axes[2].set_xlabel(\"Training Time (s)\", fontsize=12)\n    axes[2].set_ylabel(\"Validation Accuracy\", fontsize=12)\n    axes[2].grid(True, linestyle='--', alpha=0.7)\n    \n    plt.tight_layout()\n    plt.savefig('pso_visualization_run1.png')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T18:08:03.182977Z","iopub.execute_input":"2025-11-26T18:08:03.183236Z","iopub.status.idle":"2025-11-26T18:08:03.192432Z","shell.execute_reply.started":"2025-11-26T18:08:03.183218Z","shell.execute_reply":"2025-11-26T18:08:03.191732Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"visualize_results()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T18:08:03.193444Z","iopub.execute_input":"2025-11-26T18:08:03.193703Z","iopub.status.idle":"2025-11-26T18:08:04.291638Z","shell.execute_reply.started":"2025-11-26T18:08:03.193687Z","shell.execute_reply":"2025-11-26T18:08:04.290807Z"}},"outputs":[{"name":"stdout","text":"Plotting Phase 1 Search Results: 100 trials (20 particles x 4 epochs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 2000x600 with 3 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAB8UAAAJOCAYAAAAu69ZBAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdd3xT1fsH8M9N2rRJJ6WF0kILZVT2UKYIgiDiQBC+gCAOFEUUF7i+gKDi14kDRRGcoCIgyFAUBwg/FAQRsCAUaEuhpS3dK2mTJvf3R801adP2ps3u5/16+TLcpMm5z5O0J+e55xxBFEURREREREREREREREREREREPkjh7gYQERERERERERERERERERE5C4viRERERERERERERERERETks1gUJyIiIiIiIiIiIiIiIiIin8WiOBERERERERERERERERER+SwWxYmIiIiIiIiIiIiIiIiIyGexKE5ERERERERERERERERERD6LRXEiIiIiIiIiIiIiIiIiIvJZLIoTEREREREREREREREREZHPYlGciIiIiIiIiIiIiIiIiIh8lp+7G0DUXJWWluKzzz7Dnj17cPHiRRQUFEChUCAqKgp9+vTBjBkz0KdPH3c3U7bff/8dt99+OwDg9ddfxw033NCk5/vrr7+wbt06HD58GLm5uTAYDAgPD0fXrl1xww034KabboJSqXRE08lDPfHEE9i6dStCQkKwY8cO7Nu3D08//bTNxyqVSrRo0QK9evXC9OnTMXToUBe3tm5btmzByy+/jIKCAgBAcnKydF9OTg5uuOEGlJaW4uabb8Yrr7zirmYSEZEXu/nmm3Hq1CkAQNu2bfHTTz9BEAQ3t8q3WPZ1zQRBQHBwMNq2bYsrrrgCkyZNwmWXXWb1mIyMDFxzzTUAgAcffBBz585t1Otv2bIFFy5csOvnZ8yYgYMHDyI2Nha7du0CAGzevFnqT61ZswYDBw5sVHvqc/HiRWzatAkDBgywev6RI0ciMzMTAwYMwNq1ax3+uvZKSUnB+PHjodfrcffdd+OJJ56Q2miLWq1Gu3btMHLkSMycORNhYWEubrFtOTk5WLp0KX744QcAtd9nr7zyCj788EOoVCps2bIFHTt2dFdTiYiaJfPfYzkc+TfSsu/y4osv4pZbbmnU83ja32+9Xo+tW7fi+++/x6lTp1BcXAw/Pz9ER0fjiiuuwLRp09CtWzeXtefUqVP48ccfMWrUKHTt2tVlr9sYlv3SmgIDA9G2bVsMGzYM99xzD1q2bOmUNuzZswdvvPEGzp49C4VCgSlTpmDBggVOeS36V0FBAcaOHYuioiJcd911eOutt+r93RQQEICYmBgMHToUs2bNQuvWrV3cYtuKi4uxbNkybNiwAaIoYsKECXjppZek+z///HM899xzAIBPPvkEgwcPdldTyYOxKE7kBvn5+ZgyZQouXLgAAIiJiUGPHj1QUFCA9PR0ZGRkYMeOHXj77bcxatQoN7fW9V5++WV89NFHAAA/Pz906tQJSqUSaWlp2Lt3L/bu3YsNGzZg1apVCA4OdnNryRn27duHrVu3AgAef/xxtGrVyur++Ph4qw5ZaWkpzp49i127dmHXrl14/PHHcc8997i0zTWlpaVhyZIlOHDgQJ2Pad26NebPn4/Fixdj69atuPHGGzFs2DAXtpKIiLzd33//LRXEgerBroMHDzql2EnVoqOjERcXB5PJhPz8fJw6dQonT57EZ599hjvuuANPPfWUdFFCQEAABgwYAACIjY1t1OtVVFTg2WefhVartasobi7QR0ZGNup1G+vrr7/GO++8gwcffNDqfdi7d2/ExsbWunDAHURRxMKFC6HX6xEXF4eHHnrI6n4/Pz/069dP+rfRaMT58+dx+vRpnD59Gtu3b8fGjRudNmAsh8lkwmeffYY333wT5eXldT5u7ty5+OGHH3DhwgUsWLAAX3zxBRQKLhpIROQqNf/u5eTkID09HQDQoUMHREVF1fnYpggNDZX6IE3pC3jS3+/s7Gzcd999Ut83PDwc3bt3R1FREdLT05GWloZNmzZh/vz5uPvuu13SpjVr1mDTpk2IjY31+KK4pRYtWqBz584AqvsUGRkZOHv2LM6ePYvt27djw4YNiImJcehr6vV6PPzww9DpdIiLi8P06dOlNpBzvfDCCygqKkJYWBgWLVpU6/7LL79cmnxmMpmQlZWFtLQ0pKWlYfv27fjiiy/cfmHl9u3b8dJLLyEvL6/Ox0ybNg3bt2/HkSNHsGjRImzfvh1qtdqFrSRvwKI4kRt88MEHUkF86dKl+M9//iPdt2fPHsyePRsmkwlvvfVWsyuKf/LJJ1JBfOjQoXjppZekLwg6nQ7vvvsuVq1ahcOHD+OVV16Rrv4i37Js2TIAQPv27TF58uRa999+++247bbbrI6dP38et956K/Ly8vDWW2/hP//5j1tn8Nx9993IzMzEoEGDYDKZ6rz6csqUKfj4449x7tw5LFu2jEVxIiKyy+bNmwEA/v7+UKvVKCkpwddff82iuBNdf/31ePLJJ6V/Z2RkYPHixdi3bx8++eQTBAYG4tFHHwUAREVFNXlW1d69e6HVau3+OXfNuvn+++9tHn/jjTdc3JK67dq1C3/++SeA6qJxYGCg1f3BwcG18mYymbBs2TJ88MEHyMzMxIcffognnnjCZW2uacuWLXjhhRcQHByMmTNnSt+halKr1Zg7dy6eeOIJHDlyBLt27Wp23zGJiNyp5t/jzz77DM8//zwA4N577230DO6GdO3a1SEzuz3l77fRaMScOXNw6tQpKBQKPPnkk5gxY4ZUyEtNTcUTTzyBpKQkvPLKK+jXrx/69u3r1DZVVVXh559/duprOMuAAQOwfPly6d+iKGL16tVYtmwZcnNzsWLFCrzwwgsOfc3c3FzodDoAwI033og777zToc9Ptp06dQrffPMNAOCuu+6yeZHM6tWrERQUZHXM/LuqqKgIy5cvx1tvveWS9try+++/Y/78+VCpVJg9ezZWrlxp83GCIGD+/PmYPn06Lly4gPXr1/N9RrXw8mAiNzhz5ox0e9CgQVb3DR8+HMuWLcPKlSvx+uuvQxRF6b7c3FwsWbIEI0eORI8ePTBw4EA8+OCDVs9ndujQIcyaNQsDBw5Ejx49MGrUKCxatAjZ2dlWj5sxYwYSExMxevRoHD9+HDfddBN69Ohh9bht27Zh2rRp6NevH3r06IExY8bggw8+QGVlZZ3n+H//93+YNGkSevXqhSFDhuD1119HVVVVvXGpqKjAe++9BwBo1aoV3n77basrZtVqNebNm4cJEyZg6NChta5QS0lJwZNPPonhw4ejR48euOKKKzB16lR8+eWXMJlMVo9NTExEYmIiFixYgNTUVMyaNQuXX345+vbti7lz5yI/Px9A9fKPl112GRITEzFnzpxabX7++eel5zp+/DiA6o7kunXrMGnSJPTt2xe9e/fGLbfcgq+++srqZ3///XfpZ7ds2YKlS5eif//+eOaZZ6TH7NixA+PHj0fPnj1x5ZVX4rnnnkNhYaHUprffftvqOZOSkjB37lwMGTIEPXr0wPDhw6UOjKWRI0ciMTERd955Jy5duoRHH30UAwYMQK9evXDnnXfi/Pnztc716NGjeOCBBzB48GB0794dV111FZYsWYJLly7VeuyOHTtw22234fLLL0fPnj1x/fXX46OPPmrwPQAABw8exN9//w2g+v0pd/nXuLg4jB07FkD11afm59i8ebMU599//93qZ8zv/8TEROmYZV62bt2K3bt3Y+LEiejVqxcGDx6MV199FQaDocH2REZG4o033sCnn35a78wwQRCkAv+pU6dkL6tGRERkMBikAY4hQ4Zg5MiRAICdO3fWOXO0qKgIr7zyCsaMGYMePXqgX79+uOOOO7B///5aj83KysLixYulvueAAQNw//3348SJE1aPM/crZsyYYXXc8m+qZZ+lof6nTqfDG2+8geuuu0563cmTJ0vnasloNGLt2rWYOHEi+vTpg549e2LcuHHYuHEjjEYjAGDdunVSOzZu3Gj183q9HpdffjkSExMxadKkeuNdl7Zt2+K9995Dp06dAAAffvihdC4ZGRk2YyCKIr744gtMnToVQ4cORc+ePTF8+HDMmzfPauZ/YmKi1ezwxMREKc+WfZz9+/fjscceQ9++ffH+++9bxdn8+JpEUcSHH36IMWPGoGfPnhgxYgTeeecdKW4A8Pbbb0uvkZGRYfXz5rybn9/82NOnTwMA3nnnHSQmJkoXbtT1PtHpdFi5ciXGjx+Pvn37olevXrj22muxdOlS5OTkWD32qaeeQmJiIrp37w69Xo/XXntN6vvfcMMN+L//+796c2X2ySefAKi+aMHcf2yIQqHArFmzpH8fO3asViwa8xkoKyvDM888gyFDhqBnz56YNGkSkpKSGmyP0WjEuHHj8P3332P69On1Pnbs2LHS96pPP/1U1vkSEZF7OKqfZPk3yPy3GLB/PMjW3zjL/kFWVhY++OADjB49Gj169MDo0aOxZcuWWue1f/9+TJ06Fb1798bAgQMxf/585Ofn46qrrkJiYiKeeuqpeuPyww8/SH3Q6dOn484777TaVjEhIQHvv/8+EhIScMstt1j1Z4DqWaYzZsxA//790aNHDwwdOhSPPPKIzb+5SUlJeOihhzB69GipvbfeeqvVuN5TTz0lzVIHgKefftrmuJPZHXfcgcTERHTr1k3aXs/s+PHjUjzNF0zI7Ss6iiAImDlzJvz9/QEAR44cke5zxDjnU089ZdUnfffdd2vl/ejRo3jooYcwdOhQ6b1955134rvvvrN6Hcv+tXny1KBBg6TVASzfn5mZmXjttddw5ZVXom/fvrjzzjtx7tw5VFZW4tlnn8XgwYPRq1cvzJ49u1ZeGvNZkzuOKOc7jFlZWRlee+016fvb5ZdfjpkzZ+Lw4cOycmvu+wUEBGDKlCmyfgYAbrvtNqlQbtnvres7hmVeLPPqiP670WjEsGHD8M0330gXH9fliiuuQPfu3QFUF/Zr1gSIWBQncgPLZZ8ff/xx/Prrr9Dr9dKx66+/HiNGjEDnzp2lgmB2djYmTpyIdevW4dKlS+jSpQv8/Pzw448/YsqUKVaF8QMHDuCuu+7C3r17IQgCunbtiuzsbGzYsAGTJ09GWVlZrTZptVrMmzcPZWVlVgXCF154AY8//jgOHz6M4OBgdOrUCefOncOrr76K+++/32aR88iRI3jggQdgMBigVCqRn5+P999/H0uXLq03LocPH5Y6kzfeeCM0Go3Nx7300kv48MMPcccdd0jH9u3bh1tuuQVbtmxBfn4+OnfujMDAQBw5cgSLFy/GQw89ZHWBgVl2djZuu+02nD9/HsHBwdBqtfjhhx+kAa+YmBj0798fAPDrr79KVzQC1bNFdu7cCaB6gLJHjx4Aqv/YL1myBElJSWjZsiXi4uJw4sQJLFiwAO+8847Nc9q4cSPWr1+PuLg4tGjRAkB1h//RRx/FyZMnYTKZEBkZiW+//RZz5861eS779u3Drbfeih9++AF6vR6JiYkoKSnBZ599hhkzZli13aykpAR33nknjhw5gvDwcFRWVmL//v2YPn261XvSPNj2008/wWAw4LLLLkNxcbHUKbYcsFy+fDkeffRRHDp0CEFBQdJ75uWXX7a5RE9N5s6uIAh2701v+X4MDQ2162dt2bt3L+bOnQuDwQCVSoWCggJ88MEHeP311xv82c8++wzXX3+9rNe54YYbpM+6+T1FRETUkN27d6OwsBBAdf/R/HdHq9Xa/HtSUFCAyZMn48MPP8SFCxeQkJCAoKAgHDhwAHfeeSe+/vpr6bHnzp3DLbfcgi+//FLqewqCgF27dmHy5Mk2i+j2stX/NJlMePDBB7Fy5UqkpaWhU6dOCAoKwrFjxzBv3jx8/PHH0s+LooiHH34YS5cuxfHjxxEVFYV27dohOTkZCxcuxMKFCwEA48aNkwZ0as5i3r9/v9Q3bmxRHABUKpV0kZvBYMCPP/5Y7+NfeuklPPvsszhy5AhCQkLQo0cPVFVV4ZtvvsHUqVOlAdoBAwZYzeYYMGAAevfuXev53nnnHezevRudO3euNcujLqtXr8Y777yDoKAg+Pn54eLFi3j77belFXvsFRsba9W22NjYWu2vqaysDLfeeiveeOMNnDx5EhEREYiPj8eFCxewdu1aTJgwAampqbV+rqqqCgsWLMD69esRFhYGk8mEs2fPYtasWQ0OFBcUFEgXIV533XXS4K8cloOUISEhsn+uLhUVFXjggQfw008/oUWLFtDr9UhKSsKMGTOki3TrcvPNN+PVV1+1uoi4LiqVCmPGjAFQffF0zUFfIiLyPE3tJzVE7nhQQ15//XWpPwFUr+L35JNPYvfu3dJjjh07hlmzZuHIkSOorKxEVFQUDhw4gNmzZ6O4uFjW6/z000/SbVsrCgJAy5Yt8d133+HFF1/EFVdcIR3/73//i/nz5+PgwYNQKpXo0qULSktL8d1332Hq1KlWRdekpCTceuut2LlzJ0pLS9G1a1e0atUKf/75JxYsWIBXXnkFQHUR3nJJ+Q4dOmDAgAF1jkWNHz8eQHVfwjI2AKxef+LEiQDk9xUdyWQySQXEgIAA6bgjxjkTEhJs9hMTEhIAVF/sOW3aNOzcuRNlZWXSd4/9+/fjkUceqXPW+q+//oq33noLbdq0qbX1IgC88sor2Lx5MwIDA6HVarF//348/PDDePbZZ/Hjjz9Co9GgsrISu3fvxuOPP24Vi8Z81uSMI8r9DgP821devXo10tPT0alTJ4SGhuLXX3+t88JmS0ajET/88AMA4Morr0RERES9j7ckiqI0/uyIfm9T+u9XXHEFVq9ejfj4eFmvdeONNwIALly4IE2aIjJjUZzIDaZNm2Z15d3MmTOlWc0vvfQS9u7dW+uqsNdeew05OTnw8/PD559/js2bN2PXrl0YMGAAysvL8fLLL0uPXb16NQRBQFBQEHbs2IGNGzdKBemcnBzpj6GlvLw89OjRA7t27cKmTZsQHR2NI0eOYM2aNQCAa6+9Fr/88gu2bNmC//3vfwCqOx62Zuxs27YNGzduxNatW/HDDz9IA2HmPaLrkpKSIt1u3759Q2GUVFRU4Mknn0RFRQUiIyPxzTff4Ouvv8aePXtw8803AwB+/PFHm6+/b98+3Hnnndi5cyd27dqFq666CgBw4sQJ6QrUCRMmSK+zd+9e6WcPHz6M3NxcAJCWu/rtt9+kK2JnzJiBn376Cdu3b8eSJUsAACtXrkRWVlatdhw7dgwbN27Epk2bpCvezB0mhUKBTz75BFu3bsXPP/9s88uJ0WjEM888A4PBgNjYWPz444/YtGkTvvvuO4SHh+P06dM2l806ceIE+vfvj927d+OHH36QlvK/dOkSfvnlFwBAeXk5Fi9ejKqqKsTFxUnP/fXXX0OlUiEnJwdvvvkmgOrlqsxL2IwaNQq7du3C119/La0AsHnzZqurC235448/AACdO3eWLhCQIy0tDTt27AAAtGnTxiH7En3//ff45JNPsG3bNvzwww9SB3vDhg0N/qxKpZL9OhEREejQoQMAyL7Sk4iIyDzrR61WY/To0VYDHZYFbrM333xT2sNy+fLl2LZtG3bt2oUhQ4YAAJ577jlpme7nn38eBQUF8PPzw5dffin1PTt16oSqqiosWrTI5kV69rDV//zzzz9x8OBBqFQqzJw5E1u2bMEPP/wg/Z207M98++23UvH5jjvuwI8//ogdO3bggQcekOJjvkjvpptuAlB98aj5QgIA0pKXarVaGjxprF69ekm3ba3kZMk8Y33SpEn47rvvsG7dOuzevRvDhw9Hly5dpKW9165dK/VPzf+2tYxpamoqvv32W2zYsKHWFjN1SU9Px/fff4/Nmzfju+++k/rsa9askT1AbemWW26xGvCbMGEC1q5dW+/WMG+++SZOnjwJoHp52Z9//hnbt2+Xvs/k5+fj2WeftfmzaWlp2L17N7Zt24aXXnoJQPXgXc1ZSzUdOnRIum2++FUOk8kk9WkBOGTLm0uXLkGtVuOXX37Bt99+i0ceeQRA9cykbdu21fuz9vQ1geq9IoHqGJnfX0RE5Lma2k9qiJzxIDn+/PNP7Ny5E1u2bLFajWT9+vXS7eXLl0szZV9++WV888032LVrF6KioupdhdKS+SI5QRDsGjP84YcfsGnTJgDVF5H+3//9HzZv3oxvv/0WrVq1kvq15lWWtm7dCoPBgKCgIOzevRtffvkltm/fjmeeeQZdunRBWloaDAYD7r33Xvz3v/+VXufee+/F2rVr69xX/Nprr5Um/9S8eNJ80eZll12Gbt26AZDfV3QUURSxYsUKaTx66NChABw3znnvvffa7Cfee++9yMnJweLFi2E0GtGxY0f8+OOP2Lx5M/bs2SN9T1mzZo00Xmjpjz/+wHvvvYevv/4aL774Yq37z507h59++gk//PAD+vXrB6B6lcZTp05Jx83L7O/bt0+6KLGxnzU544hyv8MA1WP85lWY3nrrLWzZsgU///wzbrzxRhgMhga3FT158qR0AbA9/V4A+Oijj6Tvho7a6rGx/ffG9nsBjrNSbSyKE7lB9+7dsXbtWumPMQBUVlbiyJEj+PjjjzFr1ixcd911OHr0KIDqgqd5wK5Hjx7SlXUBAQHSlYa//fYbSktLAVQv2ZiUlIQ///xTGhi13Efn4sWLNts1a9Ysq6Wqv/32W+n2zJkzoVBU/8q4+eabsXLlSqxcudLqqkizcePGSVexRkVFYcSIEQCqr3Ktb8aD5TKf4eHhdT6upn379iEvLw9A9f7M5s6xUqnEww8/LD3O1v6GLVq0wMyZM6XHW+7lZF4yasyYMVCr1QCsO67mKzn9/f0xbtw4ANazfKdNmybdnjRpEpRKJQwGA3bt2lWrHcOHD7eKZUFBAdLS0gAAAwcOlDouwcHBVss2mh0/fhyZmZkAqmcdm4vJ0dHRuPrqq2u1zUwQBDz22GNS3i33tzef/2+//SbN4J80aZL03B07dsTHH3+MlStXShcf/PTTT1IHeurUqfDz85POLyYmBkDd+0wC1R0h88URtt5bZmvWrMGMGTOk/yZMmICbbroJxcXF0Gg0+N///md3h8mW4cOHS1cXR0RESEsDlZWVOXyGjfmL09mzZx36vERE5Jvy8vKkpeauueYaabaveZWVQ4cO4cKFC9LjTSaT1Hdp3769tKewv78/nnvuOWnrHr1ej6KiIvz6668Aqrf6Ma+GExQUhLfeegsrV67EggUL7JpFVJea/c8rrrgCSUlJSEpKkvbt9vf3l9pg2Y+17Kvec8890u2ZM2dKfVXzLNpbb70VQPUMBfMFoqIoSv2ya6+9FsHBwU06F8ufr2v5ejNz3/LXX3/Fpk2bcOHCBahUKqxatQobNmywWhFJjptuuknqa8l12223SStYRUdHSxcOGAwGl8yoEEUR27dvB1A9W8hyWdahQ4dK/d/ff//d6kIGs9mzZ0sxv/HGG6WY2toGyJLlBQt1DVyXlZVZ9TWnT5+Oq6++WhoAHTFiRJ2z1Oz18MMPS/1WW31xR7HsW7O/SUTkHZrST2qInPEgOWbMmCH1Jy6//HJp5q9lP9RcwI2Li5PGj1QqFR588EHZr2PuW6nVarvGeywnyDz22GPSJKW2bdtKS0mXlpZKfd/AwEDp9VasWIFjx45Br9dj+vTp2L59O9577z27VpkxCwoKwujRowFUj7OZzycpKUnaosZyTNLRfcWaDh48aNXPGTZsmDTJpVOnTtLYo6PGOevz/fffS98r7r77bqn/HhgYiPvvv9/qcTV17dq13oLtrbfeCo1GA6VSKcXffC6BgYFQKpW49tprpePmsdXGftbkjCPa8x3G/L0lKipKWvVHoVBIn9fU1NR6L8a17PPV1e8Fqn/XmN8Pt912G6655hppVYTevXtj9uzZdf6sPRrbf7eX5bmy30s1+bm7AUTNVd++fbFu3TpkZWXh0KFD+Ouvv3DkyBGcOHECoiji/PnzuO+++/D999/DaDRKV2YdPXrUanlzM6PRiLNnz6Jv377Izs7GypUr8X//93/IycmptW9JXbN6ai5BYp5JBFR3XM38/PykQrct5v0UzSyX9LO1hLeZ5RJD5iK3HObiMQCp820WGxsLlUoFvV5vdT5m7du3lwq3AKyWkTG31dxx3bZtG/bs2SMtC2/umFx99dXSz5k7TwDq3J8wOTm51rGasbfc071mPGvupV7zdVetWoVVq1bVeoz5ykJLkZGRCAsLk/5t6/zPnTsnHWvXrp3Vz1suR1WzHZYdu4baYVZWViYV1S3bVVN6errNfA4bNgzPPvus3YPCdakZ+5YtW0q3tVqtXcsONcR8sYHBYEBZWVmTB+aJiMi3bdu2Tdo25MYbb5Ru33DDDVi7di1EUcSWLVuk/agLCwtRUlICwLpfB1T/fbf8G//XX39J/cWaf/s7depU6+9jU9haAm/79u1Yt24dUlJSpAvzzCz7sea+QGBgoNVyicHBwbX6qpdddhn69u2LI0eO4LvvvsOUKVNw9OhRadWfpiydbmZZuK2vHwMACxcuxOOPP46srCxpllFUVBT69++PKVOmYNCgQXa9ttylBC3V7FNa5tqyL+osBQUFUn47dOhgNehvPnbw4EHpu1HNFYQs269QKNCiRQvodLp6v28AkD4HQN15qqqqkpZYt6RUKvHaa6/huuuuky4YbirL87DsazZ0HvayjF/NzxUREXmmpvSTGiJnPEgOW+Mmqamp0nMUFxdLY5o1+x729CnNyzdrtVrodDqpmNYQ85ihSqWq1a+1nHFuHvu644478PPPPyM1NRWrV6/G6tWrERAQgJ49e2Ls2LGYOHGi7NeuacKECdi6dSsqKyuxd+9ejB07Vir0+vv7SxcoAo7vK9ZUWFho1dcJCAhAp06dMHr0aNxzzz3SmJSjxjnrU9+4rq0c2fM6ljm3fL/HxsbaPG753m/MZ03OOKI932HM8c/NzbVZDwCq41/XapmWqz/V9/2krtnUCxYswNSpUx0y8QhofP/dXiqVChqNBlqtlv1eqoVFcSI3a9OmDcaNGyfNND537hzmzp2L06dPo6ioCIcOHbKaUd6iRYs6/9ApFAqUl5fjjjvukDoK7dq1Q+vWrWEwGBpctrpmp87yD7x5Txk5av6hlDtgZHkFYX2F0/z8fOkPZ00122m5/4mtdtS8urPmYJzZ+PHjsW3bNpSUlODAgQMICAiotXR6TZdffjmUSmWt47b2uKkv9jUvamhIfHy81b71lvR6vVV+aubK1vlbtsWeL1g9evSwuS9827Zt6/wZy1lV9e2HuWjRImlp0JKSEowdOxZ5eXk4ceKEdFWvLTXb31Bs5b4/HKHm7DIWxYmIqD6Wy6PXdeX+li1b8OCDD0IQBLv6dY3tA9b8O2su1NenZh9o/fr1eOaZZwBUDxT16tULgYGBSE1NrXXRpPn15PZPpk6diiNHjuDgwYPIz8+X9qaMj4+3ezlBWyyXsaxvJgZQPajYp08ffP311/j9999x/Phx5ObmYseOHdixYwcWLlxoNXO6IY0ZnK3Zr7HcvslWH9befpQ9bL3PLNtjqw/W2H6anP5meHg4fv/9d+nfq1atwrJly2A0GpGamlrn95vGfAYs++Ou6mual9EkIiLP1pR+UkPkjAfJ0dDfY8u/jXL+Ltala9euOH78OIDqMUPL/aktpaamIj4+vlZfxjw+aNk+y76G+W97VFQUtm7diu+++w579+7FkSNHkJmZiT/++AN//PEHvv32W3z66aeNKhIOHDgQbdq0QVZWFn788UeroviIESOsLkxwdF+xpjFjxmD58uV2/UxTxjnlqtkntPy3rf5XQ69jORHKMveWz2Xrvd/Yz5qc/qm932EAQKPRSLPUa6pvHFTuOOuff/4p3f/tt9/iscceA1C9/Hpd7/XG9HtdPc6q1WrZ76VaWBQncrGcnBx8//33SElJQb9+/aTlz83at2+PyZMnS3uAl5aWIiIiAkFBQSgvL0dCQkK9ewT9/PPPUkF8woQJ0v4cJ06cqLN4WxfLq+nS09OlGd8mkwkffvghRFFEfHy8tHxLU/Xu3RuxsbHIzMzEtm3b8Mgjj9gsfC9duhQ//vgjRo0ahRdeeMHqKsKaS6KcO3dOGrCrebWhPQYPHozWrVsjJycHP/74o9SpioqKslqmxzJmL730Uq2ZWHJZFrVrLiFjufe6mWWh+frrr5f2I3SEmu8DS9u3b0dWVhZUKhXuvPNOq8fOnz8fgwcPtuu1LDtoDS07ahYaGoqnn34a8+bNQ35+PpYuXWq1T5Fl59Dy6kCj0ejw5XmawrKTVl9HlYiI6Pjx4/VeQGiWkZGBgwcPYuDAgVb9yZp//5KTk7Fnzx4AwOjRo63+ntd87B9//CEVf2+55RZERkYiICAAQO3Zp5azPuT68ssvAVQPkHzzzTdSW2bPno3du3dbPbZdu3ZITU1FZWUlsrOzER0dDaC6/7xu3ToA1RfpmfciHDt2LF588UUUFRVh586d0vZEt9xyS5MHZLRaLT777DMA1YNW11xzTYM/06ZNG8yZMwdz5syByWTC8ePH8d///hdnzpzBO++806SBTjnOnj0r7RcJWOe6TZs2ACDlFqjOrzkfZWVldg++1xQREYHw8HAUFRUhNTUVJpPJaoDS3OdVKBR27R3akJr9zYZm9QPAXXfdhW3btuHMmTNYuXIlRo8ebTVbx5GfAWex7Gvy4ksiIu9kTz/JU4SHhyMgIACVlZW1+pX2LGs8duxYaZ/tTz75BG+88Uatx+Tl5WHy5MnQaDS47777MH36dCQkJCAlJQUGgwHnzp2T9oQGrMfXLMcMVSoVbr75Zmmp99zcXKxcuRKfffYZ/vzzT/z666/1rqBZF4VCgZtuugmrVq3Cnj17cPjwYWnp9IkTJ9Z6vLv7ioDjxjnrU3Nc13I/aMv3SFPGde3lzM+aPd9h2rVrh7NnzyI0NLTeekBdGjPOesMNN2Dz5s3Yt28fNm/ejOuvvx5XXXWVdL9lv9fyQhNP6vcC//Z92e+lmrinOJGLCYKA119/HevXr8eLL75Ya1m+4uJi7NixA0B1Z+nyyy+HQqGQ9iA5cuSI1SyUDz/8EHPmzMEzzzwDg8FgNaBnXqKlsrISK1askI7LXTbkuuuuk25/8MEH0hWU27dvx2uvvYZly5ZJnTdH8PPzw/z58wFUDyzOmTPH6vkrKyvxyiuvYMeOHTAYDDAajQgKCsKVV14pFew3btwodbL1ej3efPNN6efNndnGUCgU0mz+Xbt2SUunjxs3zuqqQ8v9aT766CPpqrnk5GTcc889mDdvnrRXfH0iIyOlpXx+//13HDlyBEB1B8nW0ujdu3eXlgzftm2bNIvdYDBgwYIFeOihh/Dhhx/ae9oAgKuuukrqQHz11VfSvvCpqalYtGgRli1bhv379wMARo4cKQ1mfvrpp9IFCZcuXcJ9992Hxx57rN7OY3BwsHTVqeUSPw258cYbceWVVwKovqLRPPMLsL5g4JtvvpFuf/HFF/Xuce9q5iVX/f392WEjIqJ6bd68Wbq9atUqJCcnW/1nHjAE/p1RrlAopP3yzp8/L/U3q6qq8Oqrr2LZsmVYvnw5NBoNIiIiMGDAAADW/RCtVovnn38ey5Ytw4cffigtY2n+W3v27FmcPHkSQHV/szEDN+a+rJ+fH8LDwwFU73m4b98+6THmv5mWfdX3339fur127VosW7YMy5Yts1oqOyAgQLpI9KOPPkJaWhqUSiUmTJhgdzstZWdnY/bs2dLyhnPmzKl3i5Xk5GRMnjwZgwYNkvr1CoUCvXr1srmMqOWMHMv9OZtqzZo1Up/xwoUL0v7eGo0G3bt3B2DdjzLfDwDvvPOOzdndlv3ihr4nCIIg9a+zs7Px+eefS/f99NNP0vtu5MiRVtssNZXlc8ntb/r7+2PJkiUQBAEGgwFPP/201WwYR34GnMVyeX/zZ4uIiLyLPf0kT2Ke1Z2eni71QfV6vV0zla+88kppUsqOHTvwzjvvSPtQA9UFuXvvvRelpaXIycmRLvCznIy0fPly6e93Wlqa1GeOioqSxpTmzZuHa665Bs8++6z0czUnxJjH+iz7aHLHR83tKSsrw/PPPy89v2XR0d6+4h133IHrrrtO2iPdkRw1zlmf6667Tiq0fvLJJ9Le22VlZXjvvfcAVL/3mzKuay9nftbs+Q4zatQoANV9Zcu9yLdu3YrZs2fjqaeeqnec3/LiT3vGWRcvXizlZNGiRVYXV5r7vVqtVrrIuKKiwuZ4tbvo9Xpp2wb2e6kmzhQncrFWrVrh2WefxcKFC1FUVIQZM2YgLi4OUVFRKC8vl65eBIAHH3xQmhUxb9487N+/H3l5eZgxYwY6deoEvV6P1NRUAMDTTz8Nf39/9OnTB2FhYSguLsZHH32EAwcOIDMzEwkJCRgxYgR2796N9evX4/jx41i/fn29bR04cCCmTJmC9evXY/fu3bj66qvRsmVLaaCnZ8+e0hLWjnL99dcjNzcXL7/8Mv7880+MHj0anTp1gkqlQmpqqvQHbcCAAfjf//4HoHom8Msvv4w5c+agoKAAN954Izp16oSsrCypIzVlyhSpI9FY48ePx+rVq6XBQ6D2lZxDhgzBjTfeiG+++Qbr1q3D7t270bp1a5w8eRJ6vR69e/e2Wia+Pg8++CCefvppGI1GKedZWVlWV7WaKZVKLFmyBHPmzEFmZiauvfZa6fG5ubkIDAzEHXfc0ajzDg4OxuLFi/Hkk09Kz92+fXucOXMGlZWVCA8Px1NPPQWg+qrNe++9FytXrsTu3bsxfPhwtGvXDmfOnEF5eTni4+OxcOHCOl9LEAR07NgRp0+fxqlTp+xq5+LFi3HTTTehsrISS5YsQf/+/REWFoaePXuiW7du+Pvvv/Hjjz/ipptuQlBQEM6dO4eBAwdaLY3pSJZfmCwH5C2Pb9iwQboi1Py5qmt7BCIiIqD6C755QCQmJsZqAM2sV69euOyyy3Dq1Cns3LkTixYtQlBQEObNm4eDBw8iMzMT8+bNw8qVK1FUVIScnBwAwGOPPSatVrN48WJMnz5d6q927twZWVlZKCwshEKhwJIlS6SBkilTpmDv3r0QRRHTp09H7969cerUKfTv39/m3n/1GT58OE6cOAGDwYCxY8ciIiICKSkpePjhh7Fs2TIA1ft/z549GxMnTsT333+PPXv24IsvvsDevXsRGBgozSgZOXJkrRWNpk6dio8//lgqLl911VV1bjtTlx07dkjLd5aWluL06dPSxaMzZ87ErFmz6v35zp07QxRFFBYWSrHVaDTIzs6WCuuW/TbLvfemTJmCmJgYfPXVV3a12ZbWrVvjuuuuQ3x8vNXen/fff7+0Bc7IkSMRFRWF3NxcfPrppzh69Kg0yGR+j1mKiopCSEgISktLsWXLFpw9exYTJ07EtGnTbLbh4YcfxqFDh3Dy5EksXboUn3/+OQRBkGaatG3bVlq60lG6dOki3T558qTsGU9XXHEFJk2ahI0bN+LEiRP48MMPcd999wFw7GdAro8//hgff/wxAOulRT/++GNpkH/s2LF4+umnAcAqV+xvEhF5J3v6Sc6Y0dtYc+bMwR9//AGTyYR58+bhvffeQ2FhYb3b69mybNkyPPzww/jtt9/w9ttv4+OPP0ZCQgJKSkpw4cIFGI1GKBQKPP7449LkolGjRmHSpEn46quvsGPHDhw6dAhRUVE4c+YMDAYD1Go1Xn31Valf27dvX3zzzTf44osv8Msvv6BNmzbQarU4c+YMgOq/oeYCevv27aFUKmE0GvH+++9j3759uPfee+sdf+zYsSN69uyJpKQkaRzo5ptvtiqw29tXvHDhAjIzM51S/HPkOGddzGPlCxYsQGpqKkaNGoUOHTogPT0dpaWlAIBHH320we2JHMmZn7Xx48fL/g4za9Ys/PDDD0hNTcW8efPw7rvvQqlU4vTp0xBFETNmzKg375Z9vpMnT8pe0TMuLg73338/3nzzTWRlZeGVV17Bc889BwCYPHkyvvzyS4iiiEcffRT9+vVDeno6evXqZVcc7PHdd9/hxRdfrHX8+++/x2+//QYA6NOnj3ShjfmzBbDfS7VxpjiRG4wfPx5btmzBtGnT0LlzZ+Tl5eHIkSNIT09HbGwsJkyYgM8//xwPPPCA9DNt2rTBpk2bMHnyZLRq1Qpnz55FTk4OBgwYgOXLl+POO+8EUL0M4cqVK9G3b18EBgYiKysL1157LVatWiUV2UVRlIrLDXnuuefwwgsvoHfv3igrK8PZs2eRkJCAJ554Al988YXVkoqOcscdd+Cbb76RljnKzMzEqVOnoFarcdVVV2HZsmX49NNPpdlJQPUVo5s2bcK4ceMQFhYmDU4OHjwYb7zxhvSHuyk6depktX9L7969rQYpzV599VUsXLgQ3bp1Q3FxMf7++2/ExsZi9uzZ+Oijj+rd68XSLbfcgueff166MKKwsBATJkyQriQFrPdeGT58OL744guMHDkSAQEBOHHiBEwmE8aMGYN169ZZLT9kr3HjxmHNmjW4+uqr4efnh1OnTiE8PBzTp0/Ht99+a1Wof/TRR/Hqq6+ib9++0Ol0OHHiBMLDwzFjxgx88cUX9c6cAqoHHAHgzJkzdl11GR8fL+2pmpubK21BIAgCVq9ejdGjR0Oj0SAzMxNBQUH47LPPrJaBcrScnBzpP/Mgc83j5iuUCwsLpcHffv36Oa1NRETk/Xbt2iXNBpg4cWKdextPnjwZQPUV/Dt37gRQXbD86quvcPvttyM2NhapqakoLy/HkCFDsHbtWsycOVP6+U6dOuGrr77CxIkT0bJlS2ngZdSoUfj6668xduxY6bGjRo3Cyy+/jHbt2kGv1+P8+fO46667pIvm7HH//fdj5syZaN26NUpKSuDv7493330X9957L6ZOnQqNRoPi4mIolUooFAqsWLECTz75JLp27Yq8vDxcuHABXbt2xQsvvIAVK1bUWhY9Pj5eWk7dHEN7ZWdn4+DBgzh48CBSUlIQHR2N8ePHY8OGDXjyyScb/HmFQoE1a9Zg7ty56NSpEzIzM/HXX3/BZDJhyJAheOONNzBnzhzp8VOnTsW1114LjUaD8vLyJu3Habl35sKFC3HXXXehqKgIVVVVaN++PRYtWoR7771XekxgYCA+//xzDBo0CAEBAUhLS0N8fDzWrl1rc5sjpVKJF198EW3btoWfnx+ysrLq7fsGBwdj3bp1ePjhh9GlSxdkZWXh4sWL6NixI2bPno2vv/7a7osWGmLuawLAoUOH7PrZ+fPnS33Zd955Rxq8dORnQK6ysjKpT2l50W55ebl03HJG0OHDh6Xbffv2dVq7iIjIeezpJ3mSwYMH4+2330ZiYiKUSiXy8/MxbNgwvPvuu9Jj5GxlExoaio8++ghvvfUWRo0ahaCgIJw6dQpZWVlo164dpkyZgq1bt1r1aQHghRdewCuvvIIrrrgCOp0OZ86cQVRUFCZOnIivv/7aqlB42223YcWKFRg2bBgMBgP++usvZGVloXPnzpg9e7bVWGhkZCQWLVqEqKgoCIKAnJwcWWN+NVcpqrndpb19RWdz1Dhnfcxj4aNHj0ZgYCBOnToFPz8/jBgxAh9//LF0IaKrOPOzZs93mODgYKxfvx533XUX2rVrh/T0dJw/fx69evXC888/jwULFtT7Wpdddpm0GqW9/d67775bGvdev369tEpot27dsGrVKnTq1AmiKCI1NRXXX389Xn31VatVoxxJp9NZjafaOm45hmzZ7+U4K9UkiOY1L4iIyCscPnxYmm2zaNEih8/Wd7dDhw5J5+SL52fL2rVrpSL+2rVrpSVriYiIyPFmzJiBgwcPIjIyEr/88gv8/f3d3SRysdtvvx2///47oqKisHv3bp9/D+j1eowcORK5ubkYMGCARy3rTkREzVdOTo60mt7MmTNlXVxIRPZ5+umnsXnzZgQEBOCXX35pcLKSL7jllltw4sQJxMXFYefOnXVeSE7NE98NREQe6q233sKYMWPQv39/pKSkAKie3fPFF19Ij+nfv7+7muc0/fv3l/axXLNmjc39Kn2JyWSSBia7devGgjgREZETffXVVzh48CCA6tWJfL0YSraZV9nKzc212p/RV+3YsUOaTd7YLZWIiIgaa926dbjhhhvQr18/abYpUD3mY+aL41tEnsDc762srMS6devc2xgX+OOPP3DixAkA1Ss/sCBONfEdQUTkoYYMGYKsrCyUlJRg/Pjx+M9//oORI0fim2++AVC9d2FiYqKbW+kc8+fPBwCkp6dj/fr1bm6Nc3355ZdIT08HUL2XKxERETne3LlzMXToUGmJwU6dOuH22293c6vIXUaMGCEto/7OO+9YbXXja3Q6Hd5++20A1ctHXnPNNW5uERERNTcDBgxAXl4eysvLcffdd2PixIkYPXo0PvjgAwDAVVddhREjRri5lUS+KTExETfddBMA4JNPPrHadsfXmEwmvPbaawCq90WfMmWKm1tEnohFcSIiD9W/f398+eWXuOGGGxAVFYWTJ0+irKwMl19+OV588UU8++yz7m6i0wwZMgTjx48HACxbtgyXLl1yb4OcJCcnB6+//joA4Oabb8ZVV13l5hYRERH5pqqqKhQUFCAiIgLjxo3DJ5984pD9D8k7CYKA5557DiqVChcuXJCKxr5o+fLlyMjIgL+/P5YuXSprz1YiIiJH6tixIzZs2IDJkycjNjYWZ86cQV5eHnr06IGnn34a7733Hv8+ETnRf//7X4SHh6OkpETavtEXrVu3DkeOHAEAPPfcc/y+RzZxT3EiIiIiIiIiIiIiIiIiIvJZnClOREREREREREREREREREQ+i0VxIiIiIiIiIiIiIiIiIiLyWX7uboCzVFVVobi4GAEBAVAoWPsnIiIiagqTyYTKykqEhYXBz89nu5B1Yt+SiIiIyHHYt2TfkoiIiMhR5PYtfbbXWVxcjHPnzrm7GUREREQ+pX379mjZsqW7m+Fy7FsSEREROR77lkRERETkKA31Ld1eFM/MzMSzzz6LY8eOQaPR4Prrr8e8efNqXSVpMBjw7rvvYtu2bcjPz0evXr3wwgsvoF27djafNyAgAEB1ANRqtUPaajKZkJmZidjYWF7F6UWYN+/EvHkn5s07MW/eydV50+l0OHfunNTHam6c0bf0dUajEadPn0aXLl2gVCrd3RyywNx4LubGczE3nou58Vz15cab+pbOGLusq2/J72byME7yMVbyME7yMVbyME7yMVbyME71k9u3dHtRfO7cuejevTt++ukn5Ofn47777kNkZCTuuusuq8etWrUKW7ZswXvvvYf27dvj/fffx5w5c7B161abbwDzMbVaDY1G45C2mkwmKJVKaDQavum8CPPmnZg378S8eSfmzTu5K2/N9T3ijL6lrzMajQAAjUbDIoWHYW48F3PjuZgbz8XceC45ufGGvqUzxi7r6lvyu5k8jJN8jJU8jJN8jJU8jJN8jJU8jJM8DcXGrZFLSkrCqVOnMH/+fISEhKB9+/a48847sX79+lqP3bVrF/7zn//gsssuQ2BgIObOnYuCggIcO3bMDS0nIiIiIiIiIiIiX8axSyIiIiLf4dai+IkTJxAbG4uwsDDpWPfu3ZGWloaysrJajxcEQbqtUCgQHByMkydPuqSt5tdPSEiwagd5PubNOzFv3ol5807Mm3di3oiIiIiInMvVY5fs48vDOMnHWMnDOMnHWMnDOMnHWMnDODmGW5dPLyoqQmhoqNUxcyezsLAQwcHB0vERI0Zg/fr1GDlyJDp06ICNGzciOzsbxcXF9b6GyWSCyWSS/q1QKKz+DVS/mQRBaPC4KIowGAxQqVR1Pt78ODnHFQoFRFF0yPHGnpOc475wTgaDAf7+/tIxXzgnOW335nMCqvfj8vPzs/pF783n5It5qtl2y9+TvnJODR33hXOq+XvSF87JF/Nk67it35POOidbv6eJiIiIiHyZs8cua45bCoIAvV4vfTczH+N3qLrHaH3lnBpqe2PPyZ7v+95yTs7IEwBUVVVBqVTWGof01nNyVp7Mn7+AgACfOaemtL2+c6pZk/CFc2pq21kHaPw5mT97/v7+UCqVPnFOco87ctzS7XuK1zzBusyaNQtFRUW4++67YTKZMGnSJPTv37/B/aIyMzOlx4SGhqJVq1bIy8tDSUmJ9JiIiAhEREQgOzsbWq1WOt6qVSuEhoYiIyMDer0eoihCq9WiY8eOCA4Oxrlz56wCHRcXBz8/P6Smplq1ISEhAVVVVTh//rx0TKFQICEhATqdDhcvXpSOq1QqxMXFobS0FJcuXZKOazQaxMTEoLCwEAUFBdLxpp6TWUxMDDQajc+eU1lZGTQaDQRB8Jlz8sU8WZ5T27ZtceHCBQDWV1p78zn5Yp5qnpP592T37t1hMpl84px8MU81zyk9PR1arVba688XzskX81TznNq3b49z585BoVBIvyedeU7mPSGJiIiIiJoTZ45d1hy3jIyMREpKijQhB+B3KFvnZB57SExMhEql8olzclaecnJypO/7QUFBPnFOzshTZGQk8vLy4O/vD4PB4BPn5Kw8iaIInU6HHj16oKKiwifOCXB8nnJzc5GTkyPVJHzhnFgHcG+ezH/7IiMjERsb6xPn5Mg8yR23FES5PTsn2LBhA1auXIldu3ZJx44dO4YpU6bg8OHDCAoKqvfnb7rpJtxzzz24+eaba92n1Wpx8uRJJCYmQqPRSMebcqWEyWRCWloaEhISoFQqve5KicYe9/ZzqqqqQlpaGjp06GBVOPDmc/LFPNU8LooiUlNTpbz5wjn5Yp5qtt3y96S5Pd5+Tg0d94VzMhqNVr8nfeGcfDFPtrpsKSkptX5POuuctFotkpOT0bVrV6u+VXNh7ls21/NvDKPRiKNHj6JPnz4NXshKrsXceC7mxnMxN56LufFc9eXGW/pWzhq7rGvcEqjdx+d3qPrHaG2NPXjjOTXU9saekz3f973lnJyRJ1EUkZaWhvbt29cah/TWc3JWnsyfv44dO0IQBJ84p6a0va7jtmoS3n5OrAO4N0/mz16HDh3g5+fnE+ck97gjxy3dOlO8R48eyMrKQkFBASIiIgAASUlJ6NSpU61O5YkTJ1BSUoLBgwcDAHJycnD27Fn069ev3tcw/7GveayuxzZ03Bzs+h5vvl/Occvna8rxppyTnOPefk6CINR6L3j7OdniS+dkMpls5s2yPU1tO/PknHMyt8FRbfeEc2rouLefk63fk95+Tr6Yp5rH6/s96YxzqusxRERERES+ytljlzX78vX18fkdyvYYbV2vaX5MU9vu6d8L62uj+b0k9/u+t5yTM/JkLqzUNQ7pjefU2ONyzsl825fOqaE2NuacbP0+9/ZzsqeNrAM4Pk/mONX3eG87JznHHTlu6dbRzW7duqFnz55YtmwZysrKkJKSgo8//hi33norAOC6667DH3/8AQBITk7G/PnzkZ6ejrKyMixZsgTXXHMN2rVr59I2c0DYOzFv3ol5807Mm3di3rwT80ZERERE5DzuGLtkH18exkk+xkoexkk+xkoexkk+xkoexqnp3L6n+PLly7Fo0SJceeWVCA4OxtSpUzFt2jQAQFpamrQe/IQJE3D69GlMnjwZVVVVuPrqq7FkyRKXtlWhqF4Hn7wL8+admDfvxLx5J+bNOzFvRERERETO58qxS/bx5WGc5GOs5GGc5GOs5GGc5GOs5GGcHMPtRfHo6GisXr3a5n3JycnSbUEQ8NRTT+Gpp55yVdNqEUUROp0OarW6ziUCyPMwb96JefNOzJt3Yt68E/NGREREROR8rhy7ZB9fHsZJPsZKHsZJPsZKHsZJPsZKHsbJMTjX3g6iKOLixYu1Nnonz8a8eSfmzTsxb96JefNOzBsRERERkW9hH18exkk+xkoexkk+xkoexkk+xkoexskxWBQnIiIiIiIiIiIiIiIiIiKf5fbl00m+kSNHIicnBwqFAoIgICQkBIMGDcITTzyB1q1bN/n5v/rqK4wcORIRERE2709JScFbb72FP//8E0VFRQgNDcWoUaMwf/58hIaG2v16mzdvxtNPPw2VSlXrvmeffRa33HKL3c9JRERERERERERERERERGSJM8XtZKuA60oLFy5EUlIS/vrrL2zevBl5eXl45plnmvy8RqMRL730EgoLC23eX15ejjvuuAOxsbHYvn07kpKSsG7dOpw9exaPPvpoo183MjISSUlJtf5zdEHc3XmjxmHevBPz5p2YN+/EvBERERER+Rb28eVhnORjrORhnORjrORhnORjrORhnJqORXE7KBQKxMXFQaHwjLC1bt0a1157LdLS0qRjFRUVeO6553D11VejT58+mDFjBs6ePSvdv2rVKowYMQK9e/fGmDFjsHXrVgDAgAEDUFpaiptvvhnvvPNOrdc6c+YMcnNzcc8996BFixYQBAHx8fF4+eWXMWXKFGkfgzNnzuD222/HFVdcgYEDB2Lx4sWorKxs9DkmJSVh2rRpuOKKKzBkyBAsXrwYBoMBAPD777+jb9+++OSTT9CvXz8cOXLE5nN4Wt5IHubNOzFv3ol5807MGxERERGRb2EfXx7GST7GSh7GST7GSh7GST7GSh7GyTEYPTuIooiSkhKP2MheFEVcuHABW7duxY033igdf+211/D3339j/fr1OHDgAHr27IkHH3wQoijizz//xJo1a/D555/j6NGjWLRoEZYsWYL8/HypOL5161Y8+OCDtV6vbdu28Pf3xzvvvIPi4mLpeLt27XDttddCEATo9XrMnDkTvXv3xr59+7Bx40YcOnQIb731VqPP89FHH8WgQYPw+++/46uvvsLu3bvx5ZdfSvcbDAakp6fjt99+Q58+feqMlafkjeRj3rwT8+admDfvxLwREREREfkW9vHlYZzkY6zkYZzkY6zkYZzkY6zkYZwcg3uKm23cCDzzDFBaWu/DNFVVgJ+DwhYSAjz/PDBpkuwfWbp0Kf73v/9BFEUYDAYMHjwY06dPBwCYTCZs3rwZb775prTH+COPPILPPvsMf/31F0pLS6FQKBAYGAhBEDB06FAcPnwYCoUCGRkZ9b5uZGQkXn75ZTz//PPYsGEDunbtioEDB2LMmDHo1asXAGDv3r3Q6XSYO3cuVCoV4uLiMH36dHzwwQd44oknGhWiLVu2QKVSQalUIiYmBv3798fx48el+w0GA6ZNm4bAwMA6n0MURVy6dAnBwcEQBKFR7SDXY968E/PmnZg378S8ERERERH5Fvbx5WGc5GOs5GGc5GOs5GGc5GOs5GGcHINFcbNXXwVOnar3IQKcELBXX7WrKL5w4ULceuutAICSkhKsXbsW48ePx7Zt21BVVYXy8nLMmTPH6kNhMpmQlZWFkSNHolu3bhg5ciQGDx6MYcOG4eabb4ZGo5H12jfccANGjx6NQ4cO4eDBg/j111/xwQcfYMqUKXjuueeQkZGBdu3aWe1rEB8fj4sXL8JkMtlc1iEvLw89e/asdfzTTz9Fv379cODAAaxYsQLnzp1DVVUVqqqqcN1111k9NiYmRlb7iYiIiIiIiIiIiIiIiERRRF6ZHtklOhRrDQjT+CM6VI3IYBULzz6KRXGzJ54AFi2qd6a4CMBYVQWlnx8c8nEICQEef7zRPx4aGooHHngAmzZtwnfffYebbroJAPDll1+iR48eNn9m5cqVOHXqFH7++Wd8/vnn+Oijj7B582bZr6lSqXDllVfiyiuvxKOPPopt27bh8ccfx+233w69Xm/zZ+r75REZGYlff/3V5n0pKSl4+OGH8eSTT2Ly5MkIDAzE448/jqqqKqvH+Tlq5r6XySzS4UBKHjKLKhAbHohBHSMRG652d7OIiIiIiIiIiIiIiIg8liiKOHOpDHuSc5Ger4VRFKFUCIhvqcHwLlHo3Iozsn1R86wm2jJpUoMztkWTCZeysxEdHQ3Bwzazr6ysREhICMLDw5GcnGxVFM/IyEDbtm1hMBhQWVmJyy67DJdddhnuu+8+XH/99fjtt9/QvXv3ep//p59+QkZGBu68806r41dddRUAQKvVol27drhw4QL0er00Wzw1NRVt27a1OUu8ISdPnoRKpcLtt98OoPqX1MmTJ9G5c2e7n0vubHhvkVmkw+q9qbhYpINGpcTxzGIkZZZg1rAEnyqM+1remgvmzTsxb96JeSMiIiIi8i3s48vDOMnHWMnDOMnHWMnDOMnnjljllemxJzkXqXnl0jGjSURqbvW/W2hUiAoJcHm76sP3VNOxKG4HhULhUUt1V1ZW4osvvkBhYSGuueYaAMDUqVPx3nvvoU+fPoiLi8Pnn3+OlStXYvfu3VizZg1++eUXvPHGG4iOjkZKSgqKi4sRFxcn7cl97tw5tG7dGsHBwVavpdFo8Nprr0GpVGLcuHEICwtDTk4O3nzzTcTGxuKyyy5Dx44d4efnhxUrVuCBBx5ARkYG1qxZg/Hjxzfq/GJjY1FRUYGTJ08iJiYG77//PlQqFS5dugRRFGU/j6flzREOpOThYpEOU/d/jSs2fwKlQY8qkwkBfkrAz7Mu2GgsBQDfylrzwLx5J+bNOykAxLRoAbz+OjB2rLubQ0RERERETeSLY1jOwDjJx1jJwzjJx1jJwzjJ565YZZfokJ6vtXlfep4W2SU6jyqKuyNOvri8PIvidhBFEYWFhWjRooXbEr506VL873//AwAEBASgW7du+OCDDxAXFwcAmDNnDkpKSjBt2jQYDAZ07doVq1evhlqtxl133YWLFy9i/PjxqKioQJs2bTB//nx07doVADBmzBg8/PDDmDp1KhYuXGj1ukOGDMGKFSvw0UcfYcWKFSgrK0N4eDgGDx6Mzz77DCqVCiqVCqtWrcJLL72EwYMHIzw8HOPHj8fs2bMbda59+/bF9OnTcdttt0GtVuP+++/Hf//7X9x///149NFHpb3VG+IJeXO0zKIKhIl6DP/gVSirDO5uDhERuUtODsQVKyCwKE5ERERE5PV8cQzLGRgn+RgreRgn+RgreRgn+dwVq2KtAcY6Jl8aRRHFWs+qu7g6Tr66vDyL4nYQRREFBQUIDw93S7J37drV4GMCAgKwePFiLF68uNZ9KpUKzz77LJ599lmbP7t8+fJ6n3v48OEYPnx4vY/p1asXvvjiiwbbCQC33HILbrnllnofs3DhwloF+kOHDkm3k5OTG3wdd+fNGWLDA1G+J1kqiFdqglGqCYVapUSQSunm1jmGCKCqqgp+fn7wjaw1D8ybfFUmEaUVVTCaRAgCIIqAUiEgJNAPfgrXRo95804igMqgIKgeeoh5IyIiIvJCubm5KCkpcXczrISGhiIqKsrdzWi2fHEMyxkYJ/kYK3kYJ/kYK3kYJ/ncFaswjT+UgmCzMK5UCAjT+LusLXK4Ok7euLy8HCyKE3mhQR0jEXgpVfr3pon34/SkO3DvsAQE+cie4qLJhPTUVCQkJEBoxJ705B7Mm3xbD1/AzhM5SIwOgUIQYBJFJGeXYkz31ph4eTuXtoV5806iyYSMf/JGRERERN4lNzcXt911DwpKbS/b6S4RIRp89vEHLIwTERGRT4sOVSO+pcaq6GsW31KD6FDfqLM0lrctLy8Xi+JEXig2XI1w7QXp321GDMHVwxIQ4yMFcaLmILOoAhqVEop/ruxTCAI0KiUyiyrc3DIiIiIiInK2kpISFJRqETV4IoIiWru7OQCA8oIc5O7fhJKSEhbFiYiIyKdFBqswPDEKEKqLvDWXB48MVrm7iW7lbcvLy8WiuJ1CQ0Pd3QRqBF/MW1DSseobCgVGTLkW0PheQdwX89YcMG/yxIYH4nhmMUyiKM0U1+qNiA0PdEt7mDfvxLwRERERebegiNYIbdXW3c2Q5Lq7AcQ+vkyMk3yMlTyMk3yMlTyMk3zuiJUgCOjcKhgtNCpkl+hQrDUgTOOP6FA1IoNVLl3KXRRF5JXpG2yHK+PkbcvLy8WiuB0UCgVatWrl7maQnXwyb5WVwPHj1be7dgU0Gve2xwl8Mm/NAPMm36COkUjKLEFydik0KiW0eiNiwtUY3DHS5W1h3jxDZpEOB1LykFlUgdjwQAzqGInYelYAYd6IiIiIiHwL+/jyME7yMVbyME7yMVbyME7y1RWr+grFAGQVkRsiCAKiQgLcugy4KIo4c6kMe5JzkZ5fe8Z651bBEATB5e8pX11enkVxO5hMJuTl5SEyMhIK7rnqNXwyb8ePA1VV1bf79XNvW5zEJ/PWDDBv8sWGqzFrWIJVEXRwx0i7t0Gwt5BqC/PmGE3JRWaRDqv3puJikQ4alRLHM4uRlFmCWcMS6nwO5o2IiIiIyLewjy8P4yQfYyUP4yQfYyUP4ySfrVjVVyi+sWcblFZUYc/p+ovI3iKvTI89yblWxWejSURqbvW/W2hUiAoJcPl7yleXl2dR3E4lJSWIjHT9LD5qGp/L259//nvbR4vigA/mrZlg3uSLDVdj4uXtGv3zjSmk1oV5a5qm5uJASh4uFumQGB0iLaefnF2KAyl59b5HmDciIiIiIt/CPr48jJN8jJU8jJN8jJU8zT1OcpcEB2rHqq5CcXZxBTKLdPg9rQAZhTqr+2oWkV3Z/qbILtEhPV9r8770PC2yS3TS+bjyPeVJy8s7EoviRN6omRTFST5HzBYm79PYQqo7+ep7tam5yCyqgEalhOKfDqVCEKBRKZFZVOHsphMRERERERERETmMKIo4k1OGH0/m4MTFYuj0RmhUSnSLCcPobq0bnM1dV6G4XQsNsksqcOJiMcLUtWcq1ywiN6X9F4t0OJVdisxCHdILtKisMjllNnqx1mBz324AMIoiirUGh7xOY3jC8vKOxqI4kTeyLIr36eO2ZpBncORsYfIMcgvH3lZI9eX3alNzERseiOOZxTCJolRU1+qNiA0PdGaziYiIiIiIiIiIHCqvrBI7jl/EnuQ8lFZWQRQBQQAuFlfAYDRh+sA4RIXUPeZVV6E4wF+BwnIDdHojwmwMJTqiiGwuiJ+4WILMIh3C1P4YlBCB7OIKnMwqAdC42ei2Zp7HtQhCmNofSkGweb5KhYAwjX+TzoessShuB0EQEBER4bXLAjRXPpc3gwE4dqz6dpcuQGioe9vjJD6XtwY0ZfasJ80Wbm55cwZ7Csd1FVI1KgU2Hb4g+/3kqrx5ynvVGbPVm1rUHtQxEkmZJUjOLoVGpYRWb0RMuBqDO9a9JBI/b0REREREvoV9fHkYJ/kYK3kYJ/kYK3l8LU72LiV+oVCHX8/ko6SiyuI5gBJdFX49m4dhXaKkoritWIVpbBeKKw0mRIaooFYpbbazqUVk8wz3rccysflwBsr1RvgrBfSNa4EberVBj9hQVBkBnaEKgPyieF17pHdpHYyrOkchvqUaqXm1Z8bHt9QgOrR6zNLX3lPu4vzd2H2It7/pJk+ejLfffhsAMHPmTLz55psN/szChQvxxBNPyHrOpvj999+RmJiIysrKJj8XYH1+DeVt5MiRWLdunUNe1yVOngTMcfLhpdO9/fNmD3MRdOeJHKTmlmHniRys3puKzCJdwz+M+meoZhbpsOnwBSz/+cw/RVJ5z9lYzSlvzmJZOI5vGYTE6BBc/KeIW9OgjpGICVcjObsU6fnlSM4uRajaH8nZZXa9n1yVN0+Y2d7Uz1tdbOWioaK2pdhwNWYNS8CY7q2REBWMMd1b495hCYjxgIsZiIiIiIjINdjHl4dxko+xkodxko+xkseX4mQu6G45kon1BzPwbVI21h/KwJajmThzqQyijRnOuSWVKNTZnrFdqDUgr/TfOpCtWEWHqhHfUiP9W6NSIrF1CELUfujQMgjdY8JsPrdlEbkx8sr0+PlkDv44V4hyvREAYDCKOJhWgAMpBejZtgVahQQgKaMYSZlFyC2ttHn+tp7XvEe6udBvNIk4mVWKtLwyDOkYiYSoICj/iYFSISAhKgjDu0QhMrh6mXhfek+5E2eK28FkMiE7OxvR0dFQKFx/PcGMGTPQu3dvzJ8/v8nP9dFHH8l63NKlS5v8Wk2RmZmJ6667Tvq3Xq+Hn5+fFP/+/fvbPBfLY+7Om8M1k/3EfS5v9Wjq7Nn6Zgu7eqnqpubNV/ebtoc9hWNzIdUyZoVaAw6mFdj1fnLV580Tlgh31mx1W7kY/E+h3J7nsKcNzen3JBERERFRc8A+vjyMk3yMlTyMk3yMlTy+FCfLgq6Z0SQiNbf637aWEg8O9INSEFAligBEGE0iqkwiTCYRAf4KaFRKiKIIQRBsxioyWIXhiVEQBCA00B+tQwNQWlkFo0lEiyB/jOsdgxMXi/FnehFKK6ugVAjSft/mInJjZJfokJxTCj+lAEEQpIJ3TLgaFwq1OJNTikNp+SjQGhAVEih7j/G69kgHgN0nczFreAeM7xOL7GIdCsr1CArwQ0SQCio/AXllekQGqyCKos+8p9yJRXE7abW237jkHLGxsUhKSpL+PXLkSMyaNQu33nqrXc/jU3lrJkVxwMfy9g9bRd+mzp6ta9llQHDLUtWNzZsv7zcNyC/421s4rllIXf7zmUa9n5ryeZN7bo1ZItzRnDlb3d6itiP44u9JRzt16hRefPFFHD9+HAEBARgwYAAWLFiAqKgo7N+/H8uWLUNqairatGmD++67D+PGjXN3k4mIiIioGWMfXx7GST7GSh7GST7GSh5fiVN9Bd30PC2yS3S1iuLRYYHo0TYMR88XQm80QVtphMFoAgB0j22JiCAVsot1iP5nY/CasRIEAZ1bBaN1aABSc8vx44lsRIUGIL5lMP48XwSI1a8xpX87GIxGGExivcu5y1WsNaC0ogr+CgUiglTIL6uEv58CLTQqnMsvR0ahFmEaFSqqxAYvDKj5vLb2DAeAKlHE+XwthnRsCaOp+jG5ZZXIL9MjUKWEQhCREBmMNmGum1Tky3g5gZfKyMhAYmIifv31V4wfPx59+vTB1KlTkZGRIT1mxYoVGDp0KAYOHIgVK1ZY/fyMGTPw2muvYc+ePejTpw8qKv4tBhQUFKBbt244evQonnrqKTz66KN2PadZSkoKEhMTpTadP38ed999NwYOHIiBAwfiscceQ0lJSZNjsXnzZtx444146aWX0KdPH+Tk5Fi1RRRFfPzxxxgxYgT69u2LCRMm4NChQzaf69ixY5g8eTL69u2LgQMHYsGCBVax8QiWRfG+fd3XDrJbXcs2m4uDpn/+MNo7e7auZZerZ4u7d6lqe9izbLi3sWfJ7qYvwx3YpPeTvew5t8YsEe5o9sbHWVsQuHprg+ZKr9dj5syZGDBgAPbv349vvvkG+fn5WLJkCS5duoQ5c+Zg6tSp2L9/PxYsWIBFixZZXYxHRERERERERNSc1VfQNYoiirW1l0mPDFZhXK826NU2DHqDCQajCf5KAQM6ROCWfrEQAPx48hLyyvR1vq4gCNBXifjjXCHiWgYhJlyDb//Kwts/n8UbP53BazuT8cPfOVAoFOgRE4aokIAmLy0epvFHSKAf9FUmtG2hRsvgALTQqGA0iSjVVSEyOBAalR8C/f8trZovDGjoeZV1tE2pEBDXUoNT2aVYsfss5m/8Cy98ewpv/XwaGQVaRAQFIC2vHHtO5yJTq0R+uV7Wku1kG2eKN4EnLPO7Zs0avP/++wgICMDtt9+ODz74AEuWLMG+ffuwatUqfPTRR+jRowdWr16N06dP46qrrrL6+SFDhkClUmHfvn0YNWoUAGDXrl2Ijo5Gnz598OWXX0qPlfucdVm4cCFiY2Pxf//3fygrK8Pdd9+Nd999F0899VST43Dp0iUEBATg0KFD8Pf3t7pv69at+Pnnn7F582a0bt0a7733Hh566CHs27cPSqXS6rFPPPEE7rnnHkycOBF5eXmYM2cO1q9fjzvuuKPJbXQIoxE4erT6docOQESEW5tD9qlr2ea2LQKlImhjZ8/amqHqCUtV28MT9pt2FnuW7G7qMtyuno1t73Lk7phNbcme+Dhi9QJbfQUAPr0qgifR6XR49NFHMWHCBPj5+SEiIgKjR4/GZ599hu3bt6N9+/aYNGkSgOo+0ciRI7Fx40b07NnTzS0nIiIiIiIiInI/c0HXVmFcqRAQpvGvdTw00B9dWodg8hXtMLBDSxRq9WihUaF1aCAiglT4K6MYp7PL0DdOh5ZBtX/eLLtEh9zSSnRpHYxtx7Jw9EKRdF9euR5HLxTCTyk0OFO7PqIoIq9Mj9yyCgQoFWgfqYHBKMIkiugQqYFSIaC0ogpXdYlEZLAKp7KKAfxb4K7rwgBL5j3SLZegN+vSOhhKQYGdJ7KxPzUf5jAP7RQFlZ8CH/xfGkp0Bqj8FFArqtC1bSSGJ7ZqcMl2so1FcTsIgoBWrVpBEASPWeb31ltvRevWrQEAQ4cOlWY3/fjjjxg2bBguv/xyAMB9992HNWvW1Pp5f39/XHPNNfj555+lovhPP/2EsWPH1nqs3Oesy6pVqyAIAlQqFSIiInDVVVfhT8tZz01QWlqKWbNm1SqIA8C4ceMwaNAgtGnTBoIg4IYbbsDbb7+Nixcvol0768JMSUkJNBoNFAoFWrVqhQ0bNnjW/gxnzgDl//zi9PGl0y0/b76irqKvVm+yqwjqyUtVNyVv3lbEt4e9Bf+mFI4bU1RvSt687WIGe+LT1P3H6+ortG2hdsjWBr74e9LRwsLC8J///Ef6d2pqKr7++muMHTsWJ06cQLdu3awe361bN3z33Xf1PqfRaITRaHRKe32NOU6Ml+dhbjwXc+O5mBvP5a25MRqNgChC/Oc/TyCKIiCKDuvv1Jcbb8uXq7CPLw/jJB9jJQ/jJB9jJY8vxam+gm58Sw2iQ2uPqQmCgOiwQJRXVkEhCIg1qqFSKlBlMuFkVgn+yiiGiOpZ6PXFqlhrQEy4GiUVVTieWWx1nygC+ipTnUu4yyGKIs5cKsPe5FyEqv3RMzYUAxNaolhXhQuFWvgrFfBTCOjTLhzd24TiYpEOBpMIy6J4XRcGWDLvkQ6hema5URSlfdCvuawVCsorceJiiVQQD1X7oVtMKLYfu4g/0gvRIyYU/koFlH6q6jwIuU26EKA5Y1HcDoIgIDQ0FEDTB8odpW3bttJttVqNyspKAEBOTg46dOgg3efv72/1WEvXXXcdnnzySRiNRlRUVOC3337DQw89VOtx9jynLcePH8eyZcuQnJwMg8EAo9GIHj16yP75+oSGhiI4ONjmfRUVFXj77bexd+9eFBf/+4tTr6+9NMdjjz2G//73v/jwww8xdOhQ3HzzzejYsaND2ugQhw//e7sZFMXNnzdfUV/RV24R1J4Lcpo647gxmpI3T9hvui5NXRnE1QV/e4vqTcmbN17MIP/z1rSCf119haxinUMuJPDF35POkpmZiTFjxqCqqgqTJ0/GQw89hFmzZkkXFpqFh4ejsLCw3uc6ffq0M5vqk7gkvedibjwXc+O5mBvP5W25ycjIgFanQ2lZGcTApm8t5whlZWXQ6nQ4efIkSktLHfa83pYbd2IfXx7GST7GSh7GST7GSh5filN9Bd3hXaIQGayy+XOCIEBvMiE5uxT+fgpUGky4UKiFVl99YZy5mFxfrMI0/lD7K1GkNaDKZH0RoSAAKj+FrJnadckr02NPci5CAv0QpvHHJ7+dQ9twNfrHt0CPmFAYTSLaRWjQroUGf2eV4HROKSwL4kDdFwbUjEXnVsFooVEhu0SHYq0BYRp/aR/0tLxyKBXVY4cGowkJkWEo11fh8PlCiKKISqMJEX4C/PyqS7pNuRCguWNR3A4mkwkZGRlo27atx8yMq+tKI71ej6qqKqtjJpPJ5mOHDBkCk8mEw4cPIy8vD23atKk1c8re56x5X3FxMe69917ceuutWL16NYKDg/Hmm2/it99+q/Pn7WH+ZWDLkiVLcOLECaxduxYdOnTAhQsXMHr0aJuP/c9//oNRo0Zh165d+PnnnzF+/Hi88cYb0ix6t7OcWe/jRXHLz5tHzdZvAkcUfT19qeqm5M0dRXw56rsQAYDHztq3R1Py5unn1hRNLfjX1VeoMBilfc2bciGBL/6edJbY2FgkJSUhPT0dzzzzDJ544olGP1eXLl2g0Wgc2DrfZTQakZSUhJ49e9basobci7nxXMyN52JuPJe35iYkJAQatRohwcEeM2guVJRAo1aja9euDpkgUF9utFotLza0gX18eRgn+RgreRgn+RgreXwpTg0VdOubDR8VHIjKKhNOZte+2M5cTK4vVtGhamQWVM/O9lMIVoXxkEA/aFR+smZq18VyefZfknORU1KJjEId/rxQhK5tQhEZEoBinQGdWyuQEBWM9AKtXRcGWBIEAVEhATYL2WFqfwT6K+GnEJBfZoBSISCvtBI6vREqPwU0/kqolAqUl5chSBMEI9DoCwGaOxbF7WSeXezpM+NatWqF7Oxs6d96vR4XLlyw+VjzEuq7d+9GdnY2rr/++kY9p0qlQkXFvxcFnD9/XrqdmpqK8vJy3H333dKM7r///rtxJ2enpKQkjBw5Eu3bt4cgCDhx4kSdjy0sLESLFi0wceJETJw4Ee+88w6++uorFsXdxNZsfm/miKKvqy/IacwM6abkzd37TdtS14UI3yVdREZhhcfO2rdXY/PmDefWWE0t+NfVVxjQoQUyCiscciGBr/2edCZBENC+fXs8+uijmDp1KoYPH46ioiKrxxQWFiIiIqLe51EqlV414O4JGDPPxdx4LubGczE3nsvbcqNUKgFBgPDPf55AEARAEBweS1vP5025cjX28eVhnORjrORhnORjrOTxpTjVV9Ctj5xZ5qIo1hmryGAVOrUOQZFWjx5tw3D0fBEEobog3iYsEIH+ClkzteuigIAOUUEwiSLS8suRXqBFQbkeoijiyPkixIQHomOrYMSGq9EjNgxh6li7LwyQIyIoANGhgSgo16PKJKK80ogWGhX8FAKiggMQqvaHSqlAhdE8yx7QBPjhXF4ZMgp1Dm2Lr2NRvJE8fWbcsGHDsGDBAvz111/o0qUL3n333XpndY8dOxavvvoqsrOzMWfOnEY9Z/v27bF//34UFxdDr9fjyy+/lO6LiYmBQqHAkSNHMHjwYGzYsAF5eXkoKiqqNfvc0WJjY3H69Gno9XqcOnUK3377LQDg0qVLVlc+Z2dnY+zYsXj77bcxZMgQlJeX4/Tp04iLi3Nq+2Qzmf4tirdtC7Rq5d72UKM0tejrygty7Fmq3ZfVdSHCH+cKYRLhsbP2XclXz62ugr8IYNPhC41eIeD6njEQAZ+8kMDT7N+/H0uWLMF3330nXW1s/n+vXr2wc+dOq8cfP34cvXv3dnk7iYiIiIiIiIh8jZxZ5qIo1vvzHaOCUFKhwm0D49A6RIVz+VoE+CkRFKBEfMsg2TO1geo9xPPK9FJb/JUK9GsXXj0DPF+L/LJK6bEmUURJRRUyCnXIK9M3+sIAOYyiCZfHt4C+ylS997rRhMiQANzQqw0KyvXwV1oXuiODA2Ayivji9wsorayyutCgc6tgFsbrwaJ4I3n6zLixY8ciOTkZs2fPhtFoxIwZM9CnT586Hz948GBcunQJ0dHR6Ny5c6Oe8+6778aJEycwbNgwxMXF4cknn8TevXsBAK1bt5b26waAadOm4bXXXsPtt9+OadOmYd68eQ4795oee+wxPPbYYxg0aBB69+6NV155BQAwZ84cfPbZZ9LjoqOj8cILL+CFF17AxYsXERwcjGHDhtncX90t0tKAkn/2GWsGs8TJNldekGPvUu2u1tR9vuWq60IEQYBHbKNBzlWz4G/PxSIN9RU84XPk63r06IGysjK8+uqreOihh6DT6fD222/jiiuuwK233oqPPvoIGzduxLhx43DgwAHs2bMH69evd3eziYiIiIiIiIg8Us3CckOzlJtaTBYEAWFqFfq080fbFppGz9QWRRFnLpVhT3Iu0vOrZ60bjCYMiG+BthEahAQokV8GmMTqx/r7KaAQBJRXVkGjcu4KN+fzq2eoX50YhXK9ETq9EfERakztH4cdSRdxLl+LwGAl/BUKtI1Qo0dMGM7mlqG0snrCqdEkIjW3HADQQqPiXuP1EMT6LsPwYlqtFidPnkTXrl0dtu+jKIrQ6XRQq9W80sKL+EzeNm4EJk+uvr1kCbB4sVub42xNzZurCqbuUPPcHHVBTs3nPZVditzSSsS3DJIek55fjoSoYDx0je2LZ1z1eatZmDRfHOCMWex1vVbbFmocTCuoddHAmO6tva7Y6TO/J11g0+EL2HkixyPy7uq8OaNv5QrJyclYunQp/vrrL2g0GgwaNAhPPfUUWrdujUOHDmHp0qVISUlBbGws5s2bh2uvvdbm83jr+buT0WjE0aNH0adPHy6R6mGYG8/F3Hgu5sZzeWtuUlJSMHXmbLS/YQ5CW7V1d3MAACWXMnDu23fx5UcrHbaneF25ae59q7rOn9/N5GGc5GOs5GGc5GOs5PHVONkqLDd1lrKrYpVbWoktRzKRmldu+eoQAFzfsw12n8rF5iMZ0OmN8PdTIEjlB5Mool98OOaO7IxubUKd1r6kzCKsP5iBAH8F2rXQIMBfAb3BhFC1HyKCAxDop4DBaEJIoB+CA/xxOL0QRy8UoWZxVykImDKgLXrGhjulnZ5Mbt+SM8XtIAhCs+yoezufydvhw//ebgYzxZuSN19f9tsZS1XbiplWXwWFIKBdhEb2Uu2u+ry5chZ7fUtoZxTqPHYbDXv4zO9JF6hrOX13rBDAvMmTmJiItWvX2ryvf//+2Lp1q4tbRERERERkG/v48jBO8jFW8jBO8jFW8vhqnPLK9NiTfAl/Z5VCq6+qXu7bT4HyyioAYqNmKbsqVtklOqTna2u+OtQqBbT6Kkwe0BZtI9TY+McFFGoNEAD0iA3D6K6tYTKJKNFVIUzj75S2RYeqEd9Sg9S8ciTnlFrd17VNCG7uHSu99oGUPBy5UGTzeYyiiGKtwSlt9BUsitvBZDLh3LlzaN++vbQfJXk+n8mbeT9xwCuL4vbO3G5K3jx92W9PZCtmRy8USbGTW/R11efN1YXJui5E8ORtNOzhM78nXaCu5fTru1jEWZg3IiIiIiLfwj6+PIyTfIyVPIyTfIyVPL4ap+xiHf7KLEZGgQ6llVUQRUAQgJBAP1SZTBjUoaXdRXFXxapYa4DRYuFsAUCvtmFoExaIYp0BKZfKkdg6BP+7pSfyyiqRV6JHoEqJ8soqFJQboFRqEaYJq/P57V1W3lJksArDE6MAAUjPs56BPyihJULVfjCZTMjJyUFQgBpKQbA6FzOlQnBa4d5XsChuJ5PJ5O4mUCN4fd5E8d+ieOvWQEyMe9tjp8bO3G5s3jxpJqe3sBWzlkEqRAYHoGubELuKvq74vHlKYdIZs/br4uwtAbz+96QNzojZoI6RSMos8ZgVAnwxb0REREREzRn7+PIwTvIxVvIwTvIxVvL4YpzyyvTIKNChpKJKOiaKQImuChnQIa9MX+fP1lU0bhnk75JYhWn8rYrJvdqGoUWQCtuOZeHExWLEhqtRXlmFthEaXNe9NZRK4M/0QlzZORJZxToo66nXN3VZeUEQ0LlVMFpoVHUW1c3LzEdHtZBmldcU31KD6FDvm7DlSiyKEzmQ04pWFy4A+fnVt/v1q778you4eua2pxRMvUldMevaJsQjZ9d7WmHS2Xx9SwBncFbM6lpO3xtXCCAiIiIiIiIiIpJLo1JCZzDavK/CYIRGpbR5X71F486RCA0OdmazAVgvUa5RKdEmLBDbjmXh6IUihKr9oAlQQgSQVaTDvrN5mNivHWLC1bhQoMOJiyXoHhta53NXLyufa1WoNppEpOZW/1vOsvKCICAqJKDex5lMJrQMqntW+fAuUYgMVtkXmGaGRXEiB8ks0uHzbw5j7GtP4aqcCxBFwE8poErtDz9FE4vYOt2/t71y6XTXztxubgVTR/C2mDW3wiS3BLCfM2PmyhUCiIiIiIiIiIiIPEG4xh/dY8Jw1Mae1t3ahCG8jqW76y0aiyJGdwl3Uov/ZblEeYBSgZKKKpy4WIxQtR/ahAVCpVQgS1cBUQSSs8uQU1qB3NJK/JVRjISooHpnYNver7xaep4W2SU6u5eVr09Ds8qpbiyK20EQBMTFxfFN5WVclbcDKXlI3LwWPY/96tTXweWXO/f5naAxM7ebkrfmVjB1BEfFzJW/J5tTYdLZF5b44t+35rCNgi/mjYiIiIioOWMfXx7GST7GSh7GST7GSh5fjZNapcSY7q0BAMczi1FlEuGvFNA9JgxjureGuo6Z4vUWjfO1KDFEOD1WlkuU6wxVSMooRnxLDTQqPwT6V6+N3io0AFnFFcgtq0R2cQX8lQokRAU1OAO75n7lloyiiGKtwSHtN7+n5MwqJ9tYFLeTnx9D5o1ckbfMogqMOJ8s/bsiOBRGEVAqBAT61bPhhD2uvhq48UbHPJcLNXYWclPy1pwKpo7iqJjx96TjuWJLAF/LW3PZRsHX8kZERERE1Nyxjy8P4yQfYyUP4yQfYyWPL8ZJ7e8HP4WAcb3b4OrEKBTrDAhT+yM00A9GUYTa3/Y5N1Q0ttyj3JnMxWQgAMU6A6KCA63aFab2R4CfEvoqIzpEBiE2PBBBAf4NzsCuuV+5JaVCQFgdM+jt5YvvKVdzUKWueRBFEampqRDr+PCSZ3JV3mLDA9Em/TQAQK/WYMXGA3j03Z/x7e4koKDAMf9t3gz4O+YXqCuZZyGP6d4aCVHBGNO9Ne4dllDvLGR+3rwT8+Ycg/6ZtZ+cXYr0/HIkZ5c6dHl7X8ybs2PmCXwxb0REREREzRn7+PIwTvIxVvIwTvIxVvL4apwig1VoG6FBeoEWheV6KBUCCrV6pBdo0baFps7Z1OaisS1KQUCAYHR5rMx7jFsTEOivRJ+4FujSOgTtI4MRFRLQ4Cx2289VLb6lpt6l1+Xy1feUq/GyAiIHGRzpj8i8LABAZttOSL5U7nMFmKbgzG2ixuOWAPZjzIiIiIiIiIiIiBzHcglye/azNheNLfcUN4tvqUFUsOsnAlruMZ6ep4VRFKFUCIhvqcaQhJbQGaqw70yJrPOr+7k0DS69Tq7FojiRg8ScPyPdLk3sjjHdW7MAQx4ps0hnVSgc1DESsXyfejxeWGI/xoyIiIiIiIiIiMhxGrOfdb1F486RCITt/cadyWaBX+2P4EB/pOWW4cuDuaiqUdzu3CrYZmG8sRcLkOuxKE7kKH/9Jd3sdf1V6MVCDHmgzCIdVu9NxcUiHTQqJY5nFiMpswSzhiWwME5EREREREREREREDlVf0bhlkD9SUy+hVatWbmmXZYG/WGvA1qOZOJldKj3GaBKRmls9w72FRlXnxQCNuViAXI9FcTsIgoCEhARe1eFlXJa3Y8f+vd27t3Nfqxng5805DqTk4WKRDonRIVAIAkyiiOTsUhxIyXPIjFrmzTsxb96JeSMiIiIi8i3s48vDOMnHWMnDOMnHWMnDONVWV9FYFEWPidX5wnKczimzeV96nhbZJTq3Fb35nnIMhbsb4G2qqqrc3QRqBJfkzWKmOHr2dP7rNQP8vDleZlEFNColFP/88VQIAjQqJTKLKhz2Gsybd2LevBPzRkRERETkW9jHl4dxko+xkodxko+xkodxks9TYlWsNcAoijbvM4oiirUGF7fImqfEyZuxKG4HURRx/vx5iHV8KMgzuSRvRiOQlFR9OyEBCAlx3ms1E/y8OUdseCC0eiNM/8TVJIrQ6o2IDQ90yPMzb96JefNOzBsRERERkW9hH18exkk+xkoexkk+xkoexkk+T4pVmMYfyjpmYisVAsI0/i5u0b88KU7ejEVxIkdISQG02urbXDqdPNigjpGICVcjObsU6fnlSM4uRUy4GoM7Rrq7aUREREREREREREREbhEdqkZ8S43N++JbahAdqnZxi8jRuKc4kSNYLp3eq5f72kHUgNhwNWYNS8CBlDxkFlUgNjwQg/8plBMRERERERERERER+TJRFJFXpkd2iQ7FWgPCNP6IDlWjZZA/hidGAUL1HuJGUYRSISC+pQbDu0QhMljl7qZTE7EobieFgpPrvZHT83bs2L+3m9FM8cwinVVxdVDHSMQ6sLjKz5tzxIarMfHydk57fubNOzFv3ol5IyIiIiLyLezjy8M4ycdYycM4ycdYycM4yefKWImiiDOXyrAnORfp+bUL352igtBCo6pVMI8MVkGoY2l1V+F7qulYFLeDQqFAQkKCu5tBdnJJ3prhTPHMIh1W703FxSIdNColjmcWIymzBLOGJTikMM7Pm3di3rwT8+admDciIiIiIt/CPr48jJN8jJU8jJN8jJU8jJN8ro5VXpkee5JzkZpXLh0zmkSk5lb/u4VGhaiQAESFBLisTXLwPeUYvKzADqIoQqvVciN7L+OSvJlnigcHAx06OO91PMiBlDxcLNIhMToE8S2DkBgdgov/zBx3BH7evBPz5p2YN+/EvBERERER+Rb28eVhnORjrORhnORjrORhnORzdayyS3RIz9favC89T4vsEp1L2mEvvqccg0VxO4iiiIsXL/JN52WcnrfiYiA9vfp2z55AM1nCIrOoAhqVEop/lgxRCAI0KiUyiyoc8vz8vHkn5s07MW/eiXkjIiIiIvIt7OPLwzjJx1jJwzjJx1jJwzjJ5+pYFWsNMNbxWkZRRLHW4JJ22IvvKcdoHtU7ImeyXDq9Ge0nHhseCK3eCNM/v4RNogit3ojY8EA3t4yIiIiIiIiIiIiIiMhamMYfyjr2BlcqBIRp/F3cInIlFsWJmqoZ7icOAIM6RiImXI3k7FKk55cjObsUMeFqDO4Y6e6mERERERERERERERERWYkOVSO+pcbmffEtNYgOVbu4ReRKfu5ugLdRqVTubgI1glPzZt5PHGhmM8XVmDUsAQdS8pBZVIHY8EAM/qdQ7ij8vHkn5s07MW/eiXkjIiIiIvIt7OPLwzjJx1jJwzjJx1jJwzjJ58pYRQarMDwxChCq9xA3iiKUCgHxLTUY3iUKkcGemze+p5qORXE7KBQKxMXFubsZZCen581ypnjPns57HQ8UG67GxMvbOeW5+XnzTsybd2LevBPzRkRERETkW9jHl4dxko+xkodxko+xkodxks/VsRIEAZ2ighCk8kNGoRb55ZVoGRSAti00aBMWAKGOpdXdje8px+Dy6XYQRRElJSXcyN7LODVvRiOQlFR9OyEBCAlx/Gs4UWaRDpsOX8Dyn89g0+ELyCzSubtJEn7evBPz5p2YN+/EvBERERER+Rb28eVhnORjrORhnORjrORhnORzdaxEUcTZ3HL8fDIHydml0BlMKKusQkpuGbKKKzw2Z3xPOQaL4nYQRRGXLl3im87LODVvKSmAVlt928uWTs8s0mH13lTsPJGD1Nwy7DyRg9V7Uz2mMM7Pm3di3rwT8+admDciIiIiIt/CPr48jJN8jJU8jJN8jJU8jJN8ro5VXpkee5JzcSq7FCo/BSKDVNDqjUjP1+JkVikuFnlmYZzvKcfg8ulETWG5dHqvXu5rRyMcSMnDxSIdEqNDoBAEmEQRydmlOJCS1+Ql0TOLdFZ7jQ/qGIlYB+41TkREREREREREREREZI/sEh3S87Xo1TYMLYJU2HYsC8czi1FlEqHyEzC6W2vc2CsGnVsFe+xS6tR4LIoTNcWxY//e9rqZ4hXQqJRQ/POLXSEI0KiUyCyqaOLzVs9Av1ikg0alxPHMYiRllmDWsAQWxomIiIiIiIiIiIiIyC2KtQYE+CvQJiwQ245l4eiFIuk+fZWIpIxihAT6o4VGhaiQAPc1lJyCy6fbSaPRuLsJ1AhOy5sXzxSPDQ+EVm+E6Z/lNkyiCK3eiNjwwCY9r+UM9PiWQUiMDsHFf2aO24ufN+/EvHkn5s07MW9ERERERL6FfXx5GCf5GCt5GCf5GCt5GCf5XBmrMI0/4iM0KKmowvHMYqv7BAFQ+SmQnqdFdolnbDNrie+ppuNMcTsoFArExMS4uxlkJ6fmzTxTPDgY6NDBOa/hJIM6RiIpswTJ2aXQqJTQ6o2ICVdjcMfIJj2vo2ag8/PmnZg378S8eSfmjYiIiIjIt7CPLw/jJB9jJQ/jJB9jJQ/jJJ+rYxUdqkZsi0pcLKpAlcl6f+6QQD9oVH4wiiKKtQaXtUkOvqccgzPF7SCKIgoKCriRvZdxWt6Ki4H09OrbPXsCCu/6OMWGqzFrWALGdG+NhKhgjOneGvcOS0BME5c4d9QMdH7evBPz5p2YN+/EvBERERER+Rb28eVhnORjrORhnORjrORhnORzdawig1W4LDoEbcIC4a+sntgnCECo2g9twgIR6K+AUiEgTOPvkvbIxfeUY3hXFc/N+KbzTk7Lm+XS6V62n7hZbLgaEy9vh4eu6YyJl7drckEcqJ6BHhOuRnJ2KdLzy5GcXdqoGej8vHkn5s07MW/eiXkjIiIiIvIt7OPLwzjJx1jJwzjJx1jJwzjJ5+pYCYKAmHA1urYJxehurREXoUbHqCC0bxmEMLU/AAHxLTWIDm16rcSR+J5yDC6fTtRYXryfuDOZZ6AfSMlDZlEFYsMDMfifQjkREREREREREREREZG7VBfGA3FT7xiEqv1wJqccpRUGlFUY0DUmFEMSWqJlkGfNFCfHYFGcnCazSGdVGB3UMRKxvlQYNe8nDnjtTHFnMc9AJyIiIiIiIiIiIiIi8iSCIKBTVBD8ldHo0LIM2SUVCFP7I1Ttj5zSCvj7KdC5VTAEQXB3U8mBWBS3U2hoqLub4BUyi3RYvTcV/n/+gav2fwdUViI7wA/hbUIQpHLt204QRcRWVEAIDKzeHMJRvv3239s9ezrueUnCz5t3Yt68E/PmnZg3IiIiIiLfwj6+PIyTfIyVPIyTfIyVPIyTfO6KVX65Af93OhcmEQjwVyC/TI+jF4qg1RuREBWEFhoVokIC3NI2W/ieajoWxe2gUCjQqlUrdzfDKxxIyUNWQTlee+cJhORfcmtbBABOnZ+ekACEhDjzFZolft68E/PmnZg378S8ERERERH5Fvbx5WGc5GOs5GGc5GOs5GGc5HNnrLJLdDidUwajjX260/O0yC7ReUxRnO8px1C4uwHexGQy4dKlSzCZTO5uisfLLKpAm7I8txfEXeLuu93dAp/Ez5t3Yt68E/PmnZg3IiIiIiLfwj6+PIyTfIyVPIyTfIyVPIyTfO6MVbHWYLMgDgBGUUSx1uDiFtWN7ynH4ExxO5WUlCAyMtLdzfB4seGBMJ5Pk/6dNGYiNl81CUM6tcTobtEubYvJZEJGRgbatm0LhcLB14GEhwPtuHe2s/Dz5p2YN+/EvHkn5o2IiIiIyLewjy8P4yQfYyUP4yQfYyUP4ySfu2IVpvGHUhBsFsaVCgFhGn+Xt6k+fE81HYvi5BSDOkZCXXxR+veRmMsg9uyJ7sMSgHCnLmZem8kEfVBQ9TLnji6KExERERERERERERERkVeJDlUjvqUGqXnlte6Lb6lBdKiLa1nkdCyKk1PEhqsRJhZI/44b0gejhiUgxtUFcSIiIiIiIiIiIiIiIiILkcEqDE+MAoTqPcSNogilQkB8Sw2Gd4lCZLDK3U0kB2NR3A6CICAiIgKCILi7KV4hOD1Vun31uGGunyH+D+bNOzFv3ol5807Mm3di3oiIiIiIfAv7+PIwTvIxVvIwTvIxVvIwTvK5M1aCIKBzq2C00KiQXaJDsdaAMI0/okPViAxWeVT++J5yDBbF7WB+05FMycnV/w8LA6Ki3NYM5s07MW/eiXnzTsybd2LeiIiIiIh8C/v48jBO8jFW8jBO8jFW8jBO8rk7VoIgICokAFEhAW5rgxzujpOv4AbLdjCZTLh48SJMJpO7m+L5dDrg/Pnq24mJgBuvXmHevBPz5p2YN+/EvHkn5o2IiIiIyLewjy8P4yQfYyUP4yQfYyUP4yQfYyUP4+QYbi+KZ2Zm4t5778XAgQMxYsQIvPrqqzaTajKZsHz5cowcORJ9+/bFTTfdhB07dri8vVqt1uWv6ZXOngVEsfp2YqJ72wLmzVsxb96JefNOzJt3Yt6IiIiIiJzHHeOW7OPLwzjJx1jJwzjJx1jJwzjJx1jJwzg1nduXT587dy66d++On376Cfn5+bjvvvsQGRmJu+66y+px69atw8aNG/Hpp58iPj4ee/fuxYMPPoiEhARcdtllbmo91cm8dDoAdOnivnYQERERERERERE1AsctiYiIiHyHW2eKJyUl4dSpU5g/fz5CQkLQvn173HnnnVi/fn2tx544cQKXX345EhISoFQqMWLECISHhyPZsvhKnsMyLx4wU5yIiIiIiIiIiEgujlsSERER+Ra3FsVPnDiB2NhYhIWFSce6d++OtLQ0lJWVWT326quvxsGDB3Hy5Eno9Xr8/PPP0Ol0GDBggMvaKwgCWrVqBcGN+2N7DQ8qijNv3ol5807Mm3di3rwT80ZERERE5DzuGLdkH18exkk+xkoexkk+xkoexkk+xkoexskx3Lp8elFREUJDQ62OmTuahYWFCA4Olo5fe+21OHnyJMaPHw8AUKvVePnll9GmTZt6X8NkMlnt9aNQKGrt/SMIAgRBkHXcsk22Hg8Aonkv7QaOKxQKiKLokONNOaeGjjfmnHD6NMwfTVPHjlD881h3nJMoiggODpZeh3nynnMKCQmp9brefk622u5r52T+PemotnvCOdV33FfOyfL3pK+cU1Pb7g3nZOv3pLPOydbeiUREREREvspd45aW380Afoeq63h9Yw/eek71tb0p5yT3+743nZPc4/a0MTQ0FCaTqdY4pDefk7PyFBwcLI27+8o5Nbbt9R2vWZPw9nNiHcD9eTK/p+r7/HnbOck57shxS7fvKV7zBOuyZcsWbNmyBRs3bkRiYiL279+PefPmoU2bNujVq1edP5eZmQmlUgkACA0NRatWrZCXl4eSkhLpMREREYiIiEB2drbVRvWtWrVCaGgoMjIyoNfrIYoiKioq0KFDBwQHB+PcuXNWgY6Li4Ofnx9SU1Ot2pCQkICqqiqcP39eOqZQKJCQkACdToeLFy9Kx1UqFeLi4lBaWopLly5JxzUaDWJiYlBYWIiCggLpeFPPySwmJgYajcYx56TVIuDkSSgBGGJikJWfj7igILedU1paGrRaLQIDAyEIAvPkJefUtm1b5OTkQK/XS79Yvf2cfDFPNc/J/Huya9euMJlMPnFOvpinmueUnp6OiooKBAYGQqlU+sQ5+WKeap5T+/btkZaWBpPJJP2edOY5GY1GEBERERE1J64et4yMjMTp06ehVCqlPj6/Q9U+J/PYQ+fOnaFSqXzinJyVp5ycHOn7flBQkE+ckzPyFBkZiZKSEoiiCIPB4BPn5Kw8iaKIyspKdOvWDRUVFT5xToDj83Tp0iXk5uZKNQlfOCfWAdybJ/PfvoiICMTGxvrEOTkyT3LHLQVRbu/OCTZs2ICVK1di165d0rFjx45hypQpOHz4MIKCgqTjkyZNwqhRozB79mzp2AMPPIA2bdpg4cKFtZ5bq9Xi5MmTSExMhEajkY435UoJk8mEtLQ0aX8gb7tSorHH7T6nnBwI0dHV948aBXHnTreeU1VVFdLS0tChQwcoFArmyUvOSRRFpKamSnnzhXPyxTzVbLvl70lze7z9nBo67gvnZDQarX5P+sI5+WKebHXZUlJSav2edNY5abVaJCcno2vXrlZ9q+bC3LdsruffGEajEUePHkWfPn2kwV7yDMyN52JuPBdz47m8NTcpKSmYOnM22t8wB6Gt2rq7OQCAkksZOPftu/jyo5Xo2LFjk5+vvtx4Q9/KHeOWQO0+Pr9D1T9Ga2vswRvPqaG2N/ac7Pm+7y3n5Iw8iaKItLQ0tG/fvtY4pLeek7PyZP78dezYEYIg+MQ5NaXtdR23VZPw9nNiHcC9eTJ/9jp06AA/Pz+fOCe5xx05bunWmeI9evRAVlYWCgoKEBERAQBISkpCp06drDqWQPUv25qVfsurF+pi/mNf81hdj23ouDnY9T3efL+c45bP15TjTTknOcftOqczZ/69fdllEGp04m09h7PPSRCEWu+FZp8nDz8n88xHW59hc3ua2nbmyTnnZG6Do9ruCefU0HFvPydbvye9/Zx8MU81j9f3e9IZ51TXY4iIiIiIfJE7xi3r6+PzO5TtMdq6XtP8mKa23dO/F9bXRvN7Se73fW85J2fkyVxYqWsc0hvPqbHH5ZyT+bYvnVNDbWzMOdn6fe7t52RPG1kHcHyezHGq7/Hedk5yjjty3NKto5vdunVDz549sWzZMpSVlSElJQUff/wxbr31VgDAddddhz/++AMAMHLkSHz11Vc4deoUqqqqsG/fPuzfvx/XXHONO0+BbElO/vd2ly7uawcREREREREREVEjcNySiIiIyLe4fU/x5cuXY9GiRbjyyisRHByMqVOnYtq0aQAg7QUNAPfddx+qqqrwwAMPoKCgALGxsVi6dCkGDx7ssrYKgoCYmJg6r4Sgf1gWxRMT3deOfzBv3ol5807Mm3di3rwT80ZERERE5FyuHrdkH18exkk+xkoexkk+xkoexkk+xkoexskx3F4Uj46OxurVq23el2xRXPX398cjjzyCRx55xEUtq00QBI/d58ijeGBRnHnzPsybd2LevBPz5p2YNyIiIiIi53L1uCX7+PIwTvIxVvIwTvIxVvIwTvIxVvIwTo7BzSHtYDKZkJqaWmtTd6rh9Onq/wcGAu3aubctYN68FfPmnZg378S8eSfmjYiIiIjIt7CPLw/jJB9jJQ/jJB9jJQ/jJB9jJQ/j5BgsituJb7gGVFUBKSnVtzt3BmRubu9szJt3Yt68E/PmnZg378S8ERERERH5Fvbx5WGc5GOs5GGc5GOs5GGc5GOs5GGcms4zKpbkO9LSAIOh+rYHLJ1ORERERERERERERERERM2b2/cUJx9jXjodALp0cV87iIiIiIiIiIiIiHyQKIrIK9Mju0SHYq0BYRp/RIeqERmsgiAI7m4eERGRR2JR3A6CICAuLo4di/okJ/9720NmijNv9sks0uFASh4yiyoQGx6IQR0jERuudnk7mDfvxLx5J+bNOzFvRERERES+hX18eZp7nERRxJlLZdiTnIv0fC2MogilQkB8Sw2Gd4lC51bBUmyae6zkYpzkY6zkYZzkY6zkYZwcg0VxO/n5MWT18sCiOMC8yZVZpMPqvam4WKSDRqXE8cxiJGWWYNawBLcUxpk378S8eSfmzTsxb0REREREvoV9fHmac5zyyvTYk5yL1Lxy6ZjRJCI1t/rfLTQqRIUESPc151jZg3GSj7GSh3GSj7GSh3FqOu4pbgdRFJGamgpRFN3dFM9lWRT3kOXTmTf5DqTk4WKRDonRIYhvGYTE6BBc/GfmuKsxb96JefNOzJt3Yt6IiIiIiHwL+/jyNPc4ZZfokJ6vrXVco1IiQKmAzlAlHWvusZKLcZKPsZKHcZKPsZKHcXIMXlZAjmXeUzwqCmjRwr1tIbtlFlVAo1JC8c8SHApBgEalRGZRhZtbRkRERERERERERMVaA4z/FEU0KiXiWmjQuXUwAvwVyCmpRFJGMYp1BkSHqtEyyN/NrSUiIvIcLIqT45SUAFlZ1bc9aOl0ki82PBDHM4thEkUoBAEmUYRWb0RseKC7m+ZUnrKPOhERERERERERUX3CNP7wEwR0jw1Fm7BAhAT6oVxvxPdJ2UjNL4dGpUSYWlW9x3jnSIQGB7u7yURERB6BRXFyHPMscYBFcS81qGMkkjJLkJxdCo1KCa3eiJhwNQZ3jPTZwrGn7aNORERERERERERkiyiKiNAEYFDHCJhEEd8dz8KghJbYfiwLh9MLERUSgI5RwTCaTNV7jIsiRncJd3eziYiIPAKL4nYQBAEJCQkQ/llammqwLIp7yH7iQPPLW1OK17HhaswalmD184M7RkIEXF44dlXeLPdRN8+OT84uxYGUPEy8vJ1TX9sXNbfPm69g3rwT80ZERERE5FvYx5enucZJFEWcuVSGnGId2kcG4YP/S4OhyoTSiiocPl8IpVJARZUROaUVUKuUCPRXIj1fi9Kqls0uVvZqru+pxmCs5GGc5GOs5GGcHINFcTtVVVXB3597sdiUnPzvbQ+bKd5c8uaIWc+x4epaxeBNhy+4pXDsirxxH3XHay6fN1/DvHkn5o2IiIiIyLewjy9Pc4xTXpkee5Jz4a9UoMokokRnQJ924dAZTAj0UyLATwE/pYDSiipo9VUI9FfCKIoo0urd3XSv0BzfU43FWMnDOMnnKbESRRF5ZXpkl+hQrDUgTOOP6FA1IoNVHlGM9pQ4eTOFuxvgTURRxPnz5yGKorub4pmcUBTPLNJh0+ELWP7zGWw6fAGZRTq7n6M55c1y1nN8yyAkRofg4j8zx5vCHYVjV+UtNjwQWr0Rpn9ep7nso+4szenz5kuYN+/EvBERERER+Rb28eVprnHKLtEhPV+LAH8FMgorkFGkQ4nOgBYafwDiP2NbAkQR0FeZAABKQUCgwtTsYmWv5vqeagzGSh7GST5PiZV5NY4tRzKx/mAGvk3KxvpDGdhyNBNnLpV5RPs8IU7ejkVxchxzUVypBBISmvx05lnPO0/kIDW3DDtP5GD13tRGFcabC2cVr325cDyoYyRiwtVIzi5Fen45krNLpX3UiYiIiIiIiIiI3E0UReSV6pFbVoHsYh2CA5UwGk3443wh/JUKdI8Jg9FUPW4nCIDKr3rYP76lBlHBnFVIRNQQ82ocqXnlMP5TBzGaRKTmlmPP6VzklXHVDV/A5dPJMUTx3z3FO3QAVKomP6Wn7PXclD26XS02PBDHM4thEkUpZo4oXg/qGImkzBIkZ5dCo1JCqzf6TOG4rn3UYzw0x0REZJ/MzEz873//wx9//AGlUolhw4bhv//9L0pKSnDNNddAVaPP8sgjj+Duu+92U2uJiIiIiIisiaKIi0U6GE0i0vO1uFRaiQ6RwejYKhh/Z5Xg74vFGNszGkEBftVjdwFKhAT6IyEqCMM7RyIQWnefAhGRxzOvxmFLep4W2SU6RIUEuLhVtnn6Mu+ejEVxOykUrplc702FWABAZiag/ecXhsOWTnfcrOfG5s0Re3S7krOK1+4qHLvq82ZrH3VqPFfljRyLefNOzFvDZs+ejR49emDXrl0oLS3FAw88gJdffhn3338/ACApKcnNLSQiIiIi+hf7+PI0pzjllenx88lLiG+pQfeYMBy9UIS/L5ZgdNfWEATgl+RLUKuUmNA3FkaTCBEiokMDER2mRssgf5w717RtFZuL5vSeairGSh7GST5PiFWx1iDNEK/JKIoo1hpc3KLazHE6c6kMe5JzkZ6vhVEUoVQIiG+pwfAuUejcKpiF8XqwKG4HhUKBBAcsC94QcyG25PxFtC3KRrLBhIKQANzcNwatQjx0ueojR/693aWLQ57SUbOem5I3T5mtLpczi9euLhy76vNGjsW8eSfmzTsxbw0rKSlBjx49MG/ePAQFBSEoKAgTJkzA2rVr3d00IiIiIqJa2MeXp7nFKbtEh9M5ZVD5KXBt99YAgP87kwsAuL5HGwT4KaBQCGgdGoDYcA1C1X5WBZHmFKvGam7vqaZgrORhnOTzlFiFafyhFASbhXGlQkCYxr1bUZjjlFtaKS3zbmZe5h0AWmhUHjOj3ROxKG4HURSh0+mgVqudeqXFgZQ8BP7xOxb97z4ojVVOex2ncdBMcUfNem5K3py1R7cz+cqsZ1d93sixmDfvxLx5J+atYaGhoXjxxRetjmVlZaFVq1bSv5944gn89ttvqKqqwn/+8x889NBD8Pev+4uO0WiE0Wh0Wpt9iTlOjJfnYW48F3PjuZgbz+WtuTEajYAoQvznP08giiIgig7r79SXG2/Ll6uwjy9Pc4uTefbiXxnF6NU2DON6t8HViVEo1hkQ4KdE95hQxLawHYvmFqvGYpzkY6zkYZzk85RYRYeqEd9SY1VsNotvqUF0qHtXDhZFEZWVlcgurvCaZd49EYvidhBFERcvXkRCQoJTP5yZRRUY8Ocv3lkQB4DBgx3yNI6a9dyUvDlrj25qmKs+b+RYzJt3Yt68E/Nmv6SkJHz22Wd47733oFKp0LdvX4wePRovvPACTp48iblz58LPzw8PP/xwnc9x+vRpF7bYN3CJes/F3Hgu5sZzMTeey9tyk5GRAa1Oh9KyMoiBJe5uDgCgrKwMWp0OJ0+eRGlpqcOe19ty407s48vT3OJkOXvxWEYxzlwqQ7sWGgT4K5BTWoF2lWq0FTQ2f7a5xaqxGCf5GCt5GCf5PCVWkcEqDE+MAoTq4nLNZckjg1VuaxtQHaeioiIU6ZQev8y7J2NR3APFhgci7NJF6d9J103CJaMS8S016NQqxI0tk+Hqq4FevRz2dO6e9eysPbqJiIjItQ4fPoz7778f8+bNw5AhQwAAX375pXR/r169cN999+H999+vtyjepUsXaDS2B5zImtFoRFJSEnr27AmlUunu5pAF5sZzMTeei7nxXN6am5CQEGjUaoQEByM0NNTdzQEACBUl0KjV6Nq1Kzp27Njk56svN1qtlhcbEslUc/aiVm9Eck71hSsJUUGICubkHSKiphIEAZ1bBaOFRoXsEh2KtQaEafwRHapGZLDKIy5uMJlMCFcHevQy756ORXEPNKhjJFCUAwAwCQLe/c9jaB0Ziq7DEgAH7A1N8jlzj24iIiJyjV27duHxxx/HokWLMH78+DofFxsbi7y8PIiiWOeXHaVS6VUD7p6AMfNczI3nYm48F3PjubwtN0qlEhAECP/85wkEQQAEweGxtPV83pQrInfz9NmLRES+QhAERIUEeOzy4xUVFYiOivDoZd49HYvidlKpnN/JiA1Xw1h0CQCgbdkKo3q3ZSG2iZqSN3fPVm/OXPF5I8dj3rwT8+admLeG/fnnn3jyySfx1ltvYejQodLx/fv34+jRo7j//vulY6mpqYiNjfWYgWkiIiIian7Yx5enOcWpqbMXm1OsmoJxko+xkodxko+xksfPzw8tg3ihVFOwKG4HhUKBuLg457+QTgdlbnVRPLhLRxZkm8hleSOHYt68E/PmnZg378S8NayqqgoLFy7E/PnzrQriQPWSpStWrEBMTAyuv/56nDp1Ch9++CHuvvtuN7WWiIiIiJo79vHlaY5xauzsxeYYq8ZgnORjrORhnORjrOSxjJOnL/PuyRTuboA3EUURJSUlEOvYxN5hzp//93Z8vHNfqxlwWd7IoZg378S8eSfmzTsxbw07evQoUlJSsHTpUvTs2dPqvxYtWuCNN97ARx99hCuuuAL3338/ZsyYgTvuuMPdzSYiIiKiZop9fHkYJ/kYK3kYJ/kYK3kYJ/kYK3ks42S+UKpnbDiGdo5Cz9hwRIUEsCAuA2eK20EURVy6dAnBwcHOfXNZFsV5hUyTuSxv5FDMm3di3rwT8+admLeGXXHFFUhOTq7z/tjYWIwePdqFLSIiIiIiqhv7+PIwTvIxVvIwTvIxVvIwTvIxVvIwTo7BmeKeKD3939ucKU5ERERERERERERERERE1GgsinsiFsWJiIiIiIiIiIiIiIiIiByCRXE7aTQa578Ii+IO55K8kcMxb96JefNOzJt3Yt6IiIiIiHwL+/jyME7yMVbyME7yMVbyME7yMVbyME5Nxz3F7aBQKBATE+P8F2JR3KFcljdyKObNOzFv3ol5807MGxERERGRb2EfXx7GST7GSh7GST7GSh7GST7GSh7GyTE4U9wOoiiioKAAoig694XMRfGICCA42Lmv1Qy4LG/kUMybd2LevBPz5p2YNyIiIiIi38I+vjyMk3yMlTyMk3yMlTyMk3yMlTyMk2OwKG4Hl7zpqqqAjIzq25wl7hD8ZeGdmDfvxLx5J+bNOzFvRERERES+hX18eRgn+RgreRgn+RgreRgn+RgreRgnx2BR3NNcvAgYjdW3WRQnIiIiIiIiIiIiIiIiImoSFsU9jeV+4nFx7msHEREREREREREREREREZEPYFHcTqGhoc59AcuiOGeKO4zT80ZOwbx5J+bNOzFv3un/2bv7+LjKAu3j15lpJsl0Mpk0kzQktNGEtkJbxAW1VWhdddXFBV+QF9lHBR8rLru68vKA6woLq4gry67Lqstald3VXV9WUFFXUXG3yGpVUDCFEjCBlE5Im2k7mSQzadKZ8/zRJjTNJL2nmcmc++T3/Xz40J5JJve5rkl659xzzqE3AAAAwF+Y45shJ3NkZYaczJGVGXIyR1ZmyGn+llR6ADYJBAJqbm4u7xfZtev5P7MoXhIL0htKjt7sRG92ojc70RsAAADgL8zxzZCTObIyQ07myMoMOZkjKzPkVBqcKV6EfD6vvXv3Kp/Pl++LcKZ4yS1Ibyg5erMTvdmJ3uxEbwAAAIC/MMc3Q07myMoMOZkjKzPkZI6szJBTabAoXqR0Ol3eL8CieFmUvTeUBb3Zid7sRG92ojcAAADAX5jjmyEnc2RlhpzMkZUZcjJHVmbIaf5YFPeayUXx2lopHq/sWAAAAAAAAAAAAADActxT3Etc9/lF8fZ2yXEqOx4AAAAAAAAAgLUikYj2jY5rID2mocyE6sNVaonWKh4JyeH4MwBgEWFRvAiO42jZsmXlmywkk1I2e/jPXDq9JBKprH7+u6R6BjLqTCW08ZS42mK1lR4WDJT9+w1lQW92ojc70RsAAADgL8zxzZBTcfYdqtbPfpNQ376scq6rYMBRe2NYm1c3aVVzhBzFa6oYZGWGnMyRlRlyKg0WxYsw+aIrm6PvJ75yZfm+ziKRSGW19YFe9aeyCoeC6nl8j3b0p7VlUwcL4xYo+/cbyoLe7ERvdqI3AAAAwF+Y45shJ3P7Rif0s94D6k1mprbl8q56B0clSQ3hkJrqqis1PM/gNWWOrMyQkzmyMkNOpcE9xYuQz+fV39+vfD5fni9w9KI4Z4rP2/aepPpTWa1eHtGyUF6rl0fUn8pqe0+y0kODgbJ/v6Es6M1O9GYnegMAAAD8hTm+GXIyNzCU1c7dg4dv23mMvmRGA+lsBUblPbymzJGVGXIyR1ZmyKk0WBQvUiaTOf4HnSgWxUsqkRpTOBRUwHE0MTGhgOMoHAoqkRqr9NBgqKzfbygberMTvdmJ3gAAAAB/YY5vhpzMpLITOjg+UfCxnOtqKFP4scWI15Q5sjJDTubIygw5zR+XT/eSXbue/zOL4vPWFqvRjsSQ8kfeCZl3XWXGc2qL1VR4ZAAAAAAAAABQXrHaKgUDhe8/Gww4qg9XLfCIAABzcV1XyZFxDaSzGspMqD5cpZZorRqX8vO6FFgU95JFfKZ44shlzROpMbXFarShMz7v+35v6IyrK5FW955h5Q+Oa+DgsNpiYW3sjJdo1AAAAAAAAADgTS31NepojmrP6MzLp7c3htUSnd/xVwBA6biuq6f2jmhb96D69mWUc10FA47aG8PavCquaCRS6SFaj0XxIjiOo+bmZjlO4XfXzdvkongwKLW2ludreFAildXWB3rVn8oqHApqR2JIXYm0tmzqmNfCeFusVls2dejnv0vq6b0pvbA5plecElfrPBfbsTDK/v2GsqA3O9GbnegNAAAA8Bfm+GbIyVzj0pBev/5kPdi7X33J7PQFltVNikdClR6iJ/CaMkdWZsjJHFk9Lzkyrm3dg+pNjk5ty+Vd9Q6OSnL1R+uWk9M8sSheBMdxFI1Gy/cFJhfFTz5ZWrJ4qtnek1R/Kqs1LXUKOI7yrqvugWFt70nqgjNXzOu522K1ettZKyTN73mw8Mr+/YayoDc70Zud6A0AAADwF+b4ZsjJXCAQ0ItaY4pHwzMuxRuPhFhcOYLXlDmyMkNO5sjqeQPprPr2Fb5veF8yq2TmkE5q4Of2fAQqPQCb5PN57dq1S/l8vvRPPjIi7d9/+M+L7tLpYwqHggocmYQFHEfhUFCJ1FhJnr+svaFs6M1O9GYnerMTvQEAAAD+whzfDDmZy+fzevbZZ9W4tErr22I6e1WT1rfF1FRXzYL4UXhNmSMrM+RkjqyeN5SZUM6debsLScrl89p7YISc5olF8SKNj4+X54mPvp/4ypXl+Roe1RarUWY8p/yRb/a86yoznlNbrKZkX6NsvaGs6M1O9GYnerMTvQEAAAD+whzfDDmZIysz5GSOrMyQkzmyOqw+XKXgLG9YCgYcRWsWzxWmy4VFca84elF8kZ0pvqHz8H2+uweG1bdvVN0Dw2qN1WpjZ7zSQwMAAAAAAAAAAADKqiVaq/bGcMHH2hvDaopULfCI/Ie3FXjFIl4Ub4vVasumDm3vSSqRGlNbrEYbjyyUAwAAAAAAAAAAAH4Wj4S0eU2T5Eh9yYxyrqtgwFF7Y1ibV8VVo8L3G4c5FsWL4DiOWltby3O/lUW8KC4dXhi/4MwVZXnusvaGsqE3O9GbnejNTvQGAAAA+AtzfDPkZI6szJCTObIyQ07myOp5juNoVXNEDeGQBtJZDWUmVB+uUku0Vo1LqzQ2toSc5olF8SI4jqNwuPClC+ZtkS+Kl1NZe0PZ0Jud6M1O9GYnegMAAAD8hTm+GXIyR1ZmyMkcWZkhJ3NkNZ3jOGqqq1ZTXfWMx8hp/rineBHy+bx6e3uVz+dL/+S7dj3/55UrS//8i1hZe0PZ0Jud6M1O9GYnegMAAAD8hTm+GXIyR1ZmyMkcWZkhJ3N+zsp1XQ0OH1RXIqUHnxpUVyKlweGDcl236Ofyc04LiTPFi1S2F9zkmeLNzVIt99I+EYlUdtp9yTd0xtV25L7k/KCwE73Zid7sRG92ojcAAADAX5jjmyEnc2RlhpzMkZUZcjLnx6xc19VTe0e0rXtQffuOuT/46iatao4UfSl0P+a00FgU94Lxcam///CfuXT6CUmkstr6QK/6U1mFQ0HtSAypK5HWlk0dOik68zITAAAAAAAAAAAAQKklR8a1rXtQvcnRqW25vKvewcN/bwiHCl4iHeXF5dO9YPduafJyCVw6/YRs70mqP5XVmpY6tTcu1ZqWOvUfOXMcAAAAAAAAAAAAWAgD6az69mUKPtaXzGggnV3gEUFiUbwojuNo5cqVRV/S4LgmL50ucab4CUqkxhQOBRU40k3AcRQOBZVIjZWvN5QVvdmJ3uxEb3aiNwAAAMBfmOObISdzZGWGnMyRlRlyMufXrIYyE8rNcu/wnOtqKDNR1PP5NaeFxqJ4kZYsKcMV51kUn7e2WI0y4znlj/yQybuuMuM5tcVqJJWpN5QdvdmJ3uxEb3aiNwAAAMBfmOObISdzZGWGnMyRlRlyMufHrOrDVQrOsoAdDDiqD1cV/Zx+zGmhsSheBNd11dvbK3eWd3ecMBbF521DZ1ytsVp1Dwyrb9+ougeG1Rqr1cbOePl6Q1nRm53ozU70Zid6AwAAAPyFOb4ZcjJHVmbIyRxZmSEnc37NqiVaq/bGcMHH2hvDaonWFvV8fs1pofG2Ai9gUXze2mK12rKpQ9t7kkqkxtQWq9HGIwvl+Xy+0sMDAAAAAAAAAABACbmuq+TIuAbSWQ1lJlQfrlJLtFbxSKiilxqPR0LavKZJcg7fQzznugoGHLU3hrV5dZPikVDFxraYsSheBolUdtri7IbOuNpic7zrg0XxkmiL1eqCM1dUehgAAAAAAAAAAAAoI9d19dTeEW3rHlTfvpkLz6uaIxVbGHccR6uaI2oIhzy3YL+YsSheYolUVlsf6FX1w7/Spp99Tzp4UHtDQcVa6rQ0NEvcv/714f/X1Umx2IKNFQAAAAAAAAAAALBNcmRc27oH1ZscndqWy7vqHTz894ZwSE111ZUanhzHUVNddUXHgOlYFC+C4zjq6OiY8x0c23uS6j+Q0e2fuV51+/YW9wXa2yXeHVJyJr3Be+jNTvRmJ3qzE70BAAAA/sIc3ww5mSMrM+RkjqzMkJO5+WQ1kM6qb1+m4GN9yYwG0lnfLEjzmiqNQKUHYJtDhw7N+XgiNab4xGjxC+KSdPnlJzgqHM/xeoM30Zud6M1O9GYnegMAAAD8hTm+GXIyR1ZmyMkcWZkhJ3MnmtVQZkI51y34WM51NZSZmM+wPIfX1PyxKF4E13W1a9cuubN8k0lSW6xGS5LJqb//bsOrdeOtX9cP//N+6fHHZ/+vv1+6+uqF2I1Fx6Q3eA+92Yne7ERvdqI3AAAAwF+Y45shJ3NkZYaczJGVGXIyN5+s6sNVCs5y5nQw4Kg+XDXf4XkGr6nS4PLpJbahM670D5+/f0FvrEWB007Tuk0dUqy2giMDAAAAAAAAAAAA7NcSrVV7Y3jaPcUntTeG1RJlTQ7TsSheYm2xWp1/0vOxnrSqXe/d1KFWFsQBAAAAAAAAAACAeYtHQtq8pklyDt9DPOe6CgYctTeGtXl1k+KRUKWHCI9hUbxIgcDxrzjfOJqa+vP631vNGeIeYNIbvIfe7ERvdqI3O9EbAAAA4C/M8c2QkzmyMkNO5sjKDDmZO9GsHMfRquaIGsIhDaSzGspMqD5cpZZoreKRkJxZLq1uK15T88eieBECgYA6OjqO/4F79z7/5+bm8g0IRox7g6fQm53ozU70Zid6AwAAAPyFOb4ZcjJHVmbIyRxZmSEnc/PNynEcNdVVq6muuoSj8h5eU6XB2wqK4LquMpnM8W9kz6K4pxj3Bk+hNzvRm53ozU70BgAAAPgLc3wz5GSOrMyQkzmyMkNO5sjKDDmVBoviRXBdV/39/cd/0e3Z8/yfly8v76BwXMa9wVPozU70Zid6sxO9AQAAAP7CHN8MOZkjKzPkZI6szJCTObIyQ06lwaJ4ORx9pnhTU+XGAQAAAAAAAAAAAACLHIvi5TC5KB6NSjU1lR0LAAAAAAAAAAAAACxiLIoXKRQKHf+DJi+fzqXTPcOoN3gOvdmJ3uxEb3aiNwAAAMBfmOObISdzZGWGnMyRlRlyMkdWZshp/pZUegA2CQQCWrly5dwfNDYmpdOH/9zcXP5B4biMeoPn0Jud6M1O9GYnegMAAAD8hTm+GXIyR1ZmyMkcWZkhJ3NkZYacSoMzxYvguq7S6fTcN7IfHHz+zyyKe4JRb/AcerMTvdmJ3uxEbwAAAIC/MMc3Q07myMoMOZkjKzPkZI6szJBTaVR8UTyRSOi9732vXv7yl+v3f//3ddtttymfz8/4uHe/+91av379tP9OPfVUffrTn16wsbquq7179879opu8dLrE5dM9wqg3eA692Yne7ERvdqI3AAAAoLwW+rglc3wz5GSOrMyQkzmyMkNO5sjKDDmVRsUvn/7+979fa9eu1Y9//GPt27dPV1xxheLxuC6//PJpH/fFL35x2t/T6bTOPfdc/cEf/MFCDvf49u59/s+cKQ4AAAAAAABYyXfHLQEAABaxip4p3tXVpSeeeELXXnut6urq9IIXvECXXXaZvva1rx33cz/1qU/pD/7gD7RmzZoFGGkRWBQHAAAAAAAArObL45awmuu6Ghw+qK5ESg8+NaiuREqDwwc5axAAAEMVPVP8scceU1tbm+rr66e2rV27Vk8//bRGRkYUiUQKfl5fX5++9a1v6cc//vFxv0Y+n592WaNAIDDjMkeO48hxnONuz+fzqq2tnZpoFPp47dkjZ/JrNzVJ+fzh7dKMCUogEJDruiXZfqL7ZLK90NgruU/P7h/VL3r3KZEaU1usRhs64zq5ITznPtXW1k497sV98mNP893uuu603vywT37s6dixH/1zslRjr/Q+HW+7H/bp2J+TftgnP/ZU6EBHoZ+T5dqnQpeJBAAAAPyqEsctpZlzfH6HmvsYbaGvaeM+HW/skvS7vaPa9uRe9SUzyrmuggFH7fGl2ry6Sac0LS049mJ+3/fK77qV6Ml1XYXDcx9ftm2fytXT5Gtqcpsf9mk+Y59rn45dk/DDPs137KwDnPg+Hf2a8ss+mW4v5XHLii6Kp1IpRaPRadsmJ5oHDhyYdXL5uc99ThdccIGWLVt23K+RSCQUDAYlSdFoVM3NzUomk0qn01Mfs2zZMi1btkwDAwPKZDJT25ubmxWNRrV7926Nj49PbT948KDC4bCeeeaZaUGvXLlSVUfdU7z/0CGN9faqo6NDhw4d0q5du6YeCwQC6ujoUDabVX9//9T2UCiklStXanh4WHuPOus8HA6rtbVVBw4c0P79+6e2l2qfWltbZ92nJUuWqLe3d1quldqnbKBWd9y3Q/2prGqWBPTLQ3n9+pl9+tPXvki59GDBfdq1a5fy+byeeeYZT+6TH3sq1T6ddNJJvtsnP/ZUaJ8cx9HExISv9smPPR27T88884zv9knyX09H71NDQ8PUv2/l3qdcLicAAABgsajUccuqqqppc3x+h5p9n3K5nBzH8dU+FeopEAiorqlV/929R799emBquyNHOfdwDiF3mfKZIeXz+YL79Mwzz3hqn7zYU2trq3bt2uWrfSpnT4FAQJlMxlf7VMqe9u/fr2w2O/Xz3A/7xDqAN3rau3ev7/ZpIY9bOm6ht50tkDvvvFM//OEPdc8990xt6+vr0+te9zr9+Mc/1ooVK2Z8TiqV0tlnn63vf//7BR+flMlktHPnTq1Zs0bhcHhq+3zeKeG6rg4cOKBly5bN+jx65zvlfPnLkqT8Y49JL3qRZ94pcaLbC429Uvt0z6936wePDWjN8joFHEd519WTe0b0+rXL9ZaXtBXcp1wupwMHDqihoWFqm5f2yY89lWKfpMO/ZMZisan9sH2f/NjTsWM/+uek4zi+2KfjbffDPuXz+Wk/J/2wT37sqdD2/fv3z/g5Wa59ymQy6u7u1qmnnjptbrVYTM4tF+v+n4hcLqdHHnlEZ5xxxtTBXngD3XgX3XgX3XiXrd309PTokne/Ty9445WKNp9c6eFIktJ7d+uZ731WX/3iners7Jz3883VjQ1zq0octyw0x+d3qLmP0RY69mDjPh1v7I/1p/W1Xz2rXP6YQ/mOo6AjXfzSFVrb+vybOI4+U9z0932v/K5biZ6kw9+/9fX1M45D2rpP5epp8vuvsbGxZPta6X2az9hn215oTcL2fWIdoLI9TX7vNTQ0KBgM+mKfTLeX8rhlRc8UX7ZsmVKp1LRtqVRKjuPM+m7K+++/Xy984QvnnFgebfJdS8dum+1j59qez+eVSqWmxlbw4496d0OgpUU66mOO/oY+elsptp/oPpluL/Q1Z9tezn1KpMa0NLREwSOPBR1H4VBQidTYnGOf7C1wnD4qsU+l3O6nfTp60l7oc2zcp6O/rul22/bp6J+TpRp7pffJZLsf9unYn5N+2CfTMdq6T3P9nCzHPs32MQAAAIAfVeK45VxzfH6HKnyMdravOTme+Y7dK78XDmUnlHMlFRhLzj38+Gy/F5r+vu+V33XnGmO5esrn81NvSCn0OTbu04luP94+Hf39FwgEfLFPJmM8ke2F1iRs3ifWASrb07Hrk37YJ9PtpTxuWfTRzYsvvljf+MY3pp1mf6LWrVun5557btrp8F1dXTrllFO0dOnSgp9z//3365WvfOW8v3bZTF4+fckSqaGhsmPxobZYjTLjOeWPvFsk77rKjOfUFqup8MgAAAAAAABQSRy3hF/Vh6sUnGUhIhhwVB+uWuARAQBgn6IXxSORiG666SadffbZ+shHPqLf/va3J/zFTzvtNK1fv1633367RkZG1NPTo7vuuktvf/vbJUlveMMb9NBDD037nJ07d+rkk71xWamCJs8Ub24u+M49zM+GzrhaY7XqHhhW375RdQ8MqzVWq42d8UoPDQAAAAAAABXEcUv4VUu0Vu2NhS8H294YVku0doFHBACAfYq+fPoXvvAFHThwQD/4wQ/0X//1X7rkkkvU2dmpCy+8UG9605tUX19f1PPdcccduuGGG/TKV75SkUhEl1xyiS699FJJ0tNPPz3jnZ2Dg4OKxyu3ABqNRmd/MJ+XBgcP/7m5eWEGtMi0xWq1ZVOHtvcklUiNqS1Wo41HFsrnMmdv8Cx6sxO92Yne7ERvAAAAwPP8cNySOb6ZxZZTPBLS5jVNkiP1JTPKua6CAUftjWFtXt2keCQ06+cutqxOFDmZIysz5GSOrMyQ0/yd0D3FGxoa9Pa3v11vf/vbtWfPHt1333367ne/q9tvv12ve93rdPnll+u0004zeq6WlhZt3bq14GPd3d0ztu3YseNEhlwSgUBAzXMtdh84IB06dPjPy5cvzKAWobZYrS440+zeTJJBb/AkerMTvdmJ3uxEbwAAAMBMNh+3ZI5vZjHm5DiOVjVH1BAOaSCd1VBmQvXhKrVEaxWPhGa9x+tizOpEkJM5sjJDTubIygw5lUbRl08/Vl1dnRoaGtTQ0KBcLqdf//rXetvb3qbrrruuJPfv8ZJ8Pq+9e/cqn88X/oDJS6dLnCnuIcftDZ5Eb3aiNzvRm53oDQAAAJibbcctmeObWaw5OY6jprpqrW+L6exVTVrfFlNTXfWsC+LS4s2qWORkjqzMkJM5sjJDTqVxwoviv/zlL/UXf/EXeuUrX6kbb7xR8Xhc//Ef/6H7779fX//61/XII4/oox/9aCnH6gnpdHr2B1kU96w5e4Nn0Zud6M1O9GYnegMAAABmsvm4JXN8M+RkjqzMkJM5sjJDTubIygw5zV/Rl0//9Kc/rW9/+9vavXu3TjnlFF1zzTV605vepLq6uqmPWbdunW655RZdeeWVuvXWW0s6YE87elGcy6cDAAAAAAAAC4bjlgAAAJhN0Yvin/vc5/S6171Ot956q84666xZP+6UU07R+vXr5zU46+zZ8/yfOVMcAAAAAAAAWDActwQAAMBsil4U37ZtmxoaGrR///5p25999lmtWLFi6u8NDQ364he/OP8ReojjOFq2bNns92nh8umedNze4En0Zid6sxO92YnezCQSCX384x/XQw89pGAwqE2bNunDH/6wotGodu7cqVtuuUU7d+5UY2OjLrnkEr373e+u9JABAABwgmw/bskc3ww5mSMrM+RkjqzMkJM5sjJDTqVR9D3Fc7mcLrroIn3yk5+ctv3666/XRRddNGPS6SdFLYpz+XTP4IeFnejNTvRmJ3qzE72Zed/73qdoNKqf/OQnuueee/TUU0/pb/7mbzQ2NqYrrrhCGzZs0E9/+lP9/d//vf75n/9ZP/zhDys9ZAAAAJwg249bMsc3Q07myMoMOZkjKzPkZI6szJBTaRS9KH7bbbcpm83qwgsvnLb9uuuu06FDh2ZMOv0kn8+rv79f+Xy+8Adw+XRPOm5v8CR6sxO92Yne7ERvx5dOp7Vu3Tpdc801Wrp0qVpaWvSWt7xFDz30kP7nf/5HExMT+pM/+ROFw2GtXbtWF154ob72ta9VetgAAAA4QbYft2SOb4aczJGVGXIyR1ZmyMkcWZkhp9IoelH8f//3f3XLLbfozDPPnLb9jDPO0E033aQHH3ywZIPzokwmM/uDR58p3tRU/sHA2Jy9wbPozU70Zid6sxO9zS0ajerWW29VPB6f2vbcc8+publZjz32mNasWaNgMDj12GmnnaYdO3ZUYqgAAAAoAT8ct2SOb4aczJGVGXIyR1ZmyMkcWZkhp/kr+p7io6OjWrp0acHHotGoRkdH5z0oa00uitfXS9XVlR0LAAAApunq6tKXv/xl/dM//ZO+//3vKxqNTns8FosplUopn88rECj83tFcLqdcLrcQw7XeZE7k5T10411041104122dpPL5STXlXvkPy9wXVdy3ZLNd+bqplx9cdwSAAD4VTgcrvQQrFf0ovi6dev0L//yL7r55punHSwcHx/Xpz71Ka1du7akA7TK5OXTuZ84AACApzz88MP6kz/5E11zzTV6xSteoe9///sFP+5492Z68sknyzE8X+vq6qr0EDALuvEuuvEuuvEu27rZvXu3MtmshkdG5NakKz0cSdLIyIgy2ax27typ4eHhkj3vQnbDcUv4neu6So6MayCd1VBmQvXhKrVEaxWPhLjPLAD4zNE/81OZCdU4AVWNjiseqeZn/gkqelH82muv1WWXXab7779fp512mpYuXap0Oq2uri7lcjnddddd5RinJziOo+bm5sIvtmxWmvyFoUz3E0+kstrek1QiNaa2WI02dMbVFqsty9eqxNcrlzl7g2fRm53ozU70Zid6M/eTn/xE/+///T/dcMMNevOb3yxJWrZsmZ555plpH5dKpRSLxWY9S1ySVq9ezTtzDeVyOXV1dWn9+vXTLlOPyqMb76Ib76Ib77K1m7q6OoVra1UXicy4ek2lOGNphWtrdeqpp6qzs3PezzdXN5lMpixvNrT9uCVzfDOLNSfXdfXU3hFt6x5U376Mcq6rYMBRe2NYm1c3aVVzZEYmizWrYpGTObIyQ07myKqwmT/z88ofOqRTThrV5tXNBX/m4/iKXhR/8YtfrG9961v6t3/7N3V1dWn37t1qbGzUeeedp3e84x3q6Ogoxzg9wXGc2X9RGRx8/s9lWBRPpLLa+kCv+lNZhUNB7UgMqSuR1pZNHWVZqF7or1dOc/YGz6I3O9GbnejNTvRm5te//rWuv/56/cM//IPOPvvsqe3r1q3TV77yFR06dEhLlhyeEnd1denFL37xnM8XDAatOuDuBWTmXXTjXXTjXXTjXbZ1EwwGJceRc+Q/L3AcR3KckmdZ6PnK1ZXtxy2Z45tZrDklR8a1rXtQvcnnbwOQy7vqHTz894ZwSE1102/puVizKhY5mSMrM+RkjqwKm/kz31FgSZV6BzOSBgv+zMfxFb0oLknt7e264YYbZmwfGxvTz3/+c23cuHHeA/OifD6v3bt36+STT555BtHkpdOlslw+fXtPUv2prNa01CngOMq7rroHhrW9J6kLzlxh/dcrpzl7g2fRm53ozU70Zid6O75Dhw7pIx/5iK699tppC+KStHnzZkUiEf3TP/2T3vOe9+jJJ5/UN77xDd12220VGi0AAABKwebjlszxzSzWnAbSWfXtyxR8rC+Z0UA6O2OBZLFmVSxyMkdWZsjJHFkVNuNnvutqZHRUkaVLZ/2Zj+Ob1ytsfHx82n+/+MUvdOWVV5ZqbJ40Pj5e+IG9e5//c1nOFB9TOBRU4Mi7hwOOo3AoqERqrORfqxJfr9xm7Q2eRm92ojc70Zud6G1ujzzyiHp6evSxj31M69evn/bf4OCg7rzzTv3sZz/Ty172Mn3wgx/UVVddpVe96lWVHjYAAABKwNbjlszxzSzGnIYyE8q5bsHHcq6rocxEwccWY1YngpzMkZUZcjJHVjMV+pmfz+ckzf0zH3Mr+kzxVCqlG2+8UQ8++KCy2eyMx0txzyErlXlRvC1Wox2JIeVdd+rM7cx4Tm2xmpJ/rUp8PQAAgFI766yz1N3dPefHfOUrX1mg0QAAAKDcOG4JP6sPVynoOAUXxoMBR/Xhqjk/33VdJUfGNZDOaigzofpwlVqitYpHQp65jQMA4LD5/sxHYUUvit922216/PHH9cd//Me66667dMkll2h8fFw/+tGP9Ad/8Ae66qqryjFO7zt6UbwMl0/f0BlXVyKt7oFhhUNBZcZzao3VamNnvODHJ1JZbe9JKpEaU1usRhs640XdC7zYrwcAAAAAAABUEsct4Wct0Vq1N4an3VN8UntjWC3RuY/9PrV3RNu6B9W3L6Oc6yoYcNTeGNbm1U1a1RxhYRwAPGS+P/NRWNGL4g8++KBuv/12nXXWWfryl7+sd73rXVqxYoWuu+46vec979Gjjz7q28tOOo6j1tbWwhOEo+8pXpYzxWu1ZVPHtIXujZ1xtRZY6E6kstr6QK/6U1mFQ0HtSAypK5HWlk0dxgvjxXw9r5uzN3gWvdmJ3uxEb3aiNwAAAGA6249bMsc3s1hzikdC2rymSXIO30P82IXteCQ043Mms9o3Oq5t3YPTFldyeVe9g4f/3hAOLep70y7W19SJICsziyGnUl19YjFkdSJm/MyXq7pIRB3NS2f9mY/jK3pRfN++fVqxYsXhT16yRAcPHpQkRSIRfehDH9KNN97o6cnlfDiOo3A4XPjBMl8+XTq8UH3BmSuO+3Hbe5LqT2W1pqVu6tLn3QPD2t6T1IbOuPEZ5KZfz+vm7A2eRW92ojc70Zud6A0AAACYzvbjlszxzSzWnBzH0armiBrCIeNFqMmsehIp9e3LFHzevmRGA+nsol8UX4yvqRNBVmb8npPruiW7+oTfszpRJ/IzH8cXKPYTli1bpt7eXklSPB7Xjh07ph6LxWLatWtX6UbnMfl8Xr29vcrn8zMfLPPl04uRSI0pHAoqcOSbIuA4CoeC2vncsLY+0Kv7Htuj3sER3ffYHm19oFeJ1Mx7LPnJnL3Bs+jNTvRmJ3qzE70BAAAA09l+3JI5vpnFnJPjOGqqq9b6tpjOXtWk9W0xNdVVz7o4ks/ntWfPHqUyEwXvSytJOdfVUGainMP2vMX8mioWWZnxe07JkeevPjH5s2Xy6hPbnhxUcmTc+Ln8ntV8HP0z/xWdjYoHsmpcWsWC+DwUfab46173Ol111VX6xje+oXPOOUe33nqrJiYm1NDQoH//939XW1tbOcbpGbN+Y05ePr2qSqqvX7gBFdAWq9GOxJDyrjt1pnhmPKeRgxPaPzpR8AxyP5wRPhd+oNqJ3uxEb3aiNzvRGwAAAPA8Pxy3ZI5vhpzM5XI5xWqrFXScggvjwYCj+nBVBUbmLbymzJGVGT/nNJDOlvTqE37OqpRGR2feXxzFKXpR/Oqrr1Ymk1FNTY2uuOIK/eIXv9ANN9wgSaqvr9ftt99e8kFaYfJM8eZmqcLv0tjQGVdXIq3ugWGFQ0FlxnNqjdWqKuhobCI/4wzyRGqsouMFAAAAAAAA5ovjlsBMY2NjamlapvbG8LR7ik9qbwyrJVr49poAUMgQV5+ApYpeFA+Hw/r4xz8+9fdvf/vbevLJJzUxMaGOjg7V1i7Cf0DzeWlw8PCfK3zpdOnwvcC3bOqYdu/wjZ1x/bwnqb59mRlnkLfFaio9ZAAAAAAAAGBeOG4JzJTP59W4NKTNa5ok5/BZnMfe/zceCVV6mAAsUh+u4uoTsFLRi+J//ud/rptuukkNDQ1T21avXl3SQXmV4zhauXLlzOv1798v5XKH/9zcvPADK6AtVjvjkuiznUG+sTNeoVEujFl7g6fRm53ozU70Zid6AwAAAKaz/bglc3wz5GTu6KxWNUfUEA5pIJ3VUGZC9eEqtURrFY+EFn2WvKbMkZUZv+fUEq0t2dUn/J5VqZBTaRS9KP7www/rueeemza5XEyWLCkQ2eSl0yXPLIoXMtsZ5K0x/79LtmBv8Dx6sxO92Yne7ERvAAAAwPP8cNySOb4ZP+fkuq6SI+MlW7yezMpxHDXVVRd1n9/FxM+vqVIjKzN+zikeKe3VJ/ycVSmR0/wFiv2Em266Sbfddpu2bdumffv2aXx8fMZ/fuW6rnp7e+Uee0kIw0XxRCqrux9+Vnfc/5TufvhZJVLZMo10dpNnkH/gNat0wZkrFsWC+Ky9wdPozU70Zid6sxO9AQAAANPZftySOb4ZP+fkuq6e2juib/0moa/9cre+1zWgr/1qt771SEJP7R0pep/9nFUpkZM5sjLj95wmrz7x5jPadPHLTtYb17fo4peerDef0aZVzZGi3sDj96xKhZxKo+i3FfzFX/yFDh06pPe9730FH3ccR48//vi8B2aVoxfFZ7mneCKV1dYHetWfyiocCmpHYkhdibS2bOpQ2yJYmAYAAAAAAADKieOWsF1yZFzbugenXZI4l3fVO3j47w3hEGd6A/AErj4BGxW9KP7Od76Ta9Yfa8+e5/88y5ni23uS6k9ltaalTgHHUd511T0wrO09yRn3/gYAAAAAAABQHI5bwnYD6az69mUKPtaXzGggnWUBCgCAE1T0ovj73//+cozDbgaXT0+kxhQOBRU4MjEPOI7CoaASqbGFGCEAAAAAAADgaxy3hO2GMhPKzXJp3JzraigzscAjAgDAP4peFH/wwQeP+zFnn332CQ3G6xzHUUdHx8x3nBpcPr0tVqMdiSHlXXfqTPHMeE5tsZoyjhjSHL3B0+jNTvRmJ3qzE70BAAAA09l+3JI5vhk/51QfrlLQcQoujAcDjurDVUU9n5+zKiVyMkdWZsjJHFmZIafSKHpR/D3veY8cx5l2M/djS9i5c+f8R+ZRhw4dUlXVMZMPg8unb+iMqyuRVvfAsMKhoDLjObXGarWxM17G0WJSwd7gefRmJ3qzE73Zid4AAACA5/nhuCVzfDN+zaklWqv2xvC0e4pPam8MqyVaW/Rz+jWrUiMnc2RlhpzMkZUZcpq/ohfF/+3f/m3Gtkwmo9/85jd64IEHdMMNN5RkYF7kuq527do1890YR58p3tRU8HPbYrXasqlD23uSSqTG1Bar0cbOuFpjxU9kUJxZe4On0Zud6M1O9GYnP/d2++2366KLLtKKFSsqPRQAAABYxPbjln6e45eSn3OKR0LavKZJcg7fQzznugoGHLU3hrV5dZPikVBRz+fnrEqJnMyRlRlyMkdWZsipNIpeFH/Zy15WcPurXvUqveAFL9C//uu/6vd+7/fmPTCrTC6KNzRIodknJm2xWl1wJgd3AQAA5vKVr3xFn//853XWWWfpoosu0utf/3qF5phjAQAAABLHLWE/x3G0qjmihnBIA+mshjITqg9XqSVaq3gkxEIIAADzECjlk5111ln63//931I+pR0mL58+y6XTAQAAYO5nP/uZ/vEf/1FNTU268cYbdc455+hjH/uYnnjiiUoPDQAAAJZatMctYR3HcdRUV631bTGdvapJ69tiaqqrZkEcAIB5KvpM8bls375dwWCwlE/pOYHAMe8jyGSkkZHDf2ZR3LNm9AYr0Jud6M1O9GYnv/YWCoX02te+Vq997WuVzWb1k5/8RN///vd10UUXac2aNbrooov0pje9ibPHAQAAYMyW45Z+neOXGjmZIysz5GSOrMyQkzmyMkNO81f0ovgll1wyY5vrutq/f792796t8847ryQD86JAIKCOjo7pGwcHn//z8uULOyAYKdgbPI/e7ERvdqI3Oy2W3mpra/XGN75RZ555pr7yla/oC1/4gm644QZ96lOf0lVXXaW3ve1tlR4iAAAAPML245aLZY4/X+RkjqzMkJM5sjJDTubIygw5lUbRi+JVVVUztjmOozVr1ujCCy/UO97xjpIMzItc11U2m1Vtbe3zl6uZvJ+4xJniHlWwN3gevdmJ3uxEb3ZaDL1ls1n94Ac/0D333KOHH35YK1as0Ac/+EGde+65uu+++/TRj35UBw4c0JYtWyo9VAAAAHiA7cctSznHd11XyZFxX96XejH8LlQqZGWGnMyRlRlyMkdWZsipNIpeFP/Sl75UjnFYwXVd9ff3q6OjY+pFt69nlxqPPP74oWrVp7Jqi9VWbpCYoVBv8D56sxO92Yne7OTn3n71q1/pnnvu0X333afx8XG9+tWv1tatW/XKV75y6mMuv/xyNTY26vbbb2dRHAAAAJLsP25Zqjm+67p6au+ItnUPqm9fRjnXVTDgqL0xrM2rm7SqOWL17xB+/l2o1MjKDDmZIysz5GSOrMyQU2mc0AXon3jiCd1///3Ttv37v/+7nnjiiZIMyhaJVFYP/uzxqb//ZiykrQ/0KpHKVnBUAAAAdnvHO96h7du36z3veY/++7//W3fccce0BfFJL3/5y7Vv374KjBAAAABexXFLKTkyrm3dg+pNjirnupKkXN5V7+Cotj05qOTIeIVHCAAAsPCKXhT/2c9+pgsvvFA/+tGPpm3/6U9/qgsvvFA///nPSzY4r9vek9ShPc/fUzzS3qb+VFbbe5IVHBUAAIDd7rzzTt1///268sor1dTUNOvHLV++XDt27FjAkQEAAMDLOG552EA6q759mYKP9SUzGkhzQg8AAFh8il4Uv+OOO3TxxRfr4x//+LTtd955p/7P//k/+tSnPlWqsXlSKBSa+nMiNab+NacrHwhovDas59a/VOFQUInUWAVHiEKO7g32oDc70Zud6M1Ofu3tnHPO0d/93d/pb/7mb6Ztv+KKK/TJT35SuVyuQiMDAACAl/nhuGUp5vhDmYmpM8SPlXNdDWUm5v01Ks2vvwuVA1mZISdzZGWGnMyRlRlymr+iF8W7u7v1rne9S4HAzE+99NJL9eSTT5ZkYF4UCAS0cuXKqX1vi9Wo65Qz9MUv/EBf+NJPNFrfoMx4Tm2xmgqPFEc7tjfYgd7sRG92ojc7+bm3z3zmM/qP//gPveAFL5i2ffPmzbr77rv1T//0T5UZGAAAADzN9uOWpZrj14erFJzlfqPBgKP6cNW8nr/S/Py7UKmRlRlyMkdWZsjJHFmZIafSKDq9pUuX6rnnniv42HPPPadwODzvQXmV67pKp9Nyj7zTckNnXK2xWv0y0KDuiZC6B4bVGqvVxs54hUeKox3bG+xAb3aiNzvRm5383Nt3vvMd3Xbbbbr44ounbb/00kt166236tvf/naFRgYAAAAvs/24Zanm+C3RWrU3Ft7X9sawWqK183r+SvPz70KlRlZmyMkcWZkhJ3NkZYacSqPoRfHXvva1uuGGG/STn/xEyWRS2WxWe/bs0Xe+8x1de+21es1rXlOOcXqC67rau3fv1IuuLVarLZs69Pq1y9XRFNHr1y7Xezd1qDVm98TSb47tDXagNzvRm53ozU5+7m3v3r1avXp1wcde9KIXae/evQs8IgAAANjA9uOWpZrjxyMhbV7TpI6mpVNnjAcDjjqalmrz6ibFI3ZfftXPvwuVGlmZISdzZGWGnMyRlRlyKo0lxX7Ctddeqz//8z/XlVdeKeeoy/C4rquXv/zluu6660o6QK9ri9XqgjNXVHoYAAAAvrFy5Ur9z//8j97xjnfMeOw73/mOVqxg7gUAAICZOG55mOM4WtUcUUM4pIF0VkOZCdWHq9QSrVU8EpqWDQAAwGJR9KJ4JBLRF77wBT322GP67W9/q+HhYS1btkyrV6/W6aefXo4xAgAAYBF597vfrY985CP65S9/qfXr12vp0qVKp9P61a9+pZ///Oe65ZZbKj1EAAAAeBDHLZ/nOI6a6qrVVFdd6aEAAAB4QtGL4pNOOukkrV27durvzz77bEkG5HVev/cQCqM3O9GbnejNTvRmJ7/29pa3vEVLlizR5z73Of3oRz+SJAUCAb3whS/Urbfeqje/+c2VHSAAAAA8zebjln6d45caOZkjKzPkZI6szJCTObIyQ07zV/SieDKZ1JVXXqmOjg594hOfmNp+/fXX69ChQ7rzzju1bNmykg7SKwKBgFpbWys9DBSJ3uxEb3aiNzvRm5383tt5552n8847TwcPHlQ6nVZDQ4OWLDnh93MCAABgEbD9uKXf5/ilQk7myMoMOZkjKzPkZI6szJBTaQSK/YTbbrtN2WxWF1544bTt1113nQ4dOqRPfvKTJRuc17iuq/3793Mje8vQm53ozU70Zid6s9Ni6a26ulpNTU0siAMAAOC4bD9uuVjm+PNFTubIygw5mSMrM+RkjqzMkFNpFL0o/r//+7+65ZZbdOaZZ07bfsYZZ+imm27Sgw8+WLLBeQ0vOjvRm53ozU70Zid6s5Ofe9u1a5e2bNmil7/85Tr11FNn/HfaaadVeogAAADwINuPW/p5jl9K5GSOrMyQkzmyMkNO5sjKDDmVRtGn3IyOjmrp0qUFH4tGoxodHZ33oAAAALB43XDDDXriiSf0mte8RsuWLZPjOJUeEgAAACzAcUsAAADMpuhF8XXr1ulf/uVfdPPNNysQeP5E8/HxcX3qU5/S2rVrSzpAAAAALC5dXV2688479bKXvazSQwEAAIBFOG4JAACA2RS9KH7ttdfqsssu0/3336/TTjtNS5cuVTqdVldXl3K5nO66665yjNMzotFopYeAE0BvdqI3O9GbnejNTn7tLRKJqLm5udLDAAAAgGX8cNzSr3P8UiMnc2RlhpzMkZUZcjJHVmbIaf6Kvqf4i1/8Yn3rW9/SH/7hHyqdTqu7u1vj4+M677zzdPfdd+uMM84owzC9IRAIqLm5edo7TeF99GYnerMTvdmJ3uzk594uuugifeMb36j0MAAAAGAZ249b+nmOX0rkZI6szJCTObIyQ07myMoMOZVG0WeKS1J7e7tuuOGGgo/19/ertbV1XoPyqnw+r2QyqXg8zgvPIvRmJ3qzE73Zid7s5OfeIpGI/uu//kvbt2/XGWecoXA4PO1xx3F01VVXVWh0AAAA8DKbj1v6eY5fSuRkjqzMkJM5sjJDTubIygw5lcYJLYofa3x8XD/84Q9199136xe/+IUef/zxUjytJ6XTacXj8UoPA0WiNzvRm53ozU70Zie/9vaJT3xi6s87duyY8TiL4gAAADBl23FLv87xS42czJGVGXIyR1ZmyMkcWZkhp/mb16L4jh07dPfdd+u//uu/NDQ0pHXr1unaa68t1dgAAACwCD3xxBOVHgIAAAAsx3FLAAAAHK3oRfFUKqVvf/vbuueee9Td3S3HcXT55Zfr7W9/u1asWFGOMQIAAGCRymazGhwcVGtrq5YsKclFjgAAAOBTHLcEAADAbIwuPO+6rrZt26YPfOADOuecc/SpT31Kp512mr785S/LdV2df/75i2Ji6TiOli1bJsdxKj0UFIHe7ERvdqI3O9Gbnfze27e//W294Q1v0O/93u/pDW94g5577jmlUil94AMf0MGDBys9PAAAAHiEn45b+n2OXyrkZI6szJCTObIyQ07myMoMOZWG0ek2r3rVqzQ4OKgXv/jFuvHGG3Xuuedq6dKl5R6b50y+6GAXerMTvdmJ3uxEb3byc2933323PvKRj+jVr361/viP/1i33XabpMP3g3ziiSf0j//4j1z6EgAAAJL8ddzSz3P8UiInc2RlhpzMkZUZcjJHVmbIqTSMzhTfs2ePVq1apQsuuEB/+Id/aO3Ecr7y+bz6+/uVz+crPRQUgd7sRG92ojc70Zud/NzbXXfdpauuukqf+cxn9I53vEPBYFCS1NzcrL/8y7/Ud7/73QqPEAAAAF7hp+OWfp7jlxI5mSMrM+RkjqzMkJM5sjJDTqVhtCj+5S9/WS960Yv0sY99TOecc46uv/56PfTQQ+UemydlMplKDwEngN7sRG92ojc70Zud/Nrbrl279IY3vKHgY6tWrVIymVzgEQEAAMCr/Hbc0q9z/FIjJ3NkZYaczJGVGXIyR1ZmyGn+jC6fftZZZ+mss87SDTfcoHvvvVff+MY39I53vEMrV66U4zgaGRkp9zgBAACwSDQ1NenZZ5/VypUrZzzW19en+vr6CowKAAAAXsRxSwAAAJgwOlN8UiQS0aWXXqp77rlH99xzj175yleqrq5O73rXu/TOd75TX//615VKpco0VAAAACwGZ555pm666Sb96le/kuu6U9u7u7v18Y9/XJs3b67g6AAAAOBFHLcEAADAXIpaFD/aqaeeqhtvvFEPPvigbr31VknSX/3VX+mcc84p2eC8xnEcNTc3y3GcSg8FRaA3O9GbnejNTvRmJz/3dt1116mqqkrvfOc79eIXv1jZbFbnn3++3vzmN0uSrr322soOEAAAAJ5m63FLP8/xS4mczJGVGXIyR1ZmyMkcWZkhp9Iwunz6XEKhkM4//3ydf/756uvr0z333FOKcXmS4ziKRqOVHgaKRG92ojc70Zud6M1Ofu4tHo/r29/+tn70ox/pt7/9rUZGRhSNRnXGGWfo93//91VVVVXpIQIAAMACth239PMcv5TIyRxZmSEnc2RlhpzMkZUZciqNEz5TvJD29nZdddVVpXxKT8nn89q1a5fy+Xylh4Ii0Jud6M1O9GYnerOT33urqqrSueeeqw996EP62Mc+puuuu06ve93rWBAHAADACbHhuKXf5/ilQk7myMoMOZkjKzPkZI6szJBTacz7TPHFZnx8vNJDwAmgNzvRm53ozU70Zie/9vZ3f/d3x/2Yq6++egFGAgAAACwsv87xS42czJGVGXIyR1ZmyMkcWZkhp/ljURwAAACe8rnPfW7Wx+rq6lRdXc2iOAAAAAAYiEQi2jc6roH0mIYyE6oPV6klWqt4JMS9aQEAiwqL4hWWSGW1vSepRGpMbbEabeiMqy1WW+lhAQAAVMwTTzwxY1smk9EjjzyiT3/60/rIRz5SgVEBAAAAgH32jAX0yycT6tuXVc51FQw4am8Ma/PqJq1qjrAwDgBYNE5oUXzPnj16/PHHNTQ0VPDxN7/5zfMZk2c5jqPW1taSTRQSqay2PtCr/lRW4VBQOxJD6kqktWVTBwvjJVTq3rAw6M1O9GYnerPTYustHA7rFa94hRzH0V//9V/rq1/9aqWHBAAAAA+y+bjlYpvjnyhyMrdvdFw/f3pIfQeykg7nlcu76h0clSQ1hENqqquu4Ai9gdeUObIyQ07myMoMOZVG0Yvi3/rWt3TDDTfo0KFDcl13xuOO43h6cjkfjuMoHA6X7Pm29yTVn8pqTUudAo6jvOuqe2BY23uSuuDMFSX7OotdqXvDwqA3O9GbnejNTou1t5NPPrngmeQAAACA7cctF+scv1jkZG4gPabdqYOaXBA/Wl8yo4F0lkVx8ZoqBlmZISdzZGWGnEqj6EXxz372s3rlK1+p97znPWpoaFhU70rI5/N65pln9IIXvECBQGDez5dIjSkcCipwJMOA4ygcCiqRGpv3c+N5pe4NC4Pe7ERvdqI3O/m5t/Hx8YLb9+/fr89//vOqr69f4BEBAADABrYft/TzHL+UyMlcKjOh1NCQ6urqpGO+H3Kuq6HMRIVG5i28psyRlRlyMkdWZsipNIpeFN+7d6+2bt2q9vb2cozH8/L5fMmeqy1Wox2JIeVdd+pM8cx4Tm2xmpJ9DRxWyt6wcOjNTvRmJ3qzk197O/3002c9gOm6rq6++uoFHhEAAABs4Ifjln6d45caOZmJ1VZptrWTYMBRfbhqYQfkYbymzJGVGXIyR1ZmyGn+il4U7+joUCqVsnpy6RUbOuPqSqTVPTCscCiozHhOrbFabeyMV3poAAAAFfOnf/qnBRfFo9GoTj/9dJ1xxhkLPygAAAB4Hsctgela6mvU0RzVntGZtxNobwyrJVpbgVEBAFAZRS+Kf+hDH9Lf/d3f6cYbb1RnZ2c5xrRotMVqtWVTh7b3JJVIjaktVqONnXG1xpiMAACAxev9739/pYcAAAAAC3HcEpiucWlIv/+i5frlrmH17csq57oKBhy1N4a1eXWT4pFQpYcIAMCCKXpR/OMf/7j279+vP/qjP1Jtbe2MG7s7jqOf/vSnJRuglziOo5UrV5b0fkRtsVpdcOaKkj0fZipHbyg/erMTvdmJ3uzk595+9atfFfXxL33pS8s0EgAAANjE9uOWfp7jlxI5mXMcR6e1Nai1MaqB9JiGMhOqD1epJVqreCREhkfwmjJHVmbIyRxZmSGn0ih6UfzUU08t6QASiYRuvvlmPfroowqHwzr33HN1zTXXFLxRfE9Pj2666Sb99re/VSwW0+WXX67LLruspOM5niVLio4MHkBvdqI3O9GbnejNTn7t7R3veMeck3zXdeU4ztT/d+7cuYCjAwAAgFf54bilX+f4pUZO5oLBoOKRJWqqq6n0UDyN15Q5sjJDTubIygw5zV/RCd56660lHcD73/9+rV27Vj/+8Y+1b98+XXHFFYrH47r88sunfdzY2Jje85736I//+I/1uc99Tk899ZQ+/OEP65xzzlmwyyG5rqve3l51dHTwbgyL0Jud6M1O9GYnerOTn3v77Gc/q1tuuUWbN2/WmWeeqUgkoqGhIf3yl7/U9u3bdc011ygSiVR6mAAAAPAY249b+nmOX0rkZI6szJCTObIyQ07myMoMOZXGCb+t4Ne//rUef/xxjY6Oqq6uTqeffrrWrVtX1HN0dXXpiSee0F133aW6ujrV1dXpsssu07/+67/OmFx+//vfVyQS0Xve8x5J0umnn67vfve7Jzp8AAAAeNT3vvc9/d//+3916aWXTtt+/vnn69/+7d90//3362//9m8rNDoAAAB4HcctAQAAcKyiF8X379+v9773vXrsscfkuu7UdsdxtGHDBn3605/W0qVLjZ7rscceU1tbm+rr66e2rV27Vk8//bRGRkamnQH08MMPa/Xq1fqLv/gL/ehHP1I8HteVV16p888/v9hdAAAAgIdt27ZNf/7nf17wsVe96lW64447FnhEAAAAsAHHLQEAADCbohfFb7vtNu3fv1//8A//oJe85CWKRCIaHh7WQw89pE984hP6+7//e33kIx8xeq5UKqVoNDpt2+RE88CBA9MmlwMDA3rooYf00Y9+VDfeeKN+8IMf6Prrr9cpp5yi0047bdavkc/nlc/np/4eCASm/V06PDF2HOe42/P5vFzXnZpUF/p4SdMm3XNtDwQC055vPttPdJ9Mtvthn1zXnXrcL/tkMnab92ny65nuqw375Meejh370T8nSzX2Su/T8bb7YZ+O/Tnph33yY0/Hbp/cduzzlGufjv2YcnJdV48//rhWrlw547GdO3cu6FgAAABgD9uPW0oz5/j8DjX3MdpCX9PGfTre2E90n4r5fd+WfSpHT0cf1/LLPpWrp8nX1OQ2P+zTfMY+1z4d/b3nl32a79hZBzjxfTr6NeWXfTLdXsrjlkUvij/wwAP6xCc+oXPOOWdqW21trc4991xVV1fr5ptvNp5cSoUP8M72cWvXrtV5550nSXrLW96ir371q/rBD34w5+QykUgoGAxKkqLRqJqbm5VMJpVOp6c+ZtmyZVq2bJkGBgaUyWSmtjc3NysajWr37t0aHx+fGuvY2JiWLl2qZ555ZlrQK1eu1JIlS9Tb2zttDB0dHTp06JB27do1tS0QCKijo0PZbFb9/f1T20OhkFauXKnh4WHt3bt3ans4HFZra6sOHDig/fv3T22f7z5Nam1tVTgc9uU+9fX1SZKefvppOY7ji33yY0/H7tOKFSv0whe+cKo3P+yTH3s6dp+O/pk+MTHhi32a5Keejt2no39OBoNBX+yTH3s6dp9e+MIX6qSTTpr2c7Kc+5TL5bRQXvWqV+nDH/6wHnnkEa1fv16RSEQjIyN67LHH9J//+Z86++yzF2wsAAAAsIftxy2bmppUV1c3bY7P71Az92myl0OHDqmqqsoX+zSp1D3t2bNH0uHf95cuXeqLfSpHT01NTero6NCzzz6riYkJX+xTuXpyXVeBQECO4/hmn6TS97Rv3z5Jz69J+GGfWAeobE+T//bt2bNHbW1tvtinSQt53NJxTWd3R6xbt07f//73tWLFihmPPfvss/rDP/xD7dixw+i5vv71r+vOO+/UT37yk6ltjz76qC6++GI9/PDD0y5n9KEPfUipVEp33nnn1Larr75a1dXVuvXWW2c8dyaT0c6dO7VmzRqFw+Gp7fN5p4TrupqYmFAoFJr1eSY/zmT7Yn1Hy0LvUy6X08TEhKqqqqa22b5Pfuyp0Dt5JiYmtGTJkmn/GNq8T37s6dixH/1z0nEcX+zT8bb7YZ/y+fy0n5N+2Cc/9lRo+/j4+Iyfk+Xap0wmo+7ubp166qnT5lblMDIyoptuukn33XfftAMSwWBQr371q/XXf/3XamhoMH6+n/70p7r++uv18pe/XH//938/tf2ee+7Rhz/8YVVVVU37+H//93/X6aefPm3b5NxyIfbfL3K5nB555BGdccYZUwd74Q10411041104122dtPT06NL3v0+veCNVyrafHKlhyNJSu/drWe+91l99Yt3qrOzc97PN1c35Zpb2X7c0nEcHTx4cOp3s8lt/A41+zHaQscebNyn4439RPepmN/3bdmncvQkHX6TRTAYnHEc0tZ9KldPk99/1dXVJdvXSu/TfMY+2/ZCaxK27xPrAJXtafJ7r6qqSsFg0Bf7ZLq9lMctiz5TvLm5Wb/5zW8KTi5/+9vfqrm52fi51q1bp+eee0779+/XsmXLJEldXV065ZRTZtzfp7OzU1/5ylfkuu5UyIlEYto7PwsJBAIKBAIzts32sXNtz+fz2r17tzo6Oub8+KO/cY+3fbK8+W4/0X0y3W7zPjmOM9Xb0R9j8z75sadjt+fzeT377LMzejt6PPMdOz2Vfp+O/jlZqrFXep9Mtvthn479OemHfTIdo637NNfPyXLs02wfUw6RSER/+7d/q49+9KPq6+vTyMiIwuGw2tvbje8BOWnr1q36xje+ofb29oKPv/SlL9WXvvSlUgwbAAAAFWb7ccujf6cu1fHMY/nhdyiTYw+T45nv2L3+e+FcY5zcbvr7vk37ZLq9mOOQu3btmvU4pI37dKLbj7dPx/6c8sM+mYzxRLYX+nlu8z6xDlDZno5dn/TDPpluL+Vxy6KPbr7xjW/URz/6UX3xi1/Uo48+qp6eHj366KP6/Oc/r5tvvll/9Ed/ZPxcp512mtavX6/bb79dIyMj6unp0V133aW3v/3tkqQ3vOENeuihhyRJ559/vg4cOKA777xTY2Nj+u53v6vHHntM559/frG7AAAAAAssWbJES5Ys0cTEhDo6OopeEJek6urqORfFAQAA4B8ct4SfuK6rweGD6kqk9OBTg+pKpDQ4fHDGWXQAAMBM0WeKv//979eePXt02223TdvuOI7e8pa36AMf+EBRz3fHHXfohhtu0Ctf+UpFIhFdcskluvTSSyUdvt/C5PXgly9frn/+53/WLbfcos9+9rNqbW3VZz7zGa1cubLYXUAZJFJZbe9JKpEaU1usRhs642qL1VZ6WAAAwFKf/exn9cUvflEjIyNyHEc//OEPVVNToz/5kz/R5z//ecViMaPneec73znn488995wuv/xy7dixQ9FoVB/4wAf0pje9qQR7AAAAgIXGcUv4heu6emrviLZ1D6pvX0Y511Uw4Ki9MazNq5u0qjky69l6AACgsKIXxUOhkD75yU/qmmuu0WOPPaaRkRFFo1GtW7dO8Xi86AG0tLRo69atBR/r7u6e9veXvexl+va3v1301yilhbx0qC0Sqay2PtCr/lRW4VBQOxJD6kqktWVTh2cWxunNTvRmJ3qzE73Zya+9bd26Vf/8z/+siy++WBs2bNDVV18tSVP3/v77v/973XzzzfP+OsuWLdMLXvACXX311TrllFP0ox/9SNddd52am5u1cePGgp+Ty+WUy+Xm/bUXg8mcyMt76Ma76Ma76Ma7bO0ml8tJR90j0gtc15Vct2Tznbm6KVdffjhu6dc5fqn5PafkyLi2dQ+qNzk6tS2Xd9U7ePjvDeGQmuqqjZ7L71mVCjmZIysz5GSOrMyQ0/wVvSg+afny5Vq+fHkpx+J5gUBg6nr9eN72nqT6U1mtaalTwHGUd111Dwxre09SF5w58x5OC43e7ERvdqI3O9Gbnfzc29e//nV95CMf0YUXXijp+fsSxWIxXXfddbrqqqtKsij+qle9Sq961aum/v7GN75RP/rRj3TPPffMuij+5JNPzvvrLjZdXV2VHgJmQTfeRTfeRTfeZVs3u3fvViab1fDIiNyadKWHI0kaGRlRJpvVzp07NTw8XLLnrUQ3th639PMcv5QWQ04D6az69mUKPtaXzGggnTVaFF8MWZUCOZkjKzPkZI6szJBTaRgtil9yySX63Oc+p2g0qksuueS4H//Vr3513gPzItd1lc1mVVtby+VpjpJIjSkcCipwJJOA4ygcCiqRGqvwyA6jNzvRm53ozU70Zic/97Znzx5t2LCh4GNtbW0aGhoq29dua2vTjh07Zn189erVCofDZfv6fpLL5dTV1aX169crGAxWejg4Ct14F914F914l63d1NXVKVxbq7pIRNFotNLDkSQ5Y2mFa2t16qmnqrOzc97PN1c3mUymZG829NNxSz/P8UtpMeQ0lJlQbparSORcV0OZCaPnWQxZlQI5mSMrM+RkjqzMkFNpGC2KT16q8tg/Lzau66q/v18dHR286I7SFqvRjsSQ8q47daZ4ZjyntlhNpYcmid5sRW92ojc70Zud/NzbSSedpMcff1wrVsy84szOnTtP6NKXhXzlK19RfX29zj333KltPT09Bb/upGAwaNUBdy8gM++iG++iG++iG++yrZtgMCg5jpwj/3mB4ziS45Q8y0LPV8rn99NxSz/P8UtpMeRUH65S0HEKLowHA47qw2av9cWQVSmQkzmyMkNO5sjKDDmVhtGi+Je+9KWCfy7EK/dBwsLZ0BlXVyKt7oFhhUNBZcZzao3VamNnaQ5YAwCAxeXss8/WTTfdpGw2q1e84hVyHEdDQ0N66qmndMstt+j1r399Sb7O+Pi4PvrRj2rFihV60YtepPvuu08PPPCAvv71r5fk+QEAAFB+HLeEH7VEa9XeGJ52T/FJ7Y1htURrKzAqAADsVvQ9xV/zmtfoG9/4hhoaGmY8tnPnTm3ZskUPPvhgSQYHO7TFarVlU4e29ySVSI2pLVajjZ1xtcaYnAEAgOJdddVV6unp0Yc+9CE5jiPXdXXhhRfKdV2dffbZuuqqq4yfa/369ZKkQ4cOSZJ+/OMfSzp8b8t3vvOdGh0d1Z//+Z9rcHBQJ598sj7zmc9o3bp1pd8pAAAAlB3HLeEX8UhIm9c0Sc7he4jnXFfBgKP2xrA2r25SPBKq9BABALCO8aL4r371K0lSIpHQww8/rPr6+mmPu66rBx98UOl0urQj9JhQiAlHIW2xWl1w5uyXGq00erMTvdmJ3uxEb3bya2+RSET/8i//ot/+9rd69NFHNTIyomg0qjPOOENr164t6rm6urpmfcxxHF155ZW68sor5ztkAAAAVJCfjlv6dY5fan7PyXEcrWqOqCEc0kA6q6HMhOrDVWqJ1ioeCRV16Vy/Z1Uq5GSOrMyQkzmyMkNO82e8KH799derv79fjuPo/e9//4zHJy8/9NrXvrZ0o/OYQCCglStXVnoYKBK92Yne7ERvdqI3O/m5t3//93/Xm970Jp1++uk6/fTTKz0cAAAAeJxfjlv6eY5fSoslJ8dx1FRXraa66hN+jsWS1XyRkzmyMkNO5sjKDDmVhvGi+E9+8hPt2bNHmzdv1u23365oNDrjY+rr66cuUelHrutqeHhYdXV13MjeIvRmJ3qzE73Zid7s5Ofebr/9dp199tmKRCKVHgoAAAAs4Jfjln6e45cSOZkjKzPkZI6szJCTObIyQ06lUdQ9xZcvX65/+7d/00te8hJVVVXNeHx0dFTf/OY39da3vrVkA/QS13W1d+9eRSIRXnQWoTc70Zud6M1O9GYnP/f2rne9S3fccYduvvlmFsYBAABgxA/HLf08xy8lcjJHVmbIyRxZmSEnc2RlhpxKo6hFcUl62cteJkk6cOCAUqnU1HbXdfWrX/1Kt9xyi6cnlwAAAPC2J598Uk8++aQ2btyoFStWFDzT56tf/WoFRgYAAAAv47glAAAAZlP0ongikdAHPvABPf744wUff8lLXjLvQQEAAGDxSqfTamlpUUtLS6WHAgAAAItw3BIAAACzKXpR/G/+5m/kOI7+6q/+Sh//+Mf1gQ98QLlcTt/5znd01lln6SMf+Ug5xukZ4XC40kPACaA3O9GbnejNTvRmJz/1duedd+qyyy5TTU2NvvSlL01tf/DBB3XWWWeppqamgqMDAACADfxw3NJPc/xyIidzZGWGnMyRlRlyMkdWZshp/gLFfsKvf/1r3XTTTbrkkksUDAb1+te/XldccYXuvfdePffcc7r33nvLMU5PCAQCam1tVSBQdGyoIHqzE73Zid7sRG928ltv//AP/6DR0dEZ2z/wgQ9ocHCwAiMCAACAbWw/bum3OX65kJM5sjJDTubIygw5mSMrM+RUGkWnl0ql1NTUJEkKhULKZrOHnygQ0Ac/+EH98z//c2lH6CGu62r//v1yXbfSQ0ER6M1O9GYnerMTvdnJb73Nth9+2T8AAACUn+3HLf02xy8XcjJHVmbIyRxZmSEnc2RlhpxKo+hF8ZaWFv32t7+VJDU3N+uXv/zl1GPBYFB79uwp3eg8hhednejNTvRmJ3qzE73Zid4AAACA6Ww/bskc3ww5mSMrM+RkjqzMkJM5sjJDTqVR9D3FzzvvPF199dX6zne+o9e85jW67bbbNDg4qIaGBn3zm9/UKaecUo5xAgAAAIvG4OCg0ul0pYcxJRqNTp11BQAA4FUctwQAAMBsil4U/7M/+zMtWbJEsVhM733ve9Xd3a3Pfe5zcl1X7e3tuuWWW8oxTgAAAGBRGBwc1P+5/D3aP5yp9FCmLKsL68t3fZ6FcQAA4GkctwQAAMBsil4UDwaD+tM//dOpv//TP/2TRkZGdOjQIcVisVKOzZOi0Wilh4ATQG92ojc70Zud6M1OfurNcRw5jlPpYXhCOp3W/uGMmjZeoKXLlld6OBrdv0eDP79b6XSaRXEAAOBpfjhu6ac5fjmRkzmyMkNO5sjKDDmZIysz5DR/RS+KFxKJRErxNJ4XCATU3Nxc6WGgSPRmJ3qzE73Zid7s5LfeXNfVeeedN2NhfGxsTBdffLECgcDUNsdx9NOf/nShh7jgli5brmjzyZUehiRpsNIDAAAAOEE2Hbf02xy/XMjJHFmZISdzZGWGnMyRlRlyKg2jRfEXvehFRZ25s3PnzhMekJfl83klk0nF4/FpB2bhbfRmJ3qzE73Zid7s5Lfe3vKWt1R6CAAAALCQn45b+m2OXy7kZI6szJCTObIyQ07myMoMOZWG0aL4e97znqnJpeu6+ta3vqVly5bpJS95iZYuXarh4WE99NBDymazuvTSS8s64EpLp9OKx+OVHgaKRG92ojc70Zud6M1Ofurt1ltvrfQQAAAAYCG/Hbf00xy/nMjJHFmZISdzZGWGnMyRlRlymj+jRfFrr7126s933HGHXv/61+uGG26Y8XF/9Vd/pbGxsdKNDgAAAAAAAABmwXFLAAAAmCj6HPv//M//nPVdle94xzv09a9/fd6DAgAAAAAAAIBicNwSAAAAsyl6UXxoaEjpdLrgY6Ojo7M+5geO42jZsmVF3acIlUdvdqI3O9GbnejNTvQGAAAATGf7cUvm+GbIyRxZmSEnc2RlhpzMkZUZciqNohfFTz/9dP3lX/6lHnzwQe3fv18HDx5UKpXStm3bdMMNN+i0004rxzg9gRednejNTvRmJ3qzE73Zid4AAACA6Ww/bskc3ww5mSMrM+RkjqzMkJM5sjJDTqVhdE/xo91888264oortGXLlmnbXdfV8uXL9clPfrJkg/OafD6vgYEBtbS0KBAo+v0EqBB6sxO92Yne7ERvdqI3AAAAYDrbj1syxzdDTubIygw5mSMrM+RkjqzMkFNpFL0o3tnZqR/84Af65S9/qaeeekqjo6MKh8Pq6OjQhg0bFAqFyjFOz8hkMpUeAk4AvdmJ3uxEb3aiNzvRGwAAAPA8Pxy3ZI5vhpzMkZUZcjJHVmbIyRxZmSGn+St6UVySlixZole84hV6xSteUerxAAAAAAAAAMAJ4bglAAAACjFaFL/mmmt08803KxKJ6Jprrjnux99+++3zHhgAAAAAAAAAzIXjlgAAADBhtCj+m9/8RhMTE1N/noufb/LuOI6am5t9vY9+RG92ojc70Zud6M1O9AYAAAD467glc3wz5GSOrMyQkzmyMkNO5sjKDDmVhtGi+E9+8pOCf15sHMdRNBqt9DBQJHqzE73Zid7sRG92ojcAAADAX8ctmeObISdzZGWGnMyRlRlyMkdWZsipNAKVHoBN8vm8du3apXw+X+mhoAj0Zid6sxO92Yne7ERvAAAAgL8wxzdDTubIygw5mSMrM+RkjqzMkFNpGJ0pfvbZZxs/oeM4+ulPf3rCA/K68fHxSg8BJ4De7ERvdqI3O9GbnegNAAAAi53fjlsyxzdDTubIygw5mSMrM+RkjqzMkNP8GS+Kc516AAAAAAAAAF7CcUsAAACYMFoU/8QnPmH0ZOPj43r22WfnNSAAAAAAAAAAMMFxSwAAAJgo6T3Fe3p6dNFFF5XyKT3FcRy1trby7lPL0Jud6M1O9GYnerMTvQEAAADmbDhuyRzfDDmZIysz5GSOrMyQkzmyMkNOpWF0pvjRDh48qE996lN68MEHdeDAgWmPpVIpNTU1lWxwXuM4jsLhcKWHgSLRm53ozU70Zid6sxO9AQAAANPZftySOb4ZcjJHVmbIyRxZmSEnc2RlhpxKo+gzxT/1qU/p7rvv1qpVq5RKpfR7v/d7WrNmjYaGhvTGN75RX/ziF8sxTk/I5/Pq7e1VPp+v9FBQBHqzE73Zid7sRG92ojcAAABgOtuPWzLHN0NO5sjKDDmZIysz5GTOS1m5rqvB4YPqSqT04FOD6kqkNDh8UK7rVnponsrJZkWfKX7ffffp9ttv1znnnKOXvOQl+n//7/9pxYoV2r17t/7sz/5MQ0ND5RinZ/jxBZdIZbW9J6lEakxtsRpt6IyrLVZb6WGVlB97WwzozU70Zid6sxO9AQAAAM/zw3FL5vhmyMkcWZkhJ3NkZYaczHkhK9d19dTeEW3rHlTfvoxyrqtgwFF7Y1ibVzdpVXOk4pcu90JOtiv6TPG9e/dq9erVkqRgMKjx8XFJ0sknn6zrr79et956a2lHiLJKpLLa+kCv7ntsj3oHR3TfY3u09YFeJVLZSg8NAAAAAAAAMMZxSwAAcCKSI+Pa1j2o3uSockfODM/lXfUOjmrbk4NKjoxXeIQohaIXxaPRqPbs2SNJWrZsmXp6eqYeO/nkk/Xkk0+WbnQou+09SfWnslrTUqf2xqVa01Kn/iNnjgMAAAAAAAC24LglAAA4EQPprPr2ZQo+1pfMaCBd+RNJI5FIpYdgvaIvn75p0yZde+21+tKXvqSXvvSl+pu/+RtFIhE1NDToi1/8ohobG8sxTk9wHEcrV66s+CUSSimRGlM4FFTgyD4FHEfhUFCJ1FiFR1Y6fuxtMaA3O9GbnejNTvQGAAAATGf7cUvm+GbIyRxZmSEnc2RlhpzMeSWroczE1Bnix8q5roYyEws8osNc11VyZFwDQ1kdGA1o8FBaLfW1ikdCFc/MRkUvil9zzTW69tpr5bqurrjiCj3wwAP6v//3/0o6fFmiW265peSD9JIlS4qOzNPaYjXakRhS3nUVcBzlXVeZ8ZzaYjUFP97W+4/7rbfFgt7sRG92ojc70RsAAADwPD8ct2SOb4aczJGVGXIyR1ZmyMmcF7KqD1cp6DgFF8aDAUf14aoFH9P0+5yPKpd3FQw6am9c6pn7nNvG6JX2pje9SRdddJHOO+88NTU16V//9V+nHrvvvvv0i1/8QhMTE1q3bp1aW1vLNthKc11Xvb296ujo8M0LbUNnXF2JtLoHhhUOBZUZz6k1VquNnfEZHzt5//H+VFbhUFA7EkPqSqS1ZVOHpxfG/djbYkBvdqI3O9GbnegNAAAA8NdxS+b4ZsjJHFmZISdzZGWGnMx5JauWaK3aG8PqTY7OeKy9MayW6MKvgR19n3O5rtLDaUXrouodPDzGhnBITXXVCz4umxktigeDQX30ox/VJz/5Sf3BH/yBLrzwQr385S+XJIXDYf3+7/9+WQeJ8mmL1WrLpo5pZ39v7IyrtcAi99H3H588q7x7YFjbe5K64MwVFRg9AAAAAAAAFjOOWwIAgPmKR0LavKZJcg7fQzznugoGHLU3hrV5dZPikdCCj8nkPucsihfHaFH8nnvu0ZNPPqlvfvOb+s53vqPvfe97Ovnkk/W2t71Nb3nLW9Tc3FzucaKM2mK1Rovai+H+4wAAAAAAALAHxy0BAMB8OY6jVc0RNYRDGkhnNZSZUH24Si3Ryt2/26v3ObdZwPQDV69ereuvv14PPPCA7rzzTp122mn6zGc+o1e/+tV63/vep/vvv1/5fL6cY0WFtcVqlBnPKX/km/B49x8HAAAAAAAAyo3jlgAAYL4cx1FTXbXWt8V09qomrW+LqamuumKXdZ+8z3khlbrPue2Kvnt9IBDQ5s2btXnzZqXTaX33u9/Vvffeqz/90z9VPB7XW9/6Vl199dXlGGvFOY5T8fsaVFIx9x/3ksXem63ozU70Zid6sxO9AQAAANPZftySOb4ZcjJHVmbIyRxZmSEnc2Q1u2n3OXekaF1UOhJTpe5zbjvjM8ULiUajuvTSS/XVr35Vn/70p+U4jrZu3VqqsXnSoUOHKj2Eipm8//jr1y5XR1NEr1+7XO/d1FHw/uNes5h7sxm92Yne7ERvdqI3AAAAoDBbj1syxzdDTubIygw5mSMrM+RkjqwKm7zPeUfTUgUdR3k3r2DAUUfT0ord59x2RZ8pfrRnn31W99xzj+6991719/frBS94gd71rneVamye47qudu3atajftWJ6/3EvoTc70Zud6M1O9GYnegMAAABmZ+NxS+b4ZsjJHFmZISdzZGWGnMyR1eym3ed8KKs9B4a1vKFOLfWVu8+57YpeFB8dHdX3v/99ffOb39Svf/1r1dTU6A1veIPe9ra36cwzzyzHGAEAAAAAAABgThy3BAAAfjJ5n/PGpVVqWjKm5uaoAoF5XQR8UTNeFP/5z3+ub37zm/rRj36kbDar9evX6+abb9a5556rSCRSzjECAAAAAAAAQEEctwQAwH9c11VyZFwD6ayGMhOqD1epJbp4z5IeGRlRc3NzpYdhNaNF8Ve/+tV67rnnVF9frwsvvFBve9vbtHr16nKPzZN4B4ad6M1OtvSWSGW1vSepRGpMbbEabeiMqy1WW+lhVYwtvWE6erMTvQEAAGCx89txS+b4ZsjJHFmZISdzZGWGnMwVysp1XT21d0TbugfVty+jnOsqGHDU3hjW5tVNWtUcWXQL47ym5s9oUfyFL3yhrr32Wr32ta9VKLR4b9weCATU0dFR6WGgSPRmJ1t6S6Sy2vpAr/pTWYVDQe1IDKkrkdaWTR2LcmHclt4wHb3Zid4AAAAAfx23ZI5vhpzMkZUZcjJHVmbIydxsWSVHxrWte1C9ydGpbbm8q97Bw39vCIfUVFe9YOOsNF5TpWG0KP6FL3yh3OOwguu6ymazqq2tXXTvQLEZvdnJlt629yTVn8pqTUudAo6jvOuqe2BY23uSuuDMFZUe3oKzpTdMR292ojcAAADAX8ctmeObISdzZGWGnMyRlRlyMjdbVgPprPr2ZQp+Tl8yo4F0dlEtivOaKg3OtS+C67rq7++X67qVHgqKQG92sqW3RGpM4VBQgSP/EAUcR+FQUInUWIVHVhm29Ibp6M1O9AYAAAD4C3N8M+RkjqzMkJM5sjJDTuZmy2ooM6HcLPnlXFdDmYmFGJ5n8JoqDaMzxQEAhbXFarQjMaS8606dKZ4Zz6ktVlPpoQEAAAAAAAAAYJ36cJWCjlNwYTwYcFQfrirb13ZdV8mRcQ2ksxrKTKg+XKWWaK3ikRBnaVuORXEAmIcNnXF1JdLqHhhWOBRUZjyn1litNnbGKz00YFFJpLLa3pNUIjWmtliNNnTG1RarrfSwAAAAAAAAABSpJVqr9sbwtHuKT2pvDKslWp7jfq7r6qm9I9rWPai+fRnlXFfBgKP2xrA2r27SquYIC+MWY1G8SKFQqNJDwAmgNzvZ0FtbrFZbNnVMW4zb2BlX6yJejLOhN8xkc2+JVFZbH+hVfyqrcCioHYkhdSXS2rKpw/cL4zb3BgAAAGAm5vhmyMkcWZkhJ3NkZYaczBXKKh4JafOaJsk5fA/xYxen45Hy5JscGde27sFpi/G5vKvewcN/bwiHKnYvc15T88eieBECgYBWrlxZ6WGgSPRmJ5t6a4vV6oIzV1R6GJ5gU294nu29be9Jqj+V1ZqWuqnbGHQPDGt7T9LX35u29wYAAABgOub4ZsjJHFmZISdzZGWGnMzNlpXjOFrVHFFDOLSglzEfSGfVty9T8LG+ZEYD6WxFFsV5TZVGoNIDsInrukqn09zI3jL0Zid6sxO92cn23hKpMYVDQQWOTIYDjqNwKKhEaqzCIysv23sDAAAAMB1zfDPkZI6szJCTObIyQ07m5srKcRw11VVrfVtMZ69q0vq2mJrqqst6+fKhzETB+5hLUs51NZSZKNvXnguvqdJgUbwIrutq7969vOgsQ292ojc70ZudbO+tLVajzHhO+SPjz7uuMuM5tcVqKjyy8rK9NwAAAADTMcc3Q07myMoMOZkjKzPkZM5rWdWHqxScZdE9GHBUH65a4BEd5rWcbMWiOAAAsNqGzrhaY7XqHhhW375RdQ8MqzVWq42d8UoPDQAAAAAAAIAlWqK1am8MF3ysvTGslmjtAo8IpcQ9xQEAgNXaYrXasqlD23uSSqTG1Bar0cYjC+UAAAAAAAAAYCIeCWnzmibJOXwP8ZzrKhhw1N4Y1ubVTYpHQrN+ruu6So6ML+g90FEcFsWLFA4XfocIvI3e7ERv3pVIZactQG7ojKvtyAIkvdnJ9t7aYrW64MwVlR7GgrO9NwAAAADTMcc3Q07myMoMOZkjKzPkZM5LWTmOo1XNETWEQ0Utbruuq6f2jmhb96D69s1cTF/VHJn3wriXcrIVi+JFCAQCam1trfQwUCR6sxO9eVcildXWB3rVn8oqHApqR2JIXYm0tmzqUFuslt4sxPebnegNAAAA8Bfm+GbIyRxZmSEnc2RlhpzMeTErx3HUVFetprpq489JjoxrW/egepOjU9tyeVe9g4f/3hAOFfV8x/JiTjbinuJFcF1X+/fv50b2lqE3O9Gbd23vSao/ldWaljq1Ny7VmpY69R85c5ze7ERvdqI3AAAAwF+Y45shJ3NkZYaczJGVGXIy55esBtJZ9e3LFHysL5nRQDo7r+f3S06VxqJ4EXjR2Yne7ERv3pVIjSkcCipw5HIvAcdROBRUIjVGb5aiNzvRGwAAAOAvzPHNkJM5sjJDTubIygw5mfNLVkOZCeVm2Yec62ooMzGv5/dLTpXGojgAoChtsRplxnPKH/kHOO+6yozn1BarqfDIAAAAAAAAAABYWPXhKgVnuWd4MOCoPly1wCNCISyKAwCKsqEzrtZYrboHhtW3b1TdA8NqjdVqY2e80kMDAAAAAAAAAGBBtURr1d4YLvhYe2NYLdHaBR4RCllS6QHYJhqNVnoIOAH0Zid686a2WK22bOrQ9p6kEqkxtcVqtPHIQnk+n6c3S9GbnegNAAAA8Bfm+GbIyRxZmSEnc2RlhpzM+SGreCSkzWuaJOfwPcRzrqtgwFF7Y1ibVzcpHgnN+2v4IadKY1G8CIFAQM3NzZUeBopEb3aiN29ri9XqgjNXzNhOb3aiNzvRGwAAAOAvzPHNkJM5sjJDTubIygw5mfNLVo7jaFVzRA3hkAbSWQ1lJlQfrlJLtFbxSEjOLJdWN+WXnCqNy6cXIZ/Pa+/evcrn85UeCopAb3aiNzvRm53ozU70BgAAAPgLc3wz5GSOrMyQkzmyMkNO5vyUleM4aqqr1vq2mM5e1aT1bTHFIyElR8bVlUjpwacG1ZVIaXD4oFzXLeq5/ZRTJbEoXqR0Ol3pIeAE0Jud6M1O9GYnevOuRCqrux9+Vnfc/5TufvhZJVLZqcfoDQAAAPAX5vhmyMkcWZkhJ3NkZYaczPk1K9d19dTeEX3rNwl97Ze79b2uAX3tV7v1rUcSemrvSNEL437NaSGxKA4AAOBRiVRWWx/o1X2P7VHv4Ijue2yPtj7QO21hHMf305/+VK94xSt01VVXzXjsv/7rv3TeeefpJS95id761rfqwQcfrMAIAQAAAAAA4CfJkXFt6x5Ub3JUuSML4Lm8q97BUW17clDJkfEKj3DxYVEcAADAo7b3JNWfympNS53aG5dqTUud+lNZbe9JVnpo1ti6das+9rGPqb29fcZjO3fu1PXXX69rr71W27dv12WXXaY/+7M/08DAQAVGCgAAAAAAAL8YSGfVty9T8LG+ZEYDaU56WWgsihfBcRwtW7ZMjuNUeigoAr3Zid7sRG92ojfvSqTGFA4FFTjSTcBxFA4FlUiN0Zuh6upqfeMb3yi4KP6f//mf2rx5szZv3qzq6mqdf/75Wr16te69994KjBQAAACLHXN8M+RkjqzMkJM5sjJDTub8nNVQZmLqDPFj5VxXQ5kJ4+fyc04LaUmlB5BIJHTzzTfr0UcfVTgc1rnnnqtrrrlGgcD09fp//Md/1Gc/+1ktWTJ9yP/93/+teDy+IGOdfNHBLvRmJ3qzE73Zid68qy1Wox2JIeVdVwHHUd51lRnPqS1WQ2+G3vnOd8762GOPPabNmzdP23baaaepq6tr1s/J5XLK5XIlG99sX0OuK/fIf5Xmuq7kukXv++THljsvrxgcHPTU/b2i0aiampoKPrbYurEJ3XgX3XiXrd147d976cT/zZ/NXN3Y0tdCH7dkjm+GnMyRlRlyMkdWZsjJnJ+zqg9XKeg4BRfGgwFH9eEq4+fyc04LqeKL4u9///u1du1a/fjHP9a+fft0xRVXKB6P6/LLL5/xsW9605v0iU98ogKjPCyfz2tgYEAtLS0zJr/wLnqzE73Zid7sRG/etaEzrq5EWt0DwwqHgsqM59Qaq9XGzji9lUAqlVJ9ff20bfX19frd73436+c8+eST5R6Wdu/erUw2q+GREbk1lV9kHRkZUSab1c6dOzU8PFz058/1JgO/OHDggG74648rnT1Y6aFMidZW66M3flgNDQ2zfsxi6MZWdONddONdtnXjtX/vpfn/mz8b27o52kIft2SOb4aczJGVGXIyR1ZmyMmcn7NqidaqvTGs3uTojMfaG8NqidYaP1exObmuq+TIuAbSWQ1lJlQfrlJLtFbxSGhRn21e0UXxrq4uPfHEE7rrrrtUV1enuro6XXbZZfrXf/3XgpNLL8hkCl//H95Gb3aiNzvRm10Sqax+/rtB7Xi6X+teeEgbT2lSW8x8QobyaovVasumDm3vSSqRGlNbrEYbO+NqjdUqn8/z/VYCxZ6ZtXr1aoXD4TKN5rC6ujqFa2tVF4koGo2W9WuZcMbSCtfW6tRTT1VnZ6fx5+VyOXV1dWn9+vUKBoNlHGHl9fT06JACWvnq/6Oly5ZXejga3b9HyZ/frZUrVxbsbDF1Yxu68S668S5bu/Hav/fSif+bP5u5uslkMgvyZsP5qNRxS+b4ZsjJHFmZISdzZGWGnMz5Nat4JKTNa5ok5/A9xHOuq2DAUXtjWJtXNykeCRX1fKY5ua6rp/aOaFv3oPr2zfy6q5oji3ZhvKKL4o899pja2tqmnaGzdu1aPf300xoZGVEkEpn28d3d3brkkkv05JNP6qSTTtJf/MVf6Oyzz17oYQMA4AuJVFZbH+hVIpVR/uC4Eo/v0Y7+YW3Z1MHCuIe0xWp1wZkrKj0MX2poaFAqlZq2LZVKzXk5qmAwWPYD7sFgUHIcOUf+qzTHcSTHOeF9X4jMKm2ys0hji6LNJ1d6OHIcR0mDzhZDN7aiG++iG++yrRuv/Xsvzf/f/NkUej4buuK4JQAAmA/HcbSqOaKGcGhBz9hOjoxrW/fgtDPUc3lXvYOH/94QDqmprrosX9vrKroonkqlZrwbdnKieeDAgWmTy5aWFq1YsULXXHONmpub9bWvfU3ve9/7dO+996qjo2PWr5HP55XP56f+HggEpv1d0tQvIMfbed3negAAe7FJREFUns/np93rqdDHSzPPOJpteyAQKHjvqBPZfqL7ZLLdD/vkuu7U437ZJ5Ox27xPk1/PdF9t2Cc/9nTs2I/+OVmqsVd6n4633eZ9+vnvkupPZbW6OaL00CFF6yN6cu+Itvck9dbfO9nKffJjT7Ntn9x27POUa5+O/Rg/WLdunXbs2DFtW1dXl974xjdWaEQAAADwikoct5RmzvH5HWruY7SFvqaN+3S8sZ/oPh17XNQP+1SOnuY63m/rPpWrp8nX1OQ2P+zTfMY+1z4d/b3nl32a79gX6zpA49IqNS6tmrb96DGZjP3o19Tx9mlgKKu+5KjkupIjSc7hP0vqGxzVwFB2alHeL6890+OWFb+neKEDvIVceOGFuvDCC6f+ftlll+l73/ue7r33Xn3wgx+c9fMSicTUu0+j0aiam5uVTCaVTj9/v6Zly5Zp2bJlGhgYmHb5gebmZkWjUe3evVvj4+NyXVe5XE5jY2NaunSpnnnmmWlBr1y5UkuWLFFvb++0MXR0dOjQoUPatWvX1LZAIKCOjg5ls1n19/dPbQ+FQlq5cqWGh4e1d+/eqe3hcFitra06cOCA9u/fP7V9vvs0qbW1VeFw2Jf71NfXp1wup6efflqO4/hin/zY07H7tGLFCjU1NU315od98mNPx+7T5M9JSZqYmPDFPk3yU0+TEqm8apY4Sg+llM+7Sg+l5I4fUiI1Zu0++bGn2fbphS98oerr66f9nCznPk1+b/vJRRddpLe97W36n//5H23cuFHf+c539Mwzz+j888+v9NAAAADgAQt93LKpqUnV1dXT5vj8DjVznyaPPRw6dEhVVVW+2Kdy9bRnz56p46JLly71xT6Vo6empiY1NzcrkUhoYmLCF/tUrp4mF+Ucx/HNPkml72nfvn3T1iT8sE8n2lMgEFBNTY0aGxtVXV2tXbt26dChQ1Mf7+V1gEgkojFVKXVQGhnPqyaQV3zpEtVoQiMjIwva0+S/fXv27FFbW9us+zQ8PKyB/cM6kB6SJFVXV6u6ukaZbEaOHOUUUOJARkFHykmqr3IVyh/UyMiIJHtfe6bHLR3XdHZXBl//+td155136ic/+cnUtkcffVQXX3yxHn74YS1dunTOz//gBz+ocDisj3/84zMey2Qy2rlzp9asWTPtvo9+fucR+8Q+sU/sE/tUeIz9Q2P6+e8Gp+7J/PKORp3cELZ6n06kp90HMvpF776pHA5kJ/Srpw9o9fKIAo6jvOuqe8+w3rC2hTPF2acZY8xkMuru7tapp55a9ntql9L69eslaeoXriVLDr8ntKurS5L0wx/+ULfffrsSiYROOeUU/eVf/qVe+tKXznieybnlQux/T0+PLnn3+/SCN17piUtxp/fu1jPf+6y++sU7i76n+COPPKIzzjjDikukzodtnS2mbmxDN95FN95lazde+7dDOvF/82czVzcLObc6URy35Hco9ol9Yp/YJ/Zp/vu0b3RcA0NjGspOXjq8Ro1LQ9M+3qv79NTeET3w5OR9uaWgo6n7cp9y5L7cXuzpsf60vvbLZ5Vzp58pPpSd0ODwQb3rFS9QcuSgfjc4qvZltVP7UygDr+zTsV/zRI9bVvRM8XXr1um5557T/v37p+7d2NXVpVNOOWXGxPKzn/2sXvKSl2jjxo1T23p6enTuuefO+TUCgYACgcCMbbN97Fzb8/m8du/erZNPPlmO48z68ZMvApPtk+XNd/uJ7pPpdpv3SdJUb0d/zHz2KZHKantPcmphaUNnfNr9d+lp/tuP/n4r9Dk27tPRX9d0u237dGxvXtinyftm96eyCoeCeqw/Pe2+2Yulp0Qqqy88+My0HKK1VYrWVql7z7CciXG5VSG1xcLa2Bm3Yp8mv67pdr/t01w/J8uxT3P9O+tlk4vfs3nd616n173udQs0GgAAANiiEsct55rj8ztU4WO0sx17mBzPfMfu9d8L5xqj4zhyXXfGa8r2fSpHT/l8Xs8+++ysxyFt3KcT3X68fSrHsb9K75PJGIvdLhVek7B5n05kjE/tHdG27slFZVfBgDO1qLzqyKKyV9cBBocP6oEnk+pNPn8WfM7V4b87STUsrZ66L/dC9HR0Tsfbp5b6WrXHl067p/jYobyeGxpTR1NEdTVL9JtnU4fvM15gfxZqn0y3l/K4ZUWPbp522mlav369br/9do2MjKinp0d33XWX3v72t0uS3vCGN+ihhx6SdPg+PjfffLN6e3t18OBBffGLX9SuXbv0lre8ZUHHfPQlLGCPUvY2ucB232N71Ds4ovse26OtD/QqkcqW7GvgML7f7OS13rb3HL5v9pqWOrU3LtWaljr1H3ljy2JSKId0dkIvaono9actV2s0qNeftlzv3dSh1qPe5ANv89r3GwAAAOAXlTpuyRzfDDmZIysz5GSOrMws9pySI+Pa1j2o3uTo4TOWpcOLsIOj2vbkoJIjz+fjxawGhrLq3jOs/aMHNTCU1f7RgxqbyEly1ZfMaCC98OtBpjnFIyFtXtOkjqalCh5ZZB4/lFNnc0SvW7tczw2NKTP+/OXGK7U/lVDxe4rfcccduuGGG/TKV75SkUhEl1xyiS699FJJ0tNPPz11PfhrrrlG0uF78qRSKZ1yyin6l3/5F7W0tFRs7Ficjl5Ymrrc8MCwtvckdcGZKyo9PMzheGf4w58SqTGFQ0EFjkwAAo6jcCioRGqswiNbWLPlkBnP692vPFlnxMbV0VH4HZkAAAAAsBhx3BIAgBMzkM6qb1+m4GOTi7BHn5nsJa7ramBoTD17RzR88JBcV3Icqa5miU6qr1F9bZWGMhOVHuasHMfRquaIGsIhDaSzGspMyHEcHcodPlv8t7uHpn18znU9vT+lVPFF8ZaWFm3durXgY93d3VN/rq6u1oc//GF9+MMfXqihAQWxwGanYy+hvSMxpK5EeuoS2vCvtliNdiSGlHfdqTeyZMZzaovVVHpoC4ocAAAAAKA4HLcEAGB2rusqOTI+tfB6+J7htYpHQhrKTEydIX4sry/CprOH5DiOMuM5Te6C6x7eLo0pHFqi+nBVRcd4PI7jqKnu+UuiP5Mc0X/84lkNHzw042ODAcfz+1MqFV8Ut4njOGptbZ31mvnwplL3xsLSwih1b6U6w5+zzefmxZ+TGzrj6kqk1T0wfOTM6JxaY7Xa2Bmv9NAW1Fw5eLE3HB+9AQAAAP7CHN8MOZkjKzPkZI6szCyGnFzXnfOe4Ssbwwo6TsGF8aMXYb2YVWIoo6pgQOva6vXIs6lpjw2PHVJrrEYt0YVdE5hvTkurq9RUV11wUby9Mbzg+1MpLIoXwXEchcPhSg8DRSp1byywLYxS91aKM/w52/z4vPhzsi1Wqy2bOqa9mWFjZ3zR3Tf7eDl4rTccnxe/3wAAAACcOOb4ZsjJHFmZISdzZGVmMeR09D3DJ03eM1ySzl13klYvj2jnwPCMzz16EdaLWe0fGddzQ1m9bu1ySdKOxJAO5V1VBR2tba3XOauaFI+EFnRM881p8j7jcg5fvv7YNzEs9P5UCoviRcjn83rmmWf0ghe8gHuuWqTUvbHAtjBK39v8z/DnfvLH59Wfk22xWjrS7Dl4tTfMjd4AAAAAf2GOb4aczJGVGXIyR1ZmFkNOx7tn+P7Rg3rNqc06mMvPuQi70FnNdcn3ybOw68NVum/HHq1ti+r8F5+kV61p0lB2QvW1VYrWVml5tHrBz2yfb06F7jNeaN/9jkXxIuXz+UoPASeg1L2xwFY6c12OvJS9leIMf+4nb4afk3aiNzvRGwAAAOAvzPHNkJM5sjJDTubIyozfczruPcOzE1rXVq83n9F23EXYhcrqeJd8X9UckeM4aonWamVjWI/uHtJTe0e0oiGs6qqA9o2M60BmXKedFF2Q8R5rvjkde5/xxYhFcQAVM9flyE+KlvYHcynO8Od+8gAAAAAAAACAxa4+XHXce4Z7bRH2eJd8bwiH1FRXPeNS4917hp9fPD9l8Vxq3I9YFAdQMXNdjvwtL2kr+deb7xn+3E8eAAAAAAAAALDYtURr1d4YnrbAPOnoe4Z7yfEu+T6QzqqprppLjfsYi+JFcBxHK1eu5AVvGXrzrrkuR+7F3rif/PF5sTccH73Zid4AAAAAf2GOb4aczJGVGXIyR1ZmFkNOx55NPds9w49nIbM67iXfMxPTxuWls9wXw2tqIbAoXqQlS4jMRvTmTce7HLkXe+N+8sfnxd5wfPRmJ3oDAAAA/IU5vhlyMkdWZsjJHFmZ8XtOpTybeqGyMrnk+yTXdZUcGffUmeJ+f00thEClB2AT13XV29srd5Z3ksCb6M27Nhw5y7p7YFh9+0bVPTA8dTnyQr0lUlnd/fCzuuP+p3T3w88qkcpWcPQohO83O9GbnegNAAAA8Bfm+GbIyRxZmSEnc2RlZrHkNHk29fq2mM5e1aT1bbGpy4+bWsisJi/5XsjRl3x3XVdP7R3Rt36T0Nd+uVvf6xrQ1361W996JKGn9o5UpNfF8poqN95WAKBi5roceT6fn/axiVRWWx/oVX8qq3AoqB2JIXUl0tqyqUNtXL4cAAAAAAAAAADMwvSS78mRcW3rHpx2v/Rc3lXv4OG/N4RDnrmsOorDojiAijK9HPn2nqT6U1mtaambutR698CwtvckuZw5AAAAAAAAAACYlekl3wfSWfXtyxR8jr5kRgPpLIvilmJRHIAVEqkxhUNBBY78wxRwHIVDQSVSYxUeGQAAAAAAAAAA8LrJS77Ptag9lJkoeN9xScq5roYyE+UaHsqMe4oXwXEcdXR0FHU/BFQevdnp2N7aYjXKjOeUP/KPUd51lRnPqS1WU8lh4hh8v9mJ3uxEbwAAAIC/MMc3Q07myMoMOZkjKzPkZM6LWdWHqxScZTzBgKP6cNUCj8ibOdmIRfEiHTp0qNJDwAmgNzsd3duGI/ca7x4YVt++UXUPDKs1VquNnfEKjhCF8P1mJ3qzE70BAAAA/sIc3ww5mSMrM+RkjqzMkJO5SmXluq4Ghw+qK5HSg08NqiuRUnLkoFqiNWpvDBf8nPbGsFqitQs80sN4Tc0fi+JFcF1Xu3btkjvLZRPgTaa9JVJZ3f3ws7rj/qd098PPKpHKLtAIUcixvbXFarVlU4dev3a5Opoiev3a5Xrvpg61xirzDxAK4+eknejNTvQGAAAA+AtzfDPkZI6szJCTObIyQ07mKpWV67p6au+IvvWbhL72y936XteAvvar3frmbxI6lHO1eU2TOpqWTp0xHgw46mhaqs2rmxSPhBZ0rJPj5TU1f9xTHNDhBfGtD/SqP5VVOBTUjsSQuhJpbdnUoTYWXT2jLVarC85cUelhAAAAAAAAAAAASyVHxrWte1C9ydGpbbm8q97BUX2v6zldeNbJevMZbRpIZzWUmVB9uEot0VrFIyEuYW4xFsUBSdt7kupPZbWmpU4Bx1HeddU9MKztPUkWYQEAAAAAAAAAAHxiIJ1V375MwceeSWa0a39G69tiaqqrXuCRoZxYFC9SIMAV5210vN4SqTGFQ0EFjrzDJ+A4CoeCSqTGFmJ4mAXfb3aiNzvRm53oDQAAAPAX5vhmyMkcWZkhJ3NkZYaczFUiq6HMhHJHLkUeDgW1oiGs6qqADk7k9eyBjIYyEws+puPhNTV/LIoXIRAIqKOjo9LDQJFMemuL1WhHYkh51506UzwznlNbrGaBRolj8f1mJ3qzE73Zid4AAAAAf2GOb4aczJGVGXIyR1ZmyMlcubJyXVfJkfFZL39eH67SEsfR2raoTqqvUXrskFKZCTVGQnph01KtbAyXfEzzwWuqNFgUL4Lruspms6qtreWeARYx6W1DZ1xdibS6B4YVDgWVGc+pNVarjZ3xBR4tJlX6+y2Rymp7T1KJ1JjaYjXa0Bnn/vIGKt1bKSzG7v3Q22JEbwAAAIC/MMc3Q07myMoMOZkjKzPkZK4cWbmuq6f2jmhb96D69mWUc10FA47aG8PavLpJq5ojaonW6vdf1KSJvKt7H31OOxJDOpR3VRV09LIXLtPFZ62Q67qe6Y/XVGmwKF4E13XV39+vjo4OXnQWMemtLVarLZs6pi2EbeyMq9XnC2FeVsnvt0Qqq60P9Ko/lVU4FNSOxJC6Emlt2dTh+8XR+bL95+Ri7d723hYresNCmhgfV19fX1Gfk8vltHv3btXV1SkYDJZ8TNFoVE1NTSV/XgAAgEphjm+GnMyRlRlyMkdWZsjJXDmySo6Ma1v3oHqTo1PbcnlXvYOH/94QDikeCemFTRF99Ze7VF0V0Dmr4spM5LT7QEbJkYP6WW9SzdEaz9xTnNdUabAoDhzRFqvVBWeuqPQw4AHbe5LqT2W1pqVu6nL63QPD2t6T5DXic3QPADMdHBnSM0/36oMfvknV1UX8Mui6ymSzCtfWSmX4hW1ZXVhfvuvzLIwDAAAAAIApA+ms+vZlCj7Wl8xoIJ1VU121go60oaNRqey49o+Oa9nSkF576nLtTWf1WGJ46uPgHyyKA8AxEqkxhUNBBY4cwA84jsKhoBKpsQqPDOVG996yGC9lD3jRxMGs8s4SxTe8VY2t7caf57quhkdGVBeJlPxdzKP792jw53crnU6zKA4AAAAAAKYMZSaUc92Cj+VcV0OZCbmuqz3DB/Xd306/dPra1nq9bu1yrW1zNJSZWOCRo9xYFC9SKBSq9BBwAujNTpXqrS1Wox2JIeVdd+ps4cx4Tm2xmoqMxzY2f78t5u691ttivZR9sbzWG/wt3NCkaPPJxh/vuq7cmrSi0WhZLu01WPJnBAAAqDzm+GbIyRxZmSEnc2RlhpzMlTqr+nCVgo5TcGE8GHC0LBJScmRcDzw5qEd3pzT5YRM5V488m5IknX9Gq5ZFvNXhsTm5rqvkyLgG0lkNZSZUH65SS7RW8UiIS6zPgkXxIgQCAa1cubLSw0CR6M1OlextQ2dcXYm0ugeGFQ4FlRnPqTVWq42d8YqMxya2f78t1u692NtCX8rexrPSvdgbAAAAgBPHHN8MOZkjKzPkZI6szJCTuXJk1RKtVXtjeNo9xSe1N4bVVh/WrgOjem5oTHXVS5QeOzTtY3YkhvSH61rUVh8u6bjm49icXNfVU3tHtK17UH37Msq5roIBR+2NYW1e3aRVzaW/ap8fsCheBNd1NTw8rLq6Ol5MFqE3O1Wyt7ZYrbZs6pi2QLaxM65Wjy+QeYHt32+LtXsv9raQl7K39ax0L/YGAAAA4MQxxzdDTubIygw5mSMrM+RkrhxZxSMhbV7TJDmH7yF+7IJxtHaJhhITqgo6OilWIw2NaXjskFxXchwpXB2UK1fRWu8soR6bU3JkXNu6B6ct/OfyrnoHD/+9IRzifugFeKdRC7iuq7179ypShvsionzozU6V7q0tVluWs1H9rtK9lcJi7N6LvS3kpewX+qz0UvFibwAAAABOHHN8M+RkjqzMkJM5sjJDTubKkZXjOFrVHFFDODTrpcUPX2I9oPraKlUvCSozfkjjh/IKLQmorqZKLdEaT3V3bE4D6az69mUKfmxfMqOBdLZii+Jevqw7i+IAgEXJxstlLyYLeSn7hTwrHQAAAAAAAEB5OY6jprrqWReGj77Eek1VUDVVwanHOpqWqqXe28eJhzITBe+ZLkk519VQZmKBR3SY1y/rzqI4AGDRsfVy2YvJQl7KfiHPSgcAAAAAAABQWce7xHo8Eqr0EOd0+Ex3p+DCeDBw+Ez4SvD6Zd1ZFC9SOByu9BBwAujNTvRmJxt6s/Vy2eXkxd4W6lL2C3lWeql5sTcAAAAAJ445vhlyMkdWZsjJHFmZISdzlcjK5BLrXnN0Tkef6X6s9sawWqKVOfHLy5d1l1gUL0ogEFBra2ulh4Ei0Zud6M1OtvTG5bKns6W3clnIs9JLabH3BgAAAPgNc3wz5GSOrMyQkzmyMkNO5iqZ1fEuse4lx+bk1TPdvXpZ90ksihfBdV0dOHBADQ0NnnyXCAqjNzvRm51s6Y3LZU9nS2/ltFBnpZcSvQEAAAD+whzfDDmZIysz5GSOrMyQkzmyMnNsTl49092rl3WfFKjoV7eM67rav3+/3Fne5QBvojc70ZudbOltw5GzgLsHhtW3b1TdA8PWXC67HGzpDdPRGwAAAOAvzPHNkJM5sjJDTubIygw5mfNKVq7ranD4oLoSKT341KC6EikNDh+s+LgmFcpp8kz39W0xnb2qSevbYmqqq67omwsmL+teSCUv6z6JM8UBACWRSGX1898NasfTSa1LhbTxlCa1efTy07ZeLhsAAAAAAAAAUDqu6+qpvSPa1j2ovn0zL0W+qjnCWeyGvHpZ90ksigMA5i2RymrrA71KpDLKHxxX4vE92tE/rC2bOjy9MG7b5bIBAAAAAAAAAKWTHBnXtu5B9SZHp7bl8q56Bw//vSEcsuK+417g1cu6T2JRvEjRaLTSQ8AJoDc70Zs9tvck1Z/Kas3yOo1ll6imtlZP7hnR9p4kC8+W4PvNTvQGAAAA+AtzfDPkZI6szJCTObIyQ07mKp3VQDqrvn2Zgo/1JTMaSGc9sShe6ZxMTV7W3QuZHYtF8SIEAgE1NzdXehgoEr3Zid7skkiNKRwKKhgIaOnSpZKkcCioRGqswiODCb7f7ERvAOYyMT6uvr6+go/lcjnt3r1bdXV1CgaDCzamaDSqpqamBft6AADYhjm+GXIyR1ZmyMkcWZkhJ3NeyGooM6HcLPcOz7muhjITCzyimbyQkx+wKF6EfD6vZDKpeDyuQCBQ6eHAEL3Zid7s0har0Y7EkHL5vMayWdXU1ioznlNbrKbSQ4MBvt/sRG8AZnNwZEjPPN2rD374JlVXF3hntusqk80qXFsrLeCly5bVhfXluz7PwjgAALNgjm+GnMyRlRlyMkdWZsjJnBeyqg9XKeg4BRfGgwFH9eGqCoxqOi/k5AcsihcpnU4rHo9XehgoEr3Zid7ssaEzrq5EWt17hpU/mFWg+pDaYmFt7KQ/W/D9Zid6A1DIxMGs8s4SxTe8VY2t7TMed11XwyMjqotEFux+XqP792jw53crnU6zKA4AwByY45shJ3NkZYaczJGVGXIyV+msWqK1am8MT7un+KT2xrBaorUVGNVMlc7JD1gUBwDMW1usVls2dejnvxvUjqf7te6Fy/WKU5rUGvPGhAEAgMUo3NCkaPPJM7a7riu3Jq1oNLpgi+KSNLhgXwkAAAAAADPxSEib1zRJzuF7iOdcV8GAo/bGsDavblI8Eqr0EFEiLIoDAEqiLVart/7eyTojNq6OjpO5jAsAAAAAAAAAwNMcx9Gq5ogawiENpLMaykyoPlyllmit4pHQgr6ZHOXFongRHMfRsmXL+AawDL3Zid7sRG92ojc70RsAAADgL8zxzZCTObIyQ07myMoMOZnzSlaO46iprlpNddUVHcdsvJKT7VgUL8Lkiw52oTc70Zud6M1O9GYnegMAAAD8hTm+GXIyR1ZmyMkcWZkhJ3NkZYacSoNr2xYhn8+rv79f+Xy+0kNBEejNTvRmJ3qzE73Zid4AAAAAf2GOb4aczJGVGXIyR1ZmyMkcWZkhp9JgUbxImUym0kPACaA3O9GbnejNTvRmJ3oDAAAA/IU5vhlyMkdWZsjJHFmZISdzZGWGnOaPRXEAAAAAAAAAAAAAgG+xKA4AAAAAAAAAAAAA8C0WxYvgOI6am5vlOE6lh4Ii0Jud6M1O9GYnerMTvQEAAAD+whzfDDmZIysz5GSOrMyQkzmyMkNOpbGk0gOwieM4ikajlR4GikRvdqI3O9GbnejNTvQGAAAA+AtzfDPkZI6szJCTObIyQ07myMoMOZUGZ4oXIZ/Pa9euXcrn85UeCopAb3aiNzvRm53ozU70BgAAAPgLc3wz5GSOrMyQkzmyMkNO5vL5vJLJJFkdB6+p0uBM8SKNj49Xegg4AfRmJ3qzE73Zid7sRG8AAACAvzDHN0NO5sjKDDmZIysz5DQ313WVHBnXwFBWA/uzahlLq6W+VvFIiEuEz4LX1PyxKA5YIJHKantPUonUmNpiNdrQGVdbrLbSwwIAAAAAAAAAADDmuq6e2juibd2D6kuO6kB6SA319WqPL9Xm1U1a1RxhYRxlwaI44HGJVFZbH+hVfyqrcCioHYkhdSXS2rKpg4VxAAAAAAAAAABgjeTIuLZ1D6o3OSq5riQpl3fVOzgqSWoIh9RUV13JIcKnuKd4ERzHUWtrK+9QsYztvW3vSao/ldWaljq1Ny7VmpY69R85c9zPbO9tsaI3O9GbnegNAAAA8Bfm+GbIyRxZmSEnc2RlhpzmNpDOqm9f5vBfHCkcXiodiaovmdFAOlu5wXkUr6nS4EzxIjiOo3A4XOlhoEi295ZIjSkcCipw5IddwHEUDgWVSI1VeGTlZXtvixW92Yne7ERvAAAAgL8wxzdDTubIygw5mSMrM+Q0t6HMhHJHzhCXHC1Z8vxSZc51NZSZqMzAPIzXVGlwpngR8vm8ent7lc/nKz0UFMH23tpiNcqM55Q/8o9E3nWVGc+pLVZT4ZGVl+29LVb0Zid6sxO9AQAAAP7CHN8MOZkjKzPkZI6szJDT3OrDVQpOnvHsuhpOp6cuox4MOKoPV1VwdN7Ea6o0WBQvEi84O9nc24bOuFpjteoeGFbfvlF1DwyrNVarjZ3xSg+t7GzubTGjNzvRm53oDQAAAPAX5vhmyMkcWZkhJ3NkZYacZtcSrVV74/NnPbtyp/7c3hhWS7S2EsPyPF5T88fl0wGPa4vVasumDm3vSSqRGlNbrEYbjyyUAwAAAAAAAAAA2CIeCWnzmibJkfoGRyUdPkO8Pb5Um1c3KR4JVXiE8CsWxQELtMVqdcGZKyo9DAAAAAAAAAAAAEmS67pKjoxrIJ3VUGZC9eEqtURrFY+E5ExeIv0YjuNoVXNEDeGQBoay2nMgpuUNdWqpn/vzgPliUbwIjuNo5cqVfENaht7s5IfeEqnstDP8N3TG1ebzM/z90NtiRG92ojcAAADAX5jjmyEnc2RlhpzMkZWZxZKT67p6au+ItnUPqm9fRjnXPXzGd2NYm1c3aVVzZM6F8aa6asUjIZ3aElEwGPR9XvOxWF5T5caieJGWLCEyG9GbnWzuLZHKausDvepPZRUOBbUjMaSuRFpbNnX4fmHc5t4WM3qzE70BAAAA/sIc3ww5mSMrM+RkjqzMLIackiPj2tY9qN7k6NS2XN5V75FLojeEQ2qqqz7u8wQCgbKN0U8Ww2uq3HilFcF1XfX29sp13UoPBUWgNzvZ3tv2nqT6U1mtaalTe+NSrWmpU/+RM8f9zPbeFit6sxO9AQAAAP7CHN8MOZkjKzPkZI6szCyWnAbSWfXtyxR8rC+Z0UA6e9znWCxZzRc5lQZvKwCAMkikxhQOBRU4cjmTgOMoHPr/7d15fBN1/j/wV9I2pWl6n7RAsQUqcpbDcl8eKyqiCAIu8EVBPHbxWC9kUZBF0QV+CqjgBa7o7pcFFPD4igIuIouIrkK5CrRQ2kAvStqmSUmazO+PmmzTpu2kTTKZ5PV8PHw8ZJJO3vN+z0w+mc98PhMEra5W4siIiIiIiIiIiIiIiNqn0mCGpZlOWosgoNJg9nJErmvLM9FJvtgpTuRDAvEZ1P4qNboDjmkrYRUEKBUKWAUBBpMFqdEdpA6NiIiIiIiIiIiIiKhdotQhCFIonHaMBykViFKHSBCVeO15JjrJEzvFiXxEID+D2h8NyYhHjrYKucXVUKuCYDBZkBIdhqEZ8VKHRkREDWRmZiIkJMThR84999yD559/XsKoiIiIiIiImuKIRiLyJcmRYUiLUzs8U9wmLU6N5Ejf7tdw1zPRST7YKe4ChUKB9PR0NjBkRi51a/gMatvI4tziavyQV467B3aWOjyvk0vdmpMaHYYHRqU7jPwfmhGPFD+/wUHudQtUnqgbZ77wPB5v7vPVV1+hU6dOUodBRERERAGObXxxAjVPbRnRGKi5chXzJB5zJU6g5Cleo8LozARAUf8M8cbnpXiNqtV1SJkrMc9E95VO8UDZpzyNneIuqqurQ0iIb0/5QE3JoW58BnVTcqhbS1KjwwLyhga51y1QubNunPnCe3i8ERERERH5F7bxxQnEPLV1RGMg5qotmCfxmCtxAiFPCoUC3RM1iFGr2jWDhVS5ktsz0QNhn/I0doq7QBAEXLhwgXdjyIxc6sZnUDuSS93IEesmT+6uG2e+8A4eb+6zatUq/PLLL9Dr9Rg/fjwWLFiA8PBwp++1WCywWCwejcdisQCCAOG3/6RWH4MAQYBL8dje64ltEOqD8Uo9xJBbzTxZmxZj8qGa+Spbbpgj58rKylBVVSXJZ1ssFhQVFUGtViMoKAgAEBkZiYSEBEnikQtv1MxZbZpjMpmgUrU+YskbLly4ALPZ7DPfHYD7z9UtndN4nnOObXxxAjVPbRnRGKi5chXzJB5zJU4g5UmhUCAhIrTNI6qlzJWcnokeSPuUJ7FTnMhH8BnUROQvOPMFyUn//v0xbNgwvPrqqygsLMTjjz+OF198EX/961+dvv/06dMej6moqAgGoxHVej2EDtJ0/jRUU1MDq9WKGkMNKtvQsVFVXe32mPR6PQxGI06ePIlqD6zfVXKtmSdq0xxfq5mvy8nJkToEn3PlyhU8v/RlVBmvSh2KXWRYKP7ywkLExMRIHYpP8rWa1ZnNKC2+iKSOnRAU3HLnuTeYamtRWn4ZcVd0EDpESh0OAM+dq3lOI3IPuY1oJCLydXJ/Jjq5jp3iRD4iUJ9BTUT+hzNfkJxs3rzZ/v8ZGRl46qmn8PDDD2PZsmVOR5L16NEDarXaozFFRERAHRaGCI0GkZHSXySvCQ+HUqlEuDocUS7EIwgCqqqrERkR4fa7mBW1VVCHhaFnz57IyMhw67rbQm4182RtmuNrNfNVFosFOTk56NOnT6sjXgNNXl4e6qBEl3EzEB6b5PXPFwQB+poaaMLDoVAoUFNRgvKD29ClSxfu083wVs0a16Y5ZXnHcOmzDeg4ahriUrp4LB6xyvKOoeSzDejQIdSl71dPcve5uqVzmsFg8MrNhkT+RE4jGomI5MAdz0QneWGnuIuUSqXUIVAbyKVugfoM6ubIpW7kiHWTJ3fWjTNfeA+PN/fr1KkTLBYLLl++jI4dOzZ5PSgoyOMdVUFBQYBCAcVv/0mtPgYFFAq0KR5PbIeiPhiv1EMMudbMm/H6Ws18HfPUlO0408QlIzKxk9c/XxAEoKoKUZGR9mOnnPt0i7xVs8a1aU5NRQkABcJjExCVJP3vbls8bf1+9QRPnaudrY/HTfPYxhcnEPPU1hGNgZirtmCexGOuxGGexJMqV+56Jrq3cJ9qP3aKu0CpVCI9PV3qMMhFrJs8sW7yxLrJk7vrxpkvvIPHW/udOHECO3fuxIIFC+zL8vLyoFKpkJiYKGFkRERERBSI2MYXJ1Dz1JYRjYGaK1cxT+IxV+IwT+JJnSsxz0QXBAHlepOkHedS58lfsFPcBYIgwGg0IiwszOfuEKHmsW7yJLZuWp3RoeNtSEY8UtnxJhkeb/Lkibpx5gvP4/HWfnFxcdi8eTNiY2Mxe/ZsaLVarF69GlOnTuXoJSIiIiLyOrbxxQnUPLVlRKM7cuULnUGeFqj7VFswV+IwT+L5eq4EQcCZUj325Zah4HLTG5K6J2q8Erev50kuONbeBYIg4OLFi/VTc5FssG7yJKZuWp0R736Xj13HS5Bfpseu4yV497t8aHVGL0ZKDfF4kydnddPqjNj2cyHW7DmDbT8X8rjyQTze2i8pKQnvvPMO9u7di+zsbEybNg0jR47E008/LXVoRERERBSA2MYXJ5DzZBvR2Cc1GiO6J6BPajQSIkKb7SBpb65snUHbf9Fi849F+CKnGJsPF2H7r1qcKdX7TQ0CeZ9yFXMlDvMknq/nqlxvwr7cMuSX18DyW4wWq4D8shrsO12Gcr3JK3H4ep7kgiPFiUi2fsgrx0WdEZnJEVAqFLAKAnKLq/FDXjlHqBK1g+2Gk4s6I9SqIBzTViJHW4UHRqVzJgbyO4MHD8b//u//Sh0GERERERGRz2nYGWRj6wwCgBi1qsUph4mI5K64yoiCywanrxWUG1BcZeR5UEY4UpyIZEurq4VaFQTlb3fDKhUKqFVB0OpqJY6MSN4a3nCSFheOzOQIXPztUQVEREREREREFBjEdAYREfmzSoPZPkK8MYsgoNJg9nJE1B7sFHeRSqWSOgRqA9ZNnlqrW2p0BxhMFlh/+1KyCgIMJgtSozt4IzxqBo83eWpYN95wIh883oiIiIiI/Avb+OIwT+K1J1eB1BnEfUo85koc5kk8X85VlDoEQc08oiJIqUCUOsRrsfhynuRC8k5xrVaLefPmITs7G2PHjsWKFStgtVpb/JuSkhJkZWVh7dq1XoqynlKpRJcuXaBUSp42cgHrJk9i6jYkIx4p0WHILa5GweUa5BZXIyU6DEMz4r0YKTXE402eGteNN5zIA483IiIiIiLP8vZ1S7bxxWGexGtvrnypM8iTuE+Jx1yJwzyJ5+u5So4MQ1qc2ulraXFqJEd651GTvp4nuZD8meLz589Hr169sHv3bly+fBkPPvgg4uPjcd999zX7N8uWLUNQUJAXo6wnCAKqq6sREREBRTONAfI9rJs8ialbanQYHhiVjh/yyqHV1SI1ugOG/tZRTtLg8SZPjes2JCMeOdoq5BZXQ60KgsFk4Q0nPojHGxERERGRZ3n7uiXb+OIESp4EQUC53oTiKiMqDWZEqUOQHBmGeI1K9HYLggCj0YgaixLFVbUur8fWGdTwmeI23uwM8rRA2afcgbkSh3kSz9dzFa9RYXRmAqCof2yERRAQpFQgLU6N0T0SEK/xzuhtX8+TXEjaKZ6Tk4NTp05h48aNiIiIQEREBGbPno2//e1vzTYu9+3bh7Nnz2LMmDHeDRb1O11paSk0Gg13OhlxVjftb8/GtXWkDsmIR6ofd6TKcXvFHm+p0WG4e2BnL0ZGLeF5Up4a1403nMgDjzciIiIiIs+R4rol2/jiBEKeBEHAmVI99uWWoeBy006Y7onitt22nh8vVKPgstHl9fhKZ5CnBcI+5S7MlTjMk3i+niuFQoHuiRrEqFXtukmpvXw9T3Ihaaf48ePHkZqaiqioKPuyXr164dy5c9Dr9dBoNA7vr62txdKlS/HSSy9h+/btoj7DarU6TGukVCqbTHOkUCigUChaXW61WiEIAgTbdLJO3g/A/npry5VKpcP62rO8rdskZrk/bJMgCPbXL1bW4r39+dDqjAhXBSNHq0NOUSXmjkpHanSYbLap8XLAeZ20OiPe+y4f2soG26utwtyR1yAlynEqZF/aJtvnid1WudfJX7ap4XnSXbFLvU2tLfeHbWp8nlQqlUiJ6oC7slLRmFy2yR/r1Hi5bVnj9Xhqm1qbJpKIiIiIyJ9Icd0SaNrG52+olq/ROvtMOW5T49jL9VexL7cU+WW/jdBWABargPxSPSAIiFGHIC5c1Wrs5fqr+PZkCYprrPXrVihgsVqdrqe5GLslhCNGHYLiylpUGs2ICgtBclQY4sJDHD5bzvteS9f75bpNnjpH2I4/2zJ/2Kb2xN7SNjU8n/vLNrU3djn3AyREhCIu3PkjI7xRp4b7VCAeT63FKPa6paSd4jqdDpGRkQ7LbA3NK1euNGlcvvnmm+jfvz+GDBkiunGp1WrtUxZFRkYiMTER5eXlqKqqsr8nNjYWsbGxKC4uhsFgsC9PTExEZGQkioqKYDKZIAgCDAYDjEYjNBoNzp8/75DoLl26IDg4GPn5+Q4xpKeno66uDhcuXLAvUyqVSE9Ph9FoxMWLF+3LVSoVunTpgurqapSWltqXq9VqpKSk4MqVK6ioqLAvb+822aSkpECtVvvlNhUUFMBgMODcuXNQKBT4+XIwtDojElV1UCosCFcJOHOxHD/kReKOvsmy2CZX6nTgdCnOXCxH15hQKBUWaEKBizoj9p+6hMEJ/z3B+No2derUCQDsdWu4TXLZ9/zxeGptm2znSUEQYDab/WKb/LFOjbep4XkyKCjIL7bJH+vUeJu6du0Kq9XqcJ705DZZLBYQEREREQUKKa5bxsfHw2QyObTx+Ruq6TbZrj2YzWaoVCq/2KbGdTpfokPO+WJYrPXX7iIjImEVrNDr9cipqUb/lHDUhQvo2LFjs9tkNBpRUFqF0xcroFAGITgkGGp1OK5evYqrV6/a1xMS3wHR0dEtblNtbS3iOnRAQoQSkZHhUKtDceHCBb/Z9+Lj6x8Xp9VqYTab/WKbPHWOEIT6KfkB+M02AZ6pU8M+CX/ZJvYDSFcn23dfSUkJUlNT/WKb3FknsdctJX+muLNRT86cPXsWW7ZswWeffebS+lNTU6FWqx2WxcfH27/ogP/eyZCcnOzwPtty20FptVpRUlKCsLD66WO7du3q9P3p6elNloeEhDRZDgBhYWFOl0dERDRpXANATEwMoqOjmyxv6zY1Xu6P25SWloaSkhIkJSVBqVTii4KzCFcFIy72vz9samCAtrJWNtvkSp2K9SYkxEQhLva/x4G+woBSgwXp6d18dpsEQYBarbbXreH7/bFO/rJNtvOkP21Tw+X+uk3XXHONw3nSH7bJH+vkLHaNRtPkPOmpbTIYDMjNzW3yt0RERERE/kqK65bR0dEObXz+hmq6TbZrDyEhIVAoFH6xTY2X1wpBCA+PaPACoFQoERlRfz3zqhCEpKS4FrcpLCwMtVY9QlShCAsLs687NDQUoapQ+3psN3/40m9db9fJ1tmUmJjY5DqkXLfJU3WyHX/+tE0NuXObLBaL/XzuL9vEfgDp6mQ79pKSkvxmmxrzxnVLSTvFY2NjodPpHJbpdDr7nTM2giBgyZIlmD9/PhISElz6DKVS2eRCceN/i12uVCqRmpra6vsb3s3S2nLbMP/2Lm/rNoldLudtCg4Odqhbp+gwHNdWQQCgVChgFQQYTBZ0ig5zW+y+VKeWttfZenxpmxrWzVk8YmP0pW2yfa7Y5XLbpsbnSX/YJjHL5b5Njc+TLcUol23yxzo5W97cedIT29Tce4iIiMg3lJWVOYygkFpBQQHqzHVSh0HUZlJdt2yujc/fUM1fo/WHbWosWh2CIKUSlsY3ZiiAIKUCUeoQhxsnmos9OlyFCI2m0XoUTtfjS791pahTSkqK0/e2tB5f36a2LG9tmzxx7U/qbRITo6vLnV1raylGOWwT+wGkrZOzY0/u2yR2uTuvW0raKd67d29cunQJFRUV9sZkTk4OunXrhvDwcPv7Ll68iMOHD+PMmTNYs2YNgPpef6VSib179+LTTz/1SryCIODKlSuIiYlptvDkexrXbUhGPHK0VcgtroZaFQSDyYKU6DAMzYhvfWUyJNft5fEmT6ybPLFu8sS6ERERkU1ZWRlm3DcXFdWG1t/sJbVGA4q0l9ClwTS0RHIixXVLtvHFCYQ8JUeGIS1OjfzymiavpcWpkRwZJnI9HZASGYzCShMAx1y5sh5/Fwj7lLswV+IwT+LJNVeCIKBcb0JxlRGVBjOi1CFIjgxDvEblke2Qa558jaSd4tdddx369OmDVatW4bnnnkNJSQk2btyI+++/HwBwyy23YNmyZcjKysK+ffsc/nb58uVITk7G3LlzvRavIAioqKhAdHQ0dzoZaVy31OgwPDAqHT/klUOrq0VqdAcMzYhHSrR/NgLlur083gCtzuhQtyEZ8Uhl3cgDWDd5Yt2IiIjIpqqqChXVBiQMvRvhsUlShwMAKM07hoLCDbDUsVOc5EmK65Zs44sTCHmK16gwOjMBUAAF5QZYBAFBSgXS4tQY3SMB8RqVqPXEhaswPCMWP16oRsFlY5vX4+8CYZ9yF+ZKHOZJPDnmShAEnCnVY19uGQouNz1Hd0/UuH1b5JgnXyT5M8XXrFmD559/HsOHD4dGo8G0adNw7733AgDOnTsHg8GAoKCgJvPdh4WFQaPRuDwtERFQ31F898DOUofhNYG2vf5AqzPi3e/ycVFnhFoVhGPaSuRoq/DAqHSf7xgnIiIiIiJphMcmITKxU+tv9AL95WKpQyBqN163JKkoFAp0T9QgRq1q9yjEpA5W3JmViuKqWq+MZiQi8nflehP25ZY5zOZhsQrIL6v/d4xahYSIUKnCoxZI3imenJyMd9991+lrLT0U/ZVXXvFUSEREkvshrxwXdUZkJkfYnwWfW1yNH/LKeYMDERERERERkRfwuiVJSaFQICEitN0dK3q9HumJiUiI6OCmyIiIAltxlREFl50/tqig3IDiKiM7xX2UuCePk11kZKTUIVAbsG7yFMh10+pqoVYFQfnbHbtKhQJqVRC0ulqJI2tdINdNzlg3eWLdiIiIiIj8C9v44jBP4jFX4jBP4jFX4jBP4sktV5UGMyyC4PQ1iyCg0uCZxwfJLU++SPKR4nKiVCqRmJgodRjkItZNngK9bqnRHXBMWwmrINhHihtMFqRG+/ZdvYFeN7li3eSJdSMiIiIi8i9s44vDPP2XIAgo15uanWKduRKHeRKPuRKHeRJPjrmKUocgSKFw2jEepFQgSh3i9s+UY558EUeKu8BqtaK0tBRWq1XqUMgFUtdNqzNi28+FWLPnDLb9XAitzihJHHIjdd2kNiQjHinRYcgtrkbB5RrkFlcjJToMQzPipQ6tRYFeN7li3eSJdSMiIiIi8i9s44vDPNUTBAFnSvXY/osWm38swhc5xdh8uAjbf9XiTKkegiAwVyIxT+IxV+IwT+LJMVfJkWFIi1M7fS0tTo3kyDC3f6Yc8+SL2CnuoqqqKqlDoDaQqm5anRHvfpePXcdLkF+mx67jJXj3u3x2jIsUyMdbanQYHhiVjt/1SkJ6gga/65WEeaPSkRLt/i9UdwvkuskZ6yZPrBsRERERkX9hG18c5gko15uwL7cM+eU19tGKFquA/LIa7DtdhnK9CQBzJRbzJB5zJQ7zJJ7cchWvUWF0ZgLSE8IR9NvjT4OUCqQnhGN0jwTEa1Qe+Vy55ckXcfp0Ig/6Ia8cF3VGZCZH2KfAzi2uxg955bh7YGepwyMflxodxv2EiIiIiIiIiIiaKK4youCywelrBeUGFFcZERfu/il8iYgCnUKhQPdEDWLUqmYfX0G+iZ3iRB6k1dVCrQqC8reToFKhgFoVBK2uVuLIiIiIiIiIiIiISK4qDWanz7MFAIsgoNJgdlhWVn2VnTdE5LcEQUC53uS185xCoUBCRCgSIkLdvm7yHHaKu0ChUCA2NpYNhXbS6oz4Ia8cWl0tUqM7YEhGPFI9OCW0lHVLje6AY9pKWAXBPlLcYLIgNbqD12ORGx5v8sS6yZPUdfP294K/kLpuRERERETkXmzji8M81YtShyBIoXDaMR6kVCBKHQKFQoGkpCScLa2fUr3gsgEWQUCQUoG0ODVG90hA90RNwOeS+5R4zJU4zJN47siVIAg4U6rHvlz/Pc9xn3IPdoq7wLbTUdvZnrF9UWeEWhWEY9pK5Gir8MCodI91gEhZtyEZ8cjRViG3uBpqVRAMJgtSosMwNCNeknjkhMebPLFu8iRl3aT4XvAXPN6IiIiIiPwL2/jiME/1kiPDkBanRn55TZPX0uLUSI4Mg0KhQC1U2Hda6/A+27PHASBGrQr4kY7cp8RjrsRhnsRzR67K9Sbsyy3z6/Mc9yn3UEodgJxYrVZcvHgRVqtV6lBkq+EzttPiwpGZHIGLv40Q9BQp65YaHYYHRqXjd72SkJ6gwe96JWHeqHSksKOnVTze5Il1kycp6ybF94K/4PFGRERERORf2MYXh3mqF69RYXRmAtITwhH028jBIKUC6QnhGN0jAfEaFaxWKy6UV6LAScc58N9njwc67lPiMVfiME/iuSNXxVVGFFw2OH3NX85z3KfcgyPFXWQwOD+wSBypnrEtZd1So8Nw98DOkn2+nPF4kyfWTZ6kqptU3wv+gscbEREREZF/YRtfHOapftRg90QNYtSqZp+hKwgCKqqv1k+x7mTKXWfPHg9U3KfEY67EYZ7Ea2+uKg1mp4+SAPzrPMd9qv3YKU5exWdsExFRQ/xeICIiIiIiImobhUKBhIjQFqcFjrY9e9zJa7ZnjxMRyVmU7TznpGOc5zlqiJ3i5FV8xjYRETXE7wUiosBhNplQUFAgdRh2kZGRSEhIkDoMIiIiIo9K0IQgLV6N/PKmIwxtzx4nIpKz5MgwpMWpHZ4pbsPzHDXETnEXKBQKJCYmQuFkqhkSx/aM7R/yyqHV1SI1ugOGZsR79BnbrJs8sW7yxLrJk5R1k+J7wV/weCMiObmqr8T5c/l4fOEShIY2P5LJm2Ij1Pho43vsGCciIp/BNr44zJN4CoUCiZFhGJ2pAhRlKCg3wCIICFIqkBantj97PNBxnxKPuRKHeRLPHbmK16gwOjMBUMBvz3Pcp9yDneIuUCgUiIyMlDoM2fP2M7ZZN3li3eSJdZMnqevm7e8FfyF13YiIXGG+aoRVEYz4IZMQl5ImdTioqShB2cFtqKqqYqc4ERH5DLbxxWGexFMoFFCr1egeJrT47PFAx31KPOZKHOZJPHfkSqFQoHuixq/Pc9yn3IOd4i6wWq0oKipCp06doFQqpQ6HRGLd5Il1kyfWTZ5YN3li3YhIjtQxCYhM7CR1GACAMqkDICIiaoRtfHGYJ/Ea5qq1Z48HMu5T4jFX4jBP4rkrVwqFwq/Pc9yn3IOZc5HJZJI6BGoD1k2eWDd5Yt3kiXWTJ9aNiIiIiMi/sI0vDvMkHnMlDvMkHnMlDvMkHnMlDvPUfuwUJyIiIiIiIiIiIiIiIiIiv8VOcSIiIiIiIiIiIiIiP6RWq6UOgYiIyCfwmeIuUCgUSElJgUKhkDoUcgHrJk+smzyxbvLEuskT60ZERERE5F/YxheHeRJHEARcrjGhrK4D8vMuI0odguTIMMRrVA65EwQB5XoTiquMqDSYm32fP+M+JR5zJQ7zJB5zJQ7z5B7sFHeBQqHgnXUyxLrJE+smT6ybPMmpblqdET/klUOrq0VqdAcMyYhHanSY1GF5HfNAREREROR/5PTbTErM038116EdFx6Cs2U12JdbhoLLBlgEAUFKBdLi1BjdIwHdEzVQKBQQBAFnSvWtvs/fcZ8Sj7kSh3kSj7kSh3lyD3aKu8BqteL8+fPo2rUrlErOPC8Xvlo3dmi0zFfrRi1j3eRJLnXT6ox497t8XNQZoVYF4Zi2EjnaKjwwKj2gzp+2PGh1BgimWuSoOgRkHoiIiIiI/I1cfptJjXmq11yHdo8kDUZ2T8C+3FLkl9WguroaERERsFiB/LIaAECMWoWEiFCU603Yl1uG/PIa+3otVqHJ+/wd9ynxmCtxmCfxmCtxnOWJM324jp3iLrJarVKHQG3ga3Vjx444vlY3EifQ6ybXG17kULcf8spxUWdEZnIElAoFrIKA3OJq/JBXjrsHdpY6PK+x5yEpApU6C6KiI3C6RB9weSAiIiIi8kdy+G3mC5gnNNuhbbUCeaV65JboEaJUQIDg8HcF5QYUVxmREBGK4iojCi4bnK6/4fsCAfcp8ZgrcZgn8ZgrcRrmiTN9tA07xYkkwI4dIv/EG148S6urhVoVBOVvDTqlQgG1KghaXa3EkXkX80BERERERETNdWiHhihRXFWL6lozYtWqJq9bBAGVBjMAoNJghkUQmryn8fuIiMi3cKaPtuFcBEQSYIcGkX9qeMNLWlw4MpMjcPG3kePUfqnRHWAwWWD97Qe7VRBgMFmQGt1B4si8i3kgIiIiIiKi5jq0r5qtiAoLgcXqvLM7SAmoQ4NRUmmEQqFApdGEWrMFaDSiPEipQJQ6xBOhExFRO4mZ6YOaYqe4CxQKBbp06eJzUw5odUZs+7kQa/acwbafC6HVcWdvyBfrxg6N1vli3ah1gV43ud7wIpe6DcmIR0p0GHKLq1FwuQa5xdVIiQ7D0Ix4qUPzKlseTpfoUWkJwekSfUDmgYiIiIjI38jlt5nUmKd6UeoQBDnJQeEVAyLDQtCzYySgADQaDWB/m4B4TSisFgF/P1SIOosVYapgnL9cg0qjGQ07xtPi1EiODIxZ77hPicdcicM8icdcidM4T5zpo204fbqLgoN9K2WcqlccX6vbkIx45GirkFtcDbUqCAaThR0aTvha3UicQK5banQHHNNWwioI9kcjyOWGFznULTU6DA+MSnd4ZvvQ3zqIA4ktDwfPlkOrMyI1OgzDugVeHoiIiIiI/JEcfpv5AuYJSI4MQ1qc2mHqXAAwmCywWK24pVcy/p1fjoIyAyyof9Zsx6gw9E6JxNkyPUr1V3GpshY3X5eEr48DFypqEBochPDQYPszaeM1Tadf91fcp8RjrsRhnsRjrsRpmCfbjVHOOsY500fzuKe5QBAE5OfnIz093WfuWuGzqVvni3Vjx07rfLFu1LpAr5tcb3iRU91So8P4/Yb6PEwakPpb3VKhVHLyHwo8ZpMJBQUFUocBACgoKECduU7qMIjcqqysDFVVVVKHYeeLxxnPQ0TkbnL6bSYl5qlevEaF0ZkJgKJ+qlyLUN/xnRanRqcYNbolhCMxMhTnS3S4KgQhSh0CtSoYP5+/gqNFlQCAo0WV6NspCnf064jqq3VQBSnRMboDkiPDEK9RBUx+uU+Jx1yJwzyJx1yJ0zhPzd0YBQTWTB+uYqe4zMl1ql5ixw6RP+INL0RE3nFVX4nz5/Lx+MIlCA0NlToc1BoNKNJeQhczpycj/1BWVoYZ981FRbXzZ9RJwdeOM56HiIhIagqFAt0TNYhRq1BcZUSlwYwodYhDh3ZcuApmtRXJyYlQKpX4Ia8cvxTq7OsQABwpqsSZUj06x6gxJD0W13aMlGybiIhInJZujAq0mT5cwU5xmZPzVL1ERP6IN7wQEXme+aoRVkUw4odMQlxKmtThoDTvGAoKN8BSx84o8g9VVVWoqDYgYejdCI9NkjocAL53nPE8REREvkChUCAhIhQJEc3foGUw/Pcmt/AOwU6n2zWYLDhbpsfArtGeCpWIiNzovzdGhaDwihHl1VcRHhqM5KgO7BBvATvFZU6uU/USEXmCVmd0GKU9JCMeqRylTUTkt9QxCYhM7CR1GNBfLpY6BCKPCI9N8oljDPDd44znISIikhNOt0tE5F8qjXUoraxFpbEOF3W12He6DAkRoRjdIwHdEzWckr4Rdoq7QKFQ+NxzDThVb+t8sW7UOtZNnqSsm1ZnxLvf5eOizgi1KgjHtJXI0VbhgVHp7BhvBY83eWLdiIiIiIj8C9v44vhTngRBQLne1Oz05+1fVweHXHG6Xef8aZ/yNOZKHOZJPOZKHGd5Kteb8O2p0iY3OlXX1gEAYtSqFmcSCUTsFHdRXV0dQkJCpA7DAafqbZ0v1o2ap9UZcfBsOQoratA5NhxDu3G0r5w0Pt68NXr7h7xyXNQZkZkcYX+cRG5xNX7IK+c5UgSeJ+WJdSMiIiIi8i9s44vjD3kSBAFnSvXYl1uGgstNO6hdGeHX/LrCMKp7PLonRkChUIh6Drmzdbur496X+cM+5S3MlTjMk3i+litPnPfcsc7GeSquMqLgssHpewvKDSiuMrJTvBF2irtAEARcuHCBd63IDOsmL7bRvlqdAdarRpwoDsOxixztKxeNj7fmRm9P6JeC8+V6UR3lYjvVtbpaqFVBUP52nCsVCqhVQdDqaj26zf5A7HmS09P7Fn6/ERERERH5F7bxxfGXPJXrTdiXW+Ywws9iFZBfVv9vV0b4Nbuu0hoYagyIHd4dCREdAIh7DrmNOzvufZm/7FPewFyJwzyJ52u58sR5zx3rdJanSoMZFkFw+n6LIKDSYHYpzkDATnEi8in20b5JEajUWRAVHYHTJXqO9pUpZ6O3fy3U4bVvcqFWBbc6zbkrU6KnRnfAMW0lrIJg/yyDyYLU6A7e3GS/xenpiYiIiIiIiNzHnSP8WlpXXmkViitr7Z3irnBnxz0RkRx44rznqXNplDoEQQqF047xIKUCUWrfGX3vK5RSB0BE1BBH+/oXZ/WsuVqHS5W1yEyOQFpcODKTI3DxtxHIjTXsVG/tvUMy4pESHYbc4moUXK5BbnE1UqLDMDQj3uPbGQhcqQURERERERERtcydI/xaXJdVQKWxbaMFxXTcExH5E0+c9zx1Lk2ODENanNrpa2lxaiRHciBTYxwp7iKlkvcRyBHrJh8NR/sqONpXlhoeb85Gb1cazYgOCxF144MrN0mkRofhgVHpDtN7D/2to5xa19p5kjes+CZ+vxERERER+Re28cXxhzy5c4RfS+sKUSoRFda20YKBNDWvP+xT3sJcicM8iedLufLEec9d62ycp3iNCqMzEwBFfed642nZ4zUql2P1d+wUd4FSqUR6errUYZCLWDd5GZIRjxxtFU6X6KFWqVBcoudoXxlpfLzZ6plbXA21KggGkwUdozrYO8hbm+bc1SnRU6PDOM1+G4g5T3J6et/D7zciIiIiIv/CNr44/pIn2wi/htPp2rg6wq/ZdSkU6HVNMpKj2jZgIFCm5vWXfcobmCtxmCfxfC1XnjjvuWOdzvKkUCjQPVGDGLUKxVVGVBrMiFKHIDkyDPEalU88o93XsFPcBYIgwGg0IiwsjDuTjLBu8mIb7XvwbDkKyquRFh+BYd042lcuGh9vzkZvXxOvwc4jFx06ypu78cFZpzpvknA/MedJ1sL38PuNiIiIiMi/sI0vjr/kyZ0j/JpfVxhGZsQhLrxtndfu7Lj3Zf6yT3kDcyUO8ySer+XKE+c9d6yzuTwpFAokRIS26ZnkgYid4i4QBAEXL15Eenq6TxycJA7rJj+p0WGYNCAV+fn5SE9P9anpU6hlzo43Z6O3k6I6iJrmnFOie4eY8yRr4Xv4/UZERERE5F/YxhfHX/LkzhF+za+rAyw1OgARbYoxUKbm9Zd9yhuYK3GYJ/F8LVeeOO+5Y52+lie5Yqc4ERF5nSvTnHNKdN/BWhARERERERG5jztH+Dlbl9VqRX6JHomJiW1eJ6fmJaJA4onzHs+lvoOd4kRERERERERERERE1ASn5iWiQOOJ8x7Ppb6BcxK7SKXyjylhAg3rJk+smzyxbvLEuskT60ZERERE5F/YxheHeRKPuRKHeRKPuRKHeRKPuRKHeWo/jhR3gVKpRJcuXaQOg1zEuskT6yZPrJs8sW7yxLoREbWP2WRCQUGB1GHYmUwmBAUFoaioCBEREQgKCpIsloKCAtSZ6yT7fCKiQMU2vjjMk3jMlTjMk3jMlTjMk3jMlTjMk3uwU9wFgiCguroaERERnONfRlg3eWLd5Il1kyfWTZ5YNyKitruqr8T5c/l4fOEShIZKP32d2WSC9kIBUtO6wmw2Qx0WBkh4bq81GlCkvYQuZrNkMRARBSK28cVhnsRjrsRhnsRjrsRhnsRjrsRhntyDneIuEAQBpaWl0Gg03OlkhHWTJ9ZNnlg3eWLd5Il1IyJqO/NVI6yKYMQPmYS4lDSpw0Fp3jHkn9+AmMF3QhUZiwiJz+2lecdQULgBljp2ihMReRPb+OIwT+IxV+IwT+IxV+IwT+IxV+IwT+7BTnEiIiIiIiIKSOqYBEQmdpI6DOgvFwOoj6dDTBIiIyMlvdBhi4eIiIiIiIjIXyilDoCIiIiIiIiIiIiIiIiIiMhT2CnuIrVaLXUI1AasmzyxbvLEuskT6yZPrBsRERERkX9hG18c5kk85koc5kk85koc5kk85koc5qn9OH26C5RKJVJSUqQOg1zEuskT6yZPrJs8sW7yxLoREREREfkXtvHFYZ7EY67EYZ7EY67EYZ7EY67EYZ7cgyPFXSAIAioqKiAIgtShkAtYN3li3eSJdZMn1k2eWDciIiIiIv/CNr44zJN4zJU4zJN4zJU4zJN4zJU4zJN7sFPcBdzp5Il1kyfWTZ5YN3li3eSJdSMiIiIi8i9s44vDPInHXInDPInHXInDPInHXInDPLkHO8WJiIiIiIiIiIiIiIiIiMhvsVOciIiIiIiIiIiIiIiIiIj8FjvFXRQZGSl1CNQGrJs8sW7yxLrJE+smT6wbEREREZF/YRtfHOZJPOZKHOZJPOZKHOZJPOZKHOap/dgp7gKlUonExEQolUybnLBu8sS6yRPrJk+smzyxbu6h1Woxb948ZGdnY+zYsVixYgWsVqvUYRERERFRAGIbXxzmSTzmShzmSTzmShzmSTzmShzmyT2YPRdYrVaUlpbyQqnMsG7yxLrJE+smT6ybPLFu7jF//nwkJSVh9+7d2LhxI3bv3o2//e1vUodFRERERAGIbXxxmCfxmCtxmCfxmCtxmCfxmCtxmCf3YKe4i6qqqqQOgdqAdZMn1k2eWDd5Yt3kiXVrn5ycHJw6dQpPPfUUIiIi0LVrV8yePRubN2+WOjQiIiIiClBs44vDPInHXInDPInHXInDPInHXInDPLUfO8WJiIiIKCAdP34cqampiIqKsi/r1asXzp07B71eL2FkRERERERERERE5E7BUgfgKbYpBIxGo1vXabFYYDAYOG+/jLBu8sS6yRPrJk+smzx5u262NpU/TdOk0+kQGRnpsMzWQX7lyhVoNBr7ctt219TUwGKxeDQuk8mEzqkdEWGpQgdDiUc/S4yYoKvontEV0TC4FI8gCBDMRnQwGKFQKHwiJk+RWzyerE1bY/I2n41HYYDKXOHV2rQYj4/kB5A+psbHjdTxNOZr8QDei0nsOc3XcuRr8QCAYKlC59SOMJlMqK6ubvf6bO0nvV7fpL1aW1vr8J5A09x1S/42E4d5Eo+5Eod5Eo+5Eod5Eo+5Eod5apnY65YKQRAEbwTkbZcvX8b58+elDoOIiIjIr3Tt2hVxcXFSh+EW69evx9dff41PPvnEvqygoAA333wzdu/ejc6dO9uXs21JRERE5H7+1LZ0BduWRERERO7XWtvSb0eKR0VFoWvXrggNDeVdE0RERETtZLVacfXqVYepxuUuNjYWOp3OYZlOp4NCoUBsbKzDcrYtiYiIiNzHH9uWrmDbkoiIiMh9xLYt/bZTPDg4OCDvNCUiIiLylIbTifuD3r1749KlS6ioqLB3gufk5KBbt24IDw93eC/blkRERETu5W9tS1ewbUlERETkXmLalrwVkYiIiIgC0nXXXYc+ffpg1apV0Ov1yMvLw8aNGzF9+nSpQyMiIiIiIiIiIiI38ttnihMRERERtaa4uBjPP/88fvzxR2g0GkybNg1//OMfoVAopA6NiIiIiIiIiIiI3IQjxUXSarWYN28esrOzMXbsWKxYsQJWq1XqsKgRrVaLP/zhD8jOzsawYcOwYMECVFVVAQBOnjyJGTNmYODAgbj55puxYcMGiaMlZ15++WVkZmba/33w4EFMnjwZAwYMwG233YadO3dKGB01tm7dOowYMQL9+/fH7NmzUVRUBIB182UnTpzArFmzMGjQIAwfPhxPPfUUKioqALBuvmb//v0YNmwYnnjiiSavffnll5gwYQKysrIwadIkfP/99/bXrFYrXnvtNdxwww0YPHgw5syZg8LCQm+GLivJycl49913ceTIERw4cADz589nh7hIbWkfl5SUICsrC2vXrvVSlIFLbH3Wrl2Lnj17ok+fPg7/lZeXSxB1YHDl2MnLy8PMmTPRr18/jB49Gh988IF3gw0wYmtz//33NzlmevbsiTfeeEOCqAOD2NpYrVasWbMG48aNQ1ZWFiZMmIAvv/xSgogDh9jamM1mrF69GjfccAP69++PWbNmBXQb9fDhw03OI71797ZfD+Fvs3qt5enHH3/E1KlTMWDAAIwbNw5vvfWWxBFLp7Vc2VitVkyaNAkzZ86UKFJptZYnvV6PZ599FgMGDMDgwYPx/PPPo7a2VuKopdFarhpelxg3bhxef/31gO0v4bU2cVrKE8/njlrKlU2gn8/bTCBR7rrrLmHRokVCVVWVcO7cOeHmm28WNmzYIHVY1Mjtt98uLFiwQNDr9cKlS5eESZMmCQsXLhSMRqMwcuRIYe3atUJNTY1w7Ngx4frrrxd27doldcjUwIkTJ4Trr79e6NGjhyAIglBSUiL0799f2LJli1BbWyscOHBA6Nu3r3D06FGJIyVBEISPPvpIuOWWW4S8vDyhurpa+Mtf/iL85S9/Yd18mNlsFoYPHy6sWrVKuHr1qlBRUSHcd999wvz581k3H/POO+8IN998szBt2jTh8ccfd3jtxIkTQu/evYV//etfQm1trbBjxw6hX79+wqVLlwRBEIQPP/xQGDt2rHD27FmhurpaWLp0qTBhwgTBarVKsSnkx9rSPv7jH/8oDBw4UFizZo2XogxcYuuzZs0a4dlnn5UgwsAltjZGo1EYM2aM8O677woGg0E4cuSIcNtttwlnz56VIOrA0Nbf/ZWVlcLw4cOFU6dOeSHKwCS2Nh999JEwYsQIIS8vT6irqxP27t0rXHfddcLJkycliDowiK3NG2+8IYwZM0Y4efKkYDQahddff124/fbbBYvFIkHUvmndunXCY489xt9mrbDlSavVCv379xf+/ve/CyaTSThy5IgwcOBAYfv27VKH6DNsuWroww8/FAYOHCjMmDFDmqB8UMM8zZ8/X5g/f75QUVEhXLp0Sbjvvvu4TzVgy9WpU6eE6667Tti7d69QV1cn5OXlCSNGjBA++ugjqUP0Ol5rE6elPPF87qilXDXE83nbcKS4CDk5OTh16hSeeuopREREoGvXrpg9ezY2b94sdWjUQFVVFXr37o0nn3wS4eHhSE5Oxl133YWffvoJ//rXv2A2m/Hwww9DrVajV69emDJlCmvoQ6xWKxYvXozZs2fbl3322Wfo2rUrJk+ejNDQUAwbNgzjxo3Dli1bpAuU7DZs2IAnnngC6enp0Gg0WLRoERYtWsS6+bCysjKUlZVh4sSJUKlUiImJwU033YSTJ0+ybj4mNDQUW7duRVpaWpPXtmzZgtGjR2P06NEIDQ3FHXfcgR49etjvNt68eTNmz56NjIwMaDQaPPHEE8jLy8ORI0e8vRnkx9rSPt63bx/Onj2LMWPGeC/QAMXfL77Lldr83//9HzQaDebOnYuwsDD07dsXn3/+OTIyMiSI3P+157h5/fXXcdNNNzUZjUfu4Uptjh8/joEDByI9PR1BQUEYO3YsoqOjkZubK0Hk/s+V2uzduxdTpkzBtddeiw4dOmD+/PmoqKhgG/U3Fy9exMaNG/HMM8/wt1kLGuapvLwckydPxvTp0xESEoK+ffti2LBh+Omnn6QO0yc0zJVNaWkp1q1bhxkzZkgYmW9pmCetVou9e/fihRdeQExMDJKTk7FhwwZMnDhR6jB9QsNcnTx5ElFRURg7diyCgoKQnp6OQYMG4cSJE1KH6XW81iZOS3ni+dxRS7my4fm87dgpLsLx48eRmpqKqKgo+7JevXrh3Llz0Ov1EkZGDUVGRmL58uWIj4+3L7t06RISExNx/PhxZGZmIigoyP7addddh2PHjkkRKjnxv//7vwgNDcWECRPsy44fP47rrrvO4X2sm28oKSlBUVERKisrceuttyI7OxuPPvooKioqWDcflpSUhJ49e2Lz5s2oqanB5cuX8fXXX2PMmDGsm4+ZNWsWIiIinL7WXK1ycnJQW1uLs2fPOryu0WiQlpaGnJwcj8ZMgcXV9nFtbS2WLl2KxYsXIzg42JuhBiRX65Obm4tp06bZp/Rr+EgGci9XavPzzz+jR48eeO655zBo0CDccsstAT3doqe19Xd/QUEBtm/fjvnz53sjzIDkSm3GjBmDH3/8ESdPnoTJZMKePXtgNBpx/fXXezvsgODqcdPwETVKpRIajcbhAm8gW716Ne6++26kpKTwt1kLGuapb9+++POf/+zw+qVLl5CUlCRRdL6lYa5sXn75ZUybNg1dunSRMDLf0jBPP//8Mzp27IgdO3ZgxIgRGDlyJFauXIm6ujqpw/QJDXN1/fXXo7a2Fl9++SVMJhPOnDmDn376KSBvgOa1NnFayhPP545aypUNz+dtx05xEXQ6HSIjIx2W2Rr8V65ckSIkEiEnJwcfffQRHn74Yac1jI6Ohk6nC9hnnfiS8vJyrF27FosXL3ZY3lzdeNxJr7i4GADw1VdfYePGjdixYweKi4uxaNEi1s2HKZVKrF27Fnv27MGAAQMwbNgw1NXV4cknn2TdZESn0zlceATq2yVXrlxBZWUlBEFo9nUid3G1ffzmm2+if//+GDJkiFfiC3Su1Cc5ORmdO3fGq6++igMHDmDKlCl46KGHkJ+f77V4A4krtSkuLsaePXswbNgw7N+/Hw8++CCeffbZgByB4w1t/d3/zjvv4O6770ZsbKxH4wtkrtTm5ptvxtSpU3HnnXeiT58+ePLJJ7F8+XJ07NjRa/EGEldqM3bsWGzevBm5ubkwmUz4+OOPUVxcjMrKSq/F66uKiorw9ddf47777gPAayHNaZynxjZt2oQLFy5g2rRpXo7M9zjL1f79+3H8+HE8+OCDEkbmWxrnqbi4GCUlJbh06RJ27dqFN954A1u3bsVHH30kcaTSa5yrlJQUrFq1CgsXLkSfPn1w++2344477sBNN90kcaTex2tt4rSUp8YC/XzeWq54Pm8fdoqLJAiC1CGQC37++WfMmTMHTz75JIYNG9bs+xrepUzSWb58OSZNmoRu3bpJHQqJZDsnzp07F0lJSUhOTsb8+fOxd+9eiSOjlphMJjz00EO45ZZb8NNPP+G7775DREQEnnrqKalDIxe11i5hu4W8Qex+dvbsWWzZsgULFizwcETUkNj6TJkyBWvWrEFaWhrCwsIwe/Zs9OzZkyOSPUhsbQRBQK9evTBhwgSEhYXhrrvuQt++ffHVV195OMLA5er3p06nw44dOzBr1iwPRUQ2Ymuzfft2bN++HVu2bMHRo0fx+uuvY+HChTh69KiHIwxcYmvzwAMP4MYbb8ScOXMwZswYlJSUYPDgwQ4z+gWqjz/+GDfffDMSEhKkDsWntZSnjz76CKtXr8Zbb73lMINkoGqcq6tXr2Lp0qVYtGgRQkNDJY7OdzjbpywWC5555hmEh4ejX79+mDJlCtteaJqrvLw8PP3001i+fDmOHDmCHTt2YPfu3fjwww8ljtT7eK1NHLF54vm85VzxfN5+7BQXITY2FjqdzmGZTqeDQqHg3eA+aO/evZg3bx4WLlxovzgRGxvb5A4snU6H6OhoKJU8DKR08OBB/PLLL/jDH/7Q5LWYmJgmx96VK1d43PkAW6Ok4d2OqampEAQBZrOZdfNRBw8eRFFREf70pz8hIiICSUlJePTRR/HNN99AqVSybjLh7Nyo0+kQGxtr/15z9npcXJz3giS/J7Z9LAgClixZgvnz5/NCrxe19/dLamoqSktLPRRdYHOlNgkJCU0epZGamoqysjJPhxmQ2nLc7NmzB9dccw06d+7shQgDlyu1+eijjzB16lT07dsXoaGhGDNmDIYMGcIbfTzEldqEhoZi0aJF+P777/Hvf/8bf/rTn1BSUhKwU6M2tGvXLowbN87+b14Lca5xnmxee+01rF+/Hh9++CEGDhwoQWS+p3Gu1q1bh549e2L06NESRuV7GucpPj4eoaGhUKlU9mVse9VrnKtt27ahb9++GD9+PDp06IBrr70W9957b0A+K5vX2sRpKU+2HPF8Xq+lXP31r3/l+byd2BsoQu/evXHp0iVUVFTYl+Xk5KBbt24IDw+XMDJq7D//+Q+effZZrF69Gnfeead9ee/evZGbm+vwDJicnBz069dPgiipoZ07d+Ly5csYO3YssrOzMWnSJABAdnY2evTo0eQZK8eOHWPdfEBycnKT579ptVqEhIRg9OjRrJuPslgssFqtDqM5TCYTAGDYsGGsm0z07t27Sa1s32mhoaHo3r07jh8/bn+tqqoKFy5cQN++fb0dKvkxse3jixcv4vDhw1izZg2ys7ORnZ2NL774Au+99x7uuusuKUIPCK78fnnrrbdw8OBBh2V5eXns5PMQV2qTkZGB06dPO3xva7VapKamei3eQNKW3/179uzB8OHDvRViwHKlNlarFRaLxWGZrb1L7udKbY4fP+7wfVNSUoKzZ89iwIABXovXF508eRJardbhXNKnTx/+NmvEWZ4AYOPGjfj888+xefPmJs/tDVTOcrVz504cOHDA3h7/y1/+gv/85z/Izs7GpUuXJIxWOs7y1K1bN9TU1KCwsNC+TKvVOjyXPRA5yxW/b/+L19rEaSlPAM/nDbWUq2+//Zbn83Zip7gI1113Hfr06YNVq1ZBr9cjLy8PGzduxPTp06UOjRqoq6vDokWL8NRTT2HEiBEOr40ePRoajQbr1q2D0WjEkSNHsHXrVtbQByxYsAC7du3Cjh07sGPHDrzzzjsAgB07dmDChAnQarXYsmULrl69in379mHfvn245557JI6agoODMXnyZKxfvx4FBQW4fPky3nzzTUyYMAF33XUX6+ajsrKyoFarsXbtWhiNRly5cgXr1q3D4MGDMXHiRNZNJu655x78+9//xr/+9S9cvXoVW7duxfnz53HHHXcAAKZPn44PP/wQeXl50Ov1WLlyJXr27Ik+ffpIHDn5k9bax7ZpvpKTk7Fv3z779/yOHTswbtw4TJs2zf6dT+4ntj5A/Yi+F198Efn5+bh69So2bNiACxcu8KYFD3GlNnfccQeuXLmC9evXo7a2Fp9//jmOHz9uP9+Te7lSG5uTJ0+iU6dOUoQbUFypzbhx47B161acOnUKdXV1+P7773Hw4EHccMMNUm6C33KlNrm5uXjqqadQUFAAvV6PJUuW4IYbbgj4m7BOnDiB6OhoaDQa+zJeC2nKWZ4KCwuxZs0arFu3jjeMNeAsV5s3b8YXX3xhb48/9thj6N27N3bs2IHExEQJo5WOszz16dMHvXr1wksvvYSqqiqcPHkSW7duxd133y1hpNJzlquxY8fip59+wu7du2E2m5Gfn48tW7YE5DPFea1NnJbyVF1dzfN5Ay3liufz9guWOgC5WLNmDZ5//nkMHz4cGo0G06ZNw7333it1WNTAr7/+iry8PCxbtgzLli1zeO2rr77C+vXrsXjxYrzzzjuIj4/HE088gTFjxkgTLNlFRUUhKirK/m/baP7k5GQAwNtvv41ly5bhxRdfRGpqKlasWIFrr71WkljJ0ZNPPgmTyYQpU6bAbDbjd7/7HRYtWoTw8HDWzUfFxMTg/fffx6uvvopRo0ZBpVLh+uuvx5IlSxAXF8e6+RBbB7btnLh7924A9SNvevTogZUrV2L58uXQarXo1q0b3n77bfvU1NOmTUNZWRlmzpyJmpoaZGdn44033pBmQ8ivtdQ+PnfuHAwGA4KCguzf6TZhYWHQaDScTt3DxNQHqP8+B4DZs2dDp9OhW7du+OCDD5rUjdxHbG2SkpLw9ttv46WXXsJbb72FlJQUvPnmm+jSpYuU4fs1sbWxKSsrC9hnHXqb2No8+OCDqKurwx/+8AdUVFQgNTUVy5Ytw9ChQ6UM36+Jrc1dd92F06dP45577kFdXR3GjBmDJUuWSBi5bygvL2/SJuJvs6ac5Wnnzp0wGo1NOixTUlKwa9cub4bnU5zlqvG/IyMjoVKpArq95yxPCoUCb775Jl544QWMGjUKarUa999/PyZOnChRlL7BWa6ys7Px6quvYvXq1Xj66acRExOD2267DQ899JBEUUqH19rEaSlPW7du5fm8gZZyxfN5+ymEhmPwiYiIiIiIiIiIiIiIiIiI/AinTyciIiIiIiIiIiIiIiIiIr/FTnEiIiIiIiIiIiIiIiIiIvJb7BQnIiIiIiIiIiIiIiIiIiK/xU5xIiIiIiIiIiIiIiIiIiLyW+wUJyIiIiIiIiIiIiIiIiIiv8VOcSIiIiIiIiIiIiIiIiIi8lvsFCciIiIiIiIiIiIiIiIiIr/FTnEiIiIiIiIiIiIiIiIiIvJb7BQnIllasGABMjMzW/xv5syZzf79zJkzcc8997j8mcOHD2/xPePGjcMTTzxh//cnn3ziEFPPnj0xZMgQ3Hfffdi2bRssFovoz3/77bcxduxYXLlyxb6ssLAQkydPRmZmJvLy8kSva+/evZg2bRoGDRqErKwszJkzB8eOHXN4T3FxMR5//HEMHjwYffv2xb333otffvkFAGA2mzF16lQsWLBA9GcSERERucP999+PsWPHwmq1NvueSZMmYcKECaLW17iN17g958zKlSuRmZkpLuAWtKVN2l6HDx9GZmYmRo4c6VJblIiIiCiQtfdapBi264iuXONry9+0ReNrnM39V1RUhMzMTPzjH//waDytycvLw6BBg/B///d/rb5306ZNGDFiBEpKSrwQGRFJKVjqAIiI2uLPf/4znnzySfu/Fy9ejOPHj2Pr1q32ZSEhIc3+/dq1az0aX2Mff/wx0tLSYLVaUVJSgu+++w4vv/wyPv30U7z99tsIDw9v8e+/++47rF27Fn//+98RExMDAPjqq6+waNEiJCYmuhTLd999h0ceeQTz5s3D8uXLcfXqVaxduxazZs3Cjh070LlzZ5hMJtx3331Qq9V4//33ERoaig8//BD3338/du7cic6dO+P111/HxIkT8eGHH2LWrFltzg0RERGRKyZPnownnngCP/zwA4YNG9bk9dOnT+P48eP485//3Kb1b926tcV2ZHuMHTsWr7zyCrKzswF4v00KAFu2bEGPHj1w5swZ7N+/H2PGjPF6DERERERy095rkWLceuutGDlyJGJjYz36N21h+xyb7du3Y+XKldiyZQs6duxoXx4bG4vvv/8eERERHo2nJTU1NXjkkUdw5513Yvz48a2+f+bMmfjpp5/wxz/+Ef/4xz8QHMxuMyJ/xZHiRCRLERERSEhIsP8XGhqKoKAgh2XR0dHN/n10dHSLr7tbTEwMEhISkJSUhL59++KPf/wj/vnPf+LUqVN48cUXW/xbs9mMv/zlL5gwYQL69u1rX/7KK69g0aJFmDt3rkuxfPrpp0hNTcWf/vQnXHPNNbj22mvx4osvoqamBnv27AEAfPnll8jPz8eKFSvQt29fZGZm4sUXX0RkZCTeffddAEDHjh0xd+5cvP7666ioqHAxI0RERERtc+ONNyI6OhqffPKJ09c//fRTqFQq3HHHHW1af2xsrEcu4pWUlODixYsOy7zdJq2ursauXbswa9Ys9O/fH9u2bfPaZxMRERHJWXuvRYrRoUMHJCQkICgoyKN/0xa2z7H9p9FoANS3nRsut+WkQ4cOHo2nJe+//z6uXLmCxx57TPTfPPvsszh58iS2bNniwciISGrsFCciv2ab2mffvn244YYbcPfddwNoOlVlWVkZFixYgKFDh6J3794YN24cXnnlFdTW1nostoyMDPvI60uXLjX7vp07d+LChQt45JFHHJb/7W9/w5133tmmz27cUFapVA7/3r9/P9LS0pCenm5fFhwcjGHDhuG7776zL5s5cyaUSiXef//9NsVBRERE5Cpbh/fu3buh1+sdXrNYLPjss89w0003ITo6uk1tvMbTp+fl5WHGjBno06cPRowYgdWrV0MQBIe/qaurw+rVq3HDDTegV69eGD58OB599FEUFRUBAA4dOoRRo0YBAGbNmoVx48YBaNomNZlMWLVqFcaNG4fevXtj2LBhWLBgAS5fvmx/z4IFCzBx4kQcOnQIkyZNQr9+/XDTTTfh008/bTV3n332GQDglltuwaRJk/Dtt986vbnxyJEjmDlzJvr3748RI0bgmWeeQVlZmf316upqLFmyBMOHD0dWVhamTp2KAwcO2F93Ni38oUOHkJmZaW9Lrl27FoMGDcLu3bsxYsQIPProowDqR/YsW7YMI0eORK9evTBq1CgsXLjQ4RFCLcVYUVGBPn36OB2FP2fOHEyZMqXVPBERERG1VXPXIltrLzb8W9tU6GLafW35GwDYvXs3xo8fjz59+uD222/Hvn37MGfOnHZPAd94+nRbfLa2W79+/TBmzBjs3LkTFy9exP3334+srCzccMMN+PLLLx3WdeTIEcyZMwfDhg1D//798fvf/x7/+c9/Wvx8vV6PjRs3YsaMGQ43un7zzTe4++67MWDAAAwYMADTpk3Dv//9b/vrKSkpuOuuu/Dmm2/yEUNEfoyd4kQUEN5++228/PLLWL9+vdPXn3zySfz0009466238M0332Dx4sXYtm0bXn/9dY/GdcMNN0AQBBw6dKjZ93zzzTfo0aMHOnfu7LA8LS2tTZ85depUFBUV4cMPP4TFYsHVq1exZs0aREVF2acUOnfuXJPPs33mpUuXYDQaAQBhYWEYNmwYdu/e3aZYiIiIiNpi8uTJMBqNTZ4R+P3336OsrMze8dneNp7ZbMaDDz6I8vJybNy4ER988AEqKyuxY8cOh/etX78e7777Lp5++mns3r0b69atg1artXf0ZmVlYdWqVQDqO4MbTrPZ0KJFi/D3v/8djz76KL788kssX74chw4dwgMPPODQEV9RUYE33ngDixYtwvbt25GRkYHnn3++xRstgfqp4W+++WZERETg1ltvRXBwMHbu3OnwnvPnz2P27Nno3Lkz/vnPf+KNN97AiRMn8PDDD9vf8/jjj+PAgQNYuXIltm/fjj59+uDBBx/EiRMnROXVxmKxYNOmTVi3bh2WLFkCAFi2bBk+++wzvPLKK9i9ezdWrVqFQ4cO4YUXXhAVY2xsLG6++WZs3769Sc5++OEHdooTERGRVzS+Ftlae7E5bWn3tfY3Z86cwWOPPYYuXbpgy5YtWLRoEVatWuXR55K/+uqrmDdvHrZv345rrrkGL7zwAhYuXIgZM2bgk08+QZcuXbBo0SLU1NQAqL82+T//8z+wWCx49913sXnzZiQnJ+P+++9vMc4DBw7AYDDYb0K1revxxx/H7373O+zYsQNbtmxB7969MW/ePIc8jhs3DmVlZThy5IjH8kBE0mKnOBEFhFtvvRXZ2dlISEhw+vorr7yCTZs2ISsrCx07dsTo0aMxYsQI7N+/36NxpaSkAABKS0ubfc+PP/6IQYMGue0zhwwZgv/3//4fXnvtNfTt2xf9+/fH119/jQ0bNiApKQlA/QgdZ885t02NVF1dbV82aNAgnD9/vsVtICIiInKnzMxM9OnTp8kU6p988gk6deqEIUOGAGh/G+/w4cMoLCzEc889h0GDBqFbt2544YUXEBMT4/C+e++9Fzt37sQtt9yCjh07om/fvpg8eTKOHz+OiooKqFQqREZGAgCioqKcPvOxpKQEO3fuxEMPPYQ777wTXbp0wejRo7FgwQIcP34cP//8s/29paWleP755zFgwABcc801mDNnDsxmc4ud0idPnsTx48cxefJkAPXtultuuaXJFOqbNm1CaGgoli5dih49eqB///5YsmQJ0tPTcfnyZRw7dgzff/89nn32WQwdOhRpaWl47rnncOuttzaZHr41BoMBs2fPRp8+few5eeKJJ7B161YMHz4cHTt2xODBgzF+/Hh8//339k7u1mKcPn06ioqKHG483bVrF1QqFW699VaXYiQiIiJqi8bXIltrLzanLe2+1v7m888/BwD89a9/xbXXXoshQ4ZgxYoVrd5g2R4TJ07EyJEjcc0112DatGkwGo3Izs7GuHHj7Mtqampw4cIFAMAHH3wApVKJtWvXolevXsjMzMTLL7+M8PBwfPDBB81+zuHDh6FWq9GrVy/7spMnT6Kurg6TJk1C586dkZGRgeeeew6bNm2yt9EBYPDgwfZ1EJF/CpY6ACIib+jdu3eLr5vNZrzzzjv48ccfUVFRAavVCpPJ5PFnPJrNZgD1U5M7YzAYUFNT02xnflv8+OOPeO655zBlyhTcfvvtMBgM2LRpEx555BF8/PHHTkeIt8QWW1lZGRITE90WJxEREVFLpkyZghdeeAEFBQVIS0tDZWUl9u7di4cffhgKhQJA+9t4p0+fBtC0LZmVlWV/DQBCQ0Oxc+dO7NmzByUlJTCbzairqwMAXLlyxWkneGPHjh2DIAhNbobMysoCAJw4ccL+mlqtRo8ePezvsa2/qqqq2fVv2bIFXbp0wfXXX29fNnnyZHz66ac4evQo+vbtCwA4evQoevXq5dA+HTRokP2zd+3aBQD29wP1j+b561//2uo2OtM4t0qlEps2bcJ3332H8vJyWCwWmM1mmM1mmEwmhIaGthpjXFwcunfvjk8//dR+g8SXX36J8ePH22/yJCIiIvKkxm2ctrYX29Lua+1vLly4gC5duiAqKsr+nszMTPvgHU9o2Elt+9yePXs2WWYbiHP06FH069fPYQr00NBQDBgwAMePH2/2c0pLSxEfH2//PQAAAwYMQGxsLGbMmIGpU6di6NChuPbaa+3tbBuNRoOwsDCHxwYRkX/hSHEiCggNG1CN1dTUYMaMGTh48CD+9Kc/YfPmzdi+fbvDNDueUlBQAABITU11+rqtIdhS/M1Zv349srKy7P/Zpmt65ZVX0K9fPyxcuBB9+/bFkCFD8Nprr9mnI7J9nm26osbxKBQKh7sobf/fUmOciIiIyN1uu+02hIWF2UeLf/HFF7BYLPbnNrqjjWdrD6nVaofljWfUeeqpp/D+++9j8uTJ+PDDD7F9+/ZWp8JszPZ89MbtPlsnbsO2WeN4bBo/69zm6tWr+Oyzz3DhwgVce+21yMzMRGZmJn7/+98DgMNo8aqqKqczBtnY2qctvccVDduVgiBgzpw52L59O+bNm4e///3v2L59O6ZNm+bwN63FCNQ/MmjXrl3Q6/UoKSnBTz/9xKnTiYiIyGsat+na2l50td0n5m90Op3TtlTj2ZDcKSwszP7/tg5rZ8tsMer1ehw6dMjh2mZWVhb27t3bYqd1dXV1k9wnJydjy5YtGDp0KD744ANMnDgR48aNw5YtW5r8fUREBK9xEvkxjhQnooB36NAhlJaW4r333sPIkSPtyw0Gg8c/2zaNY3Z2ttPXbY24htOVizVt2jT7M8KB/95xmZ+f3+SCoEqlQmpqqr2TPj09Hf/5z3+arPP8+fNITU1Fhw4d7MtsDcWGFzSJiIiIPM02/fdnn32GJ554Ajt27MDIkSPtj4NxRxvPdkHRaDQ6XLRr2DbT6/X49ttv8cADD+B//ud/7MutVqtL22NrSzVu99n+3Z62lq1zeNOmTU0uEu7cuRNbt27FwoULERoairi4OFRWVja7roYjjVrqmG58oVZM3k+fPo1Tp07hxRdfxKRJk+zLTSaTw/taixGon6Jz1apV2LNnD3Q6HTIyMpqMBiIiIiLyBne1F91FpVKhtra2yfLmOsulEBkZieTkZCxbtqzJa0pl82M9IyIiUFRU1GR5p06dsHjxYixevBhnzpzBpk2bsGjRInTq1AlDhw61v6+6uprXOIn8GEeKE1HAs01h3nCaItszCFu667K9cnJy8PHHH2Pq1KnNTuGpVqsRHh7epud1R0dHIy0tzf6f7TNSUlKQl5fn8F6TyYQLFy7YR6yPGTMGhYWFOHv2rMN79u/fj7Fjxzr8re3uzPj4eJdjJCIiImqPyZMnQ6vV4ptvvsGvv/5qf1424J42Xnp6OoD66RttBEFweL632WyGIAgOn2OxWLBz506n62zus3v37g2lUtnkGYa2z+rTp4+omJ3ZsmULBg0ahOuvvx49e/Z0+G/69OmoqqqyT4veo0cP5OTkOFwo/fXXXzF9+nRcuHABmZmZAOofydPQQw89hE2bNgGov4jZ+NmYv/76a6txOquZXq/H119/DeC/uWstRlsM48ePxxdffIHPP//cYd8gIiIi8iZX24uelpaWhvPnzzvcZHjs2DFotVpJ4nGmf//+OHfuHDp27OhwfVMQhBYf35iYmIjy8nKHGw5OnjyJgwcP2v/dvXt3LF26FBqNBqdOnbIv1+v1MBqNbn2MJRH5FnaKE1HA6927N4KDg7FhwwYUFhbi4MGD+MMf/oDx48dDp9PhxIkTTUanuOrKlSsoKytDWVkZcnNzsX79esyaNQsDBgzA008/3eLfXn/99fjpp58clplMJvv6bKOHbJ/R+AJkYzNnzsSBAwfwxhtvIC8vD6dOncKiRYtQVVVlH5Fz8803o2fPnnjmmWdw9OhR5Ofn47nnnoPZbMbcuXMd1nf48GF07drVPiqLiIiIyFsGDRqEa665Bi+++CLi4+Mdbt5zRxtvyJAhSEpKwqpVq/Drr7/i7NmzWLx4scOo55iYGHTt2hWffPIJcnNzcfLkSTz88MMYOHAggPq2kl6vt8/ac+DAAZw4caJJ53hCQgLuuusuvPPOO/j8889RWFiIPXv2YPny5cjOznZ4hrcrCgoKcPjwYdx6661OX+/SpQt69+5tn0J95syZsFgseOaZZ3Du3DkcPXoUS5cuhclkQufOndG3b19kZ2djxYoVOHToEC5cuIBXX30V33//PQYMGACg/nnjRUVF+Oc//4nCwkJ88skn2LdvX6uxpqenIyoqCh9//DHOnTuHX3/9FXPnzsWNN94IoH70v9FobDVGm+nTp+PAgQM4deoUJk6c2Kb8EREREbWX2Pait4wfPx5msxlLly7F2bNn8eOPP2Lx4sXNPt5RCrNmzUJNTQ2efPJJ5OTkoLCwEP/85z9x5513YvPmzc3+3eDBg2EwGByeO/7rr7/ikUcewbZt21BYWIjCwkJs2LABBoPBXgPgvzd9Dh482HMbRkSS4vTpRBTwUlNT8dJLL2HNmjW4/fbb0aNHD7zwwguIiYnB4cOH8fvf/97pM2ZcYXteI1A/+rtHjx545plnMGXKFAQHt3wqvvHGG/HnP/8ZhYWF9ot8v/zyC2bNmuX0M1JTU7F3795m1zd9+nQIgoB//OMfWL9+PYKDg9GzZ0+88847GDRoEAAgODgY7733HpYvX445c+bAZDIhKysLmzZtQnJysn1dRqMRP/zwA+655x7XEkJERETkJnfffTdWrlyJuXPnOrSr3NHGCw0Nxfr16/Hiiy9ixowZiIqKwpQpUzB9+nS89tpr9vetWLECS5YswZQpU5CUlIR58+Zh4sSJOHPmDJYtW4bg4GBMmjQJN9xwAzZu3Iht27Zh//79TT5vyZIliI2NxcqVK1FWVoaYmBjcdNNNePLJJ9ucn23btiEoKAi/+93vmn3PrbfeihUrVqCoqAgZGRnYuHEjVq5ciTvvvBMajQbDhg3Ds88+a3/W4xtvvIEVK1bg8ccfh9FoRPfu3fH222+jV69eAOo71s+cOYOVK1eirq4OI0aMwKJFixzaxM6o1WqsXLkSy5cvx8SJE5GWlobHH38cWVlZ+OWXX/Doo4/irbfewvDhw1uNEajvnE9KSkJWVpZHn5FJRERE1Box7cWWpgV3p6ysLCxbtgzr1q3DpEmT0L17dzz33HNYvnw5VCqVV2JoTVpaGjZt2oTXXnsNs2bNgtlsRteuXfHss89i+vTpzf7d8OHDoVar8e2339pnWpo+fTqMRiPee+89LF26FCEhIejWrRtWr17tcOPpt99+i4SEBPTr18/j20dE0lAInpwbmIiI2s1sNmP8+PHIzs7GSy+9JHU4Dt577z28+eab2L17N+Li4qQOh4iIiIjI7tixY5g8eTK2bNnSrunniYiIiPxNRUUFIiIiEBISAgCoq6vD8OHDceutt2Lx4sUSR9c+r7/+Ov7xj39gz5490Gg0ov6muLgYN910ExYsWNDqjZxEJF+cPp2IyMeFhITg+eefx44dO5CTkyN1OHYlJSV499138dhjj7FDnIiIiIh8RkVFBQ4fPownnngCt912GzvEiYiIiBrIy8vDyJEj8cILLyAvLw95eXlYunQpqqqqMHnyZKnDa7e5c+ciKioKq1evFv03r776Knr06MHZMIn8HEeKExHJxNtvv43Nmzdj27Ztkk//aDabMXPmTKSlpeHVV1+VNBYiIiIioobuu+8+HD16FDfeeCOef/550SOEiIiIiALF/v378eabb+L06dNQKpXo1q0bHnnkEYwaNUrq0NwiLy8PU6dOxbJly3DLLbe0+N6PPvoI69evx7Zt25CUlOSlCIlICuwUJyIiIiIiIiIiIiIiIiIiv8Xp04mIiIiIiIiIiIiIiIiIyG+xU5yIiIiIiIiIiIiIiIiIiPwWO8WJiIiIiIiIiIiIiIiIiMhvsVOciIiIiIiIiIiIiIiIiIj8FjvFiYiIiIiIiIiIiIiIiIjIb7FTnIiIiIiIiIiIiIiIiIiI/BY7xYmIiIiIiIiIiIiIiIiIyG+xU5yIiIiIiIiIiIiIiIiIiPwWO8WJiIiIiIiIiIiIiIiIiMhv/X9xosu89ss3MAAAAABJRU5ErkJggg==\n"},"metadata":{}}],"execution_count":14}]}