\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{adjustbox}
\algrenewcommand\algorithmiccomment[1]{\hfill {\texttt{// #1}}}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% --- Force Arabic Numbering for Sections ---
\renewcommand{\thesection}{\arabic{section}}
\renewcommand{\thesubsection}{\thesection.\arabic{subsection}}
\renewcommand{\thesubsubsection}{\thesubsection.\arabic{subsubsection}}
\renewcommand{\theparagraph}{\thesubsubsection.\arabic{paragraph}}

% --- Fix the spacing and formatting in section headers ---
\def\thesectiondis{\thesection}        % Adjust spacing for section
\def\thesubsectiondis{\thesubsection}  % Adjust spacing for subsection
\def\thesubsubsectiondis{\thesubsubsection} % Adjust spacing for subsubsection
\begin{document}

\title{Parameter-Efficient Fine-Tuning via Metaheuristic Algorithms for Sentiment Analysis in Foundation Models\\}

\author{
\IEEEauthorblockN{Zayaan K. Khan}
\IEEEauthorblockA{\textit{Computer Science} \\
\textit{University of Surrey}\\
Guildford, UK \\
zk00332@surrey.ac.uk}
\and
\IEEEauthorblockN{Rita San}
\IEEEauthorblockA{\textit{Computer Science} \\
\textit{University of Surrey}\\
Guildford, UK \\
rs02004@surrey.ac.uk}
\and
\IEEEauthorblockN{Steven Thomas}
\IEEEauthorblockA{\textit{Computer Science} \\
\textit{University of Surrey}\\
Guildford, UK \\
st01634@surrey.ac.uk}
\and
\IEEEauthorblockN{Joel D'Souza}
\IEEEauthorblockA{\textit{Computer Science} \\
\textit{University of Surrey}\\
Guildford, UK \\
jd01606@surrey.ac.uk}
}

\maketitle

\begin{abstract}
Large language models (LLMs) have been growing in usage over the years for various purposes such as text generation, translation and understanding the semantic meaning behind a corpus. Transformers which serve as the underlying backbone of many of these large language models are becoming increasingly harder to finetune for optimal performance through standard techniques such as data augmentation, regularisation and discriminative fine tuning (applying different learning rates to each layer of the model), ultimately leading to the model becoming overconfident and having hallucinations due to the vast number of parameters being required to control. We propose LoRA (Low-Rank Adaptation), an algorithm which for a given task ~\cite{ibm_watson}, adapts a foundation model by freezing the weights and injecting trainable rank decomposition matrices into each layer. We will be using LoRA on the DistilBERT foundation model for the downstream task of text classification on a dataset of tweets representing various emotions within their semantic meanings as well as discussing the various approaches a foundation model can use to find optimal hyperparameters through both evolutionary \& metaheuristic algorithms such that overall, we aim to improve the overall confidence score of the foundation model when performing sentiment analysis on a dataset of tweets expressing six different emotions.
\end{abstract}

\setcounter{section}{0}
\section{Introduction}
\noindent Emotion classification in text using sentiment analysis is a core challenge within natural language processing, with this downstream task being applied to mental health monitoring, social media analysis and customer service automation such that being able to accurately understand and classify emotional expressions in short text such as tweets, enables AI models to be able to better respond to user’s needs, specifically in critical scenarios such as detecting possible signals of a distressed user on such social platforms. Foundation models have owed their success to sophisticated pre-training objectives and huge model parameters. Foundation models can effectively capture knowledge from massive amounts of labelled and unlabelled data since the rich knowledge is implicitly encoded in the huge number of parameters that benefits a variety of downstream tasks, demonstrated via experimental verification and empirical analysis. Due to the increase in computational power boosted by the wide use of distributed computing devices and strategies, we can further advance the parameter scale of foundation models from million-level to billion-level and in some cases even a trillion-level. However, training these models from scratch or performing full fine-tuning at every layer requires a lot of computational and financial resources with some costing tens of millions of dollars to train ~\cite{lora_xs} which is impractical for resource-constrained environments. A solution to this is using parameter-efficient fine-tuning methods such as LoRA (Low-Rank Adaptation) which allows selective adaptation of pre-trained models with minimal computational overhead through trainable low-rank matrices whilst keeping the base model frozen and therefore reducing the overall number of hyperparameters from the base model.

Although LoRA improves the model’s efficiency and the problem of overfitting through reducing the number of hyperparameters, its performance is highly sensitive to these changes within the configuration of hyperparameters such that the choice of rank, alpha scaling factor, learning rate and other training hyperparameters can affect the model’s performance. If one resorts to using manual hyperparameter tuning algorithms such as Grid Search then not only is this slow but the search space will not be fully explored and hence this challenge of correctly choosing an efficient hyperparameter 
fine-tuning method is critical for emotion classification tasks in order for the model to capture linguistic patterns across a range of emotions.

This paper presents our research into using the LoRA fine-tuning algorithm to optimise an emotion classification system built using the foundation model DistilBERT where we utilise the emotion dataset, containing tweets across six distinct emotional categories. We aim to fine-tune DistilBERT by adapting its classification head for our task of text classification and then using LoRA combined with metaheuristic and evolutionary optimisation algorithms to then evaluate the performance of the model.

\section{Literature Review}

\noindent As larger foundation models are trained every few months, the technique of fine-tuning, which updates all parameters of a pre-trained model, becomes a critical deployment challenge, \emph{e.g.}, GPT-3 ~\cite{gpt3} with 175 billion parameters. The need for efficient approaches becomes a requirement in the field of transformer fine-tuning.

Adapter tuning ~\cite{adapter_tuning} is an early approach to address this challenge. The approach inserts 'adapter' modules that have a bottleneck architecture between layers in the Pre-Trained Models (PTMs) and only these modules get updated during fine-tuning. On the other hand, an approach like BitFit ~\cite{bitfit} updates only the biases in foundation models while freezing the rest of the other modules. These methods, while effective, tend to constrain where and how the adaptation occurs, which can end up either potentially introducing inference latency, or reducing the expressive capacity of the model for complex tasks.

Low-Rank Adaptation (LoRA) ~\cite{lora_paper} offers greater flexibility by allowing selective adaptation of any weight matrix in the model through low-rank decomposition. This is an approach that can address the previous limitations by utilising trainable rank decomposition matrices into the model layers, that are later merged with the original weights post training to eliminate inference overhead while maintaining competitive performance. 

LoRA freezes the pre-trained weights $W_0$ and injects trainable low-rank decompositions such that the adapted weight matrix $W$ is defined as:
\begin{equation}
    W = W_0 + \Delta W = W_0 + BA
\end{equation}
where given that $B \in \mathbb{R}^{d \times r}$ and $A \in \mathbb{R}^{r \times k}$ then $W \in \mathbb{R}^{d \times k}$,  such that the rank $r \ll \min(d,k)$, and the update $\Delta W$ is scaled by $\alpha/r$.

This architecture introduces several interdependent hyperparameters:
\begin{itemize}
    \item \textbf{rank $r$}: Controls adaptation capacity and parameter count.
    \item \textbf{scaling factor $\alpha$}: Affects the update magnitude.
    \item \textbf{dropout rate}: For regularisation.
    \item \textbf{target modules}: Which layers to adapt.
    \item \textbf{learning rate} and \textbf{warm-up ratio}.
\end{itemize}
These collectively form a complex mixed discrete-continuous optimisation problem that we address through meta-heuristics.

Some traditional methods of hyperparameter optimisation include grid search, random search, and Bayesian optimisation. Both grid search and random search suffer from the curse of dimensionality ~\cite{curse_dim} making them impractical for high-dimensional spaces.

%Bayesian optimisation [] offers an improved sample efficiency by building a probabilistic surrogate model (typically a Gaussian Process) of the objective function. However, the approach faces challenges with mixed discrete and continuous spaces and categorical variables, which are inherent characteristics of LoRA hyperparameter optimisation where parameters like rank and target modules are discrete, while learning rate and dropout are continuous.

%Bayesian optimization relies on the notion of proximity and smoothness in a continuous space that is naturally captured by a common Gaussian Process, hence the failure to accurately optimise discrete variables. Additionally, the structural dependency in LoRA (where changing target modules alters the model architecture) would violate the smooth function assumption underlying most Bayesian optimization approaches (where points close to each other in the input space will have similar output values).

For these problems, meta-heuristic algorithms provide an appealing alternative strategy. Because they can naturally handle both discrete and continuous variable types, population-based techniques like the Genetic Algorithm (GA) ~\cite{holland_ga}, Differential Evolution (DE) ~\cite{das_de}, and Particle Swarm Optimisation (PSO) ~\cite{wang_pso} are especially attractive. Importantly, they can simultaneously explore several regions of the search space and do not require any assumptions regarding the smoothness or differentiability of the objective function. These features make them appealing for costly black-box optimisation problems like hyperparameter tuning in LoRA, where every evaluation requires a complete model training run, which is computationally expensive.

While focused on a Convolutional Neural Network's (CNN) hyperparameters, the recent study by [] provides a valuable insight. Despite the differences in the base model (DistilBERT in our study), the authors found that the algorithms' efficiency was greatly increased by incorporating domain knowledge about valid architectures into the meta-heuristics, mainly through the design of the architecture encoding (e.g., defining search space boundaries and utilising modular building blocks). When defining the search space for LoRA hyperparameters in Transformer models, it will be crucial to explore how we can incorporate domain knowledge into the meta-heuristics to ensure that DistilBERT can be fine-tuned efficiently.

\textbf{[CHECK]}On the other hand, a study by [] compared Differential Evolution (DE), GA, and PSO, and found that DE demonstrated the best performance. The authors concluded that DE's effective ability to balance exploration and exploitation of the search space led to superior optimal solutions, contrasting with PSO's higher risk of premature convergence and GA's slower convergence and limitations for complex hyperparameter configurations.

Overall, it is clear that there are many ways in which parameter-efficient fine-tuning algorithms like LoRA can be applied. Therefore through incorporating knowledge from the existing literature and evaluating performance of population-based methods and evolutionary-based methods, we try to establish an efficient LoRA fine-tuning strategy for the downstream task of analysing the sentiment of sentences from the Emotion dataset.

\section{Optimisation Algorithms}

\subsection{Problem Representation}

\noindent A chromosome (candidate solution) is represented as a vector $G = [g_1, g_2, \dots, g_6]$. The learning rate ($g_1$) is encoded directly as a continuous float. The remaining five discrete parameters are encoded as integer indices pointing to a pre-defined list of valid options. For example, if the valid Ranks are $\{2, 4, 8, 16, 24\}$, a gene value of $2.0$ corresponds to Rank $8$. During fitness evaluation, the vector is decoded with a repair mechanism; continuous genes are used as-is, while index-based genes are rounded to the nearest integer to select the corresponding hyperparameter.

\noindent The search space is defined as follows:
\begin{itemize}
    \item Learning Rate: $[1 \times 10^{-5}, 2 \times 10^{-4}]$
    \item Rank $r$: $\{2, 4, 8, 16, 24\}$
    \item Alpha $\alpha$: $\{8, 16, 32, 64, 96\}$
    \item Warmup Ratio: $\{0.0, 0.06, 0.1\}$
    \item Dropout: $\{0.0, 0.05, 0.1, 0.2\}$
    \item Target Modules: $\{\text{Attention}, \text{Attention+FFN}\}$
\end{itemize}

\noindent We use index-based encoding to ensure uniform exploration across discrete options. For example, a mutation step size of 5 with value-based encoding would have different effects at $rank = 4$ (significant change), as opposed to $rank = 24$ (minimal change). Index-based encoding ensures consistency between irregularly spaced out hyperparameter options. 

\subsection{Algorithm 1: Baseline Random Search}
\noindent To establish a performance baseline, we employed a Random Search (RS) strategy. RS samples hyperparameter configurations uniformly from the defined search space $\Omega$. For continuous parameters (Learning Rate), values are sampled from a uniform distribution $U(min, max)$. For discrete parameters (Rank, Alpha, Warmup, Dropout), values are selected with equal probability from the pre-defined sets. 

While theoretically inefficient in high-dimensional spaces, RS is a strong baseline for Hyperparameter Optimisation (HPO) because deep learning performance is often sensitive to only a few "effective" dimensions (e.g., Learning Rate). RS guarantees that the search is not biased by local gradients, offering a pure exploration strategy that ensures fair comparison against metaheuristic approaches given a fixed computational budget.

% \subsection{Algorithm 1: RCGA BLX-$\alpha$}
% \noindent We propose a Real-Coded Genetic Algorithm (RCGA) for optimising LoRA hyperparameters within a mixed discrete-continuous search space. Unlike standard Binary GAs, which suffer from Hamming cliffs, where small mutations to the bit string can cause drastic changes in parameter values, disrupting the search, RCGA preserves the property of locality. This is crucial for parameters like Rank and Alpha, which possess an ordinal relationship (i.e the categories have a meaningful order but the distance between them is not equal; $r=4 < r=8 < r=16$). By operating in a continuous representation and mapping to the nearest discrete index, the algorithm can exploit local gradients and explore the solution space "between" parents. We can now describe the genetic operators implementing this approach:

\subsection{Algorithm 2: RCGA BLX-$\alpha$}
\noindent We propose a Real-Coded Genetic Algorithm (RCGA) for optimising LoRA hyperparameters. Unlike binary GAs, which face Hamming cliff issues where small bit-flips cause large parameter jumps, RCGA operates directly on the parameter vector. This is crucial for LoRA, where parameters like Rank ($r$) and Alpha ($\alpha$) possess an ordinal relationship ($r=4 < r=8 < r=16$). RCGA allows the optimiser to exploit these local gradients.

% \subsubsection{Crossover (BLX-$\alpha$)}
% \noindent To balance exploration and exploitation, we utilised the Blend Crossover (BLX-$\alpha$) operator. For two parent genes $x_1$ and $x_2$ (where $x_1 < x_2$), the offspring $y$ is sampled uniformly from the interval:
% \begin{equation}
%     [x_1 - \alpha(x_2 - x_1), x_2 + \alpha(x_2 - x_1)]
% \end{equation}
% We selected $\alpha = 0.5$, which allows the algorithm to search slightly outside the bounds of the parents, preserving population diversity. For the index-based genes, the resulting floating-point value is clipped to the valid index range and rounded.

\subsubsection{Crossover (BLX-$\alpha$)}
To balance exploration and exploitation, we utilise the Blend Crossover (BLX-$\alpha$) operator. This operator does not merely swap genes between parents but generates offspring in the interval extended by $\alpha$ beyond the parents' values. For two parent genes $x_1$ and $x_2$ (assuming $x_1 < x_2$), the offspring $y$ is sampled uniformly:
\begin{equation}
    y \sim U(x_1 - I \cdot \alpha, x_2 + I \cdot \alpha), \quad \text{where } I = x_2 - x_1
\end{equation}
We selected $\alpha = 0.5$, allowing the search to extend slightly beyond the parents boundaries to preserve population diversity.

% \subsubsection{Mutation}
% \noindent We applied a random reset mutation with a probability of $0.1$. If a gene is selected for mutation, it is replaced by a new random value sampled uniformly from the initialisation range of that specific parameter.

% rank_index = np.clip(int(round(self.genes[2])), 0, len(RANK_OPTIONS) - 1)
\subsubsection{Mutation and Repair}
A random reset mutation is applied with a probability of $0.1$ per gene to prevent premature convergence. A repair mechanism is strictly enforced after every operation: continuous genes (Learning Rate) are clipped to $[1e-5, 2e-4]$, and discrete genes are rounded to the nearest integer index to map back to valid LoRA configurations (e.g., an index of $1.6$ for Rank maps to index 2, corresponding to $r=8$).

% # Elitism
% offspring.append(Individual(best_individual.genes.copy()))
% \subsubsection{Selection and Elitism}
% A tournament selection mechanism was used to choose parents for the next generation. To ensure convergence stability, we implemented elitism, where the single best individual from the current generation is copied unchanged to the next generation.

\subsubsection{Elitism}
To ensure monotonic performance improvement, we implement strict elitism. The single best individual from generation $g$ is copied unchanged to generation $g+1$, ensuring the highest validation accuracy found so far is never lost due to stochastic crossover or mutation.

\begin{algorithm}[htbp]
\caption{RCGA with BLX-$\alpha$ Crossover}
\begin{algorithmic}[1]
\State \textbf{Input:} Population Size $N$, Generations $G$, $\alpha=0.5$
\State \textbf{Output:} Best Hyperparameter Vector $x_{best}$
\State Initialise population $P$ with random real-valued vectors
\State Evaluate fitness $f(x)$ for all $x \in P$
\For{$gen = 1$ to $G$}
    \State $P_{new} \gets \emptyset$
    \State $x_{best} \gets \text{argmax}_{x \in P} f(x)$
    \State Add $x_{best}$ to $P_{new}$ \Comment{Elitism}
    \While{$|P_{new}| < N$}
        \State Select parents $p_1, p_2$ via Tournament Selection
        \State $child \gets$ \textbf{empty vector}
        \For{each gene $i$}
            \State $d = |p_{1,i} - p_{2,i}|$
            \State $min = \min(p_{1,i}, p_{2,i}) - \alpha \cdot d$
            \State $max = \max(p_{1,i}, p_{2,i}) + \alpha \cdot d$
            \State $child_i \gets \text{Uniform}(min, max)$
            \State $child_i \gets \text{Clip}(child_i, \text{lower\_bound}_i, \text{upper\_bound}_i)$
        \EndFor
        \State Apply Random Reset Mutation to $child$ ($prob=0.1$)
        \State Add $child$ to $P_{new}$
    \EndWhile
    \State $P \gets P_{new}$
    \State Evaluate fitness for all $x \in P$
\EndFor
\State \Return $x_{best}$
\end{algorithmic}
\end{algorithm}

\subsection{Algorithm 3: SHADE}
\noindent The comparative study by Sen et al.~\cite{Dhar_2023} mentioned in the introduction suggests that Evolutionary Algorithms, particularly DE (Differential Evolution), are an ideal choice for navigating gradient-free search spaces. A notable drawback in standard DE is its sensitivity to the manual selection of its control parameters (Crossover Rate $CR$ and Scaling Factor $F$), such that initialising sub-optimal values for these parameters can lead to undesired convergence rates. We instead adopted an enhanced variant of DE: Success-History based Adaptive Differential Evolution (SHADE). SHADE improves on standard DE by automatically adapting its control parameters, driven by historical success information (a form of elitism). This is critical for hyperparameter optimisation where the optimal exploration-exploitation balance may shift as the search progresses through the fitness landscape.

Thoughtful considerations regarding the computational expense of training transformer models (approximately 3-5 minutes per evaluation) were carried out, leaving us with a severely constrained optimisation budget (100 evaluations). However, SHADE's adaptive mechanism allows it to learn from successful parameter combinations, and focus computational resources more effectively compared to random or grid search approaches. 

The LoRA hyperparameter optimisation problem presents itself as a mixed continuous-discrete search space. We employ the repair mechanism to map the outputs of SHADE's continuous mutation operators to discrete values. The continuous candidate vectors produced by the mutation operator can be defined as: \begin{equation} v_i = x_i + F_i \cdot (x_{pbest} - x_i) + F_i \cdot (x_{r1} - x_{r2}) \end{equation} This approach allows for smooth exploration while respecting discrete constraints.

Furthermore, SHADE maintains an external archive of recently replaced solutions, providing additional diversity for the mutation operator. This is especially valuable in hyperparameter optimisation where similar configurations may perform differently due to local interactions between parameters.

\begin{algorithm}
\caption{Success History Adaptive Differential Evolution}
\begin{algorithmic}[1]
\State \textbf{Input:} $N=20$ (pop size), $H=20$ (memory size), $G_{max}=5$, $arc\_rate=1.0$, $p=0.4$

\vspace{0.5em}
\State \textbf{Initialise:}
\State $M_{CR}, M_F = [0.5]\times H$ \texttt{// historical parameter memories}
\State $Archive = \emptyset, k = 0$
\State $P = \{\text{random individuals}\}$, evaluate all, track $x_{best}$

\vspace{0.5em}
\State \texttt{// Main Loop}:
\For{$generation = 1$ \textbf{to} $G_{max}$}
    \vspace{0.5em}
    \State sort population by fitness (descending)
    \State $p_{num} = \max(2, \lfloor p \times N \rfloor)$

    \vspace{0.5em}
    \State \texttt{// generate offspring}
    \For{\textbf{each} individual $i$}
        \State \texttt{// sample parameters from memory}
        \State $r = \text{rand\_int}(0, H-1)$
        \State $CR_i = 0$ \textbf{if} $M_{CR}[r]==-1$ \textbf{else} $\text{clip}(\mathcal{N}(M_{CR}[r], 0.1), 0, 1)$
        \State $F_i = \min(\text{Cauchy}(M_F[r], 0.1), 1.0)$ where $F_i > 0$

        \vspace{0.5em}
        \State \texttt{// mutation: current-to-pbest/1}
        \State \textbf{Select:} $p_{best}$ (from top $p_{num}$), $r1 \neq i$ (from pop), $r2$ (from pop $\cup$ Archive)
        \State $v_i = x_i + F_i \times (x_{pbest} - x_i) + F_i \times (x_{r1} - x_{r2})$

        \vspace{0.5em}
        \State \texttt{// binomial crossover}
        \State $u_i = \text{crossover}(x_i, v_i, CR_i)$
        \State $u_i = \text{repair}(u_i)$
        \State $f_{ui} = \text{evaluate}(u_i)$
    \EndFor

    \vspace{0.5em}
    \State \texttt{// selection and parameter recording}
    \State $S_{CR}, S_F, \Delta f = []$
    \For{\textbf{each} individual $i$}
        \If{$f_{ui} \geq f_i$}
            \If{$f_{ui} > f_i$}
                \State $Archive \gets x_i$ (randomly replace if full)
                \State Record: $S_{CR} \gets CR_i, S_F \gets F_i, \Delta f \gets |f_{ui} - f_i|$
            \EndIf
            \State $x_i \gets u_i, f_i \gets f_{ui}$
            \State Update $x_{best}$ if improved
        \EndIf
    \EndFor

    \vspace{0.5em}
    \State \texttt{// update memory (weighted Lehmer mean)}
    \If{$|S_{CR}| > 0$}
        \State $w = \Delta f / \text{sum}(\Delta f)$ \Comment{weights from improvement}
        \State $M_F[k] = \Sigma(w \times S_F^2) / \Sigma(w \times S_F)$
        \State $M_{CR}[k] = -1$ \textbf{if all} $S_{CR}=0$ \textbf{else} $\Sigma(w \times S_{CR}^2) / \Sigma(w \times S_{CR})$
        \State $k = (k+1) \mod H$
    \EndIf
\EndFor
\vspace{0.5em}
\State \textbf{Output:} $x_{best}, f_{best}$
\end{algorithmic}
\end{algorithm}

SHADE has demonstrated superior performance on the CEC2014 benchmark suite and various real-world optimisation problems, particularly excelling in problems with limited evaluation budgets; precisely our scenario~\cite{2014SHADE}. Although JADE (Joint Adaptive Differential Evolution) similarly addresses the limitations of manual parameter tuning found in standard DE, it does not guarantee diversity within the population due to its reliance on a single average. SHADE improves upon this foundation by saving successful parameters to a history list and randomly selects parameter pairs from the list, preventing premature convergence.~\cite{2013SHADE}

\section{Experimental Results}

\subsection{Experimental Setup \& Parameter Ranges}

% \noindent We initialise a population of candidates with random configurations of indices mapped to a specific set of LoRA hyperparameters we decide on. The fitnesses of these individuals are then calculated by obtaining the validation accuracy after training DistilBERT for 3 epochs. The following process outlines the method of evaluation:

\noindent We initialise a population of candidates with random configurations. The fitness of these individuals is calculated by obtaining the validation accuracy after training DistilBERT for 3 epochs. The evaluation process is defined as follows:

\begin{enumerate}
    \item Decode the index vector x into LoRA hyperparameters via a repair function.
    \item Load a fresh \texttt{distilbert-base-uncased} model.
    % \item Apply LoRA adaptation with decoded hyperparameters and freeze the original weights of DistilBERT
    \item Apply LoRA adaptation and freeze original weights.
    % \item Train the LoRA matrices and the final, fully connected layer for 3 epochs on 3,000 samples from the Emotion datasets train set using the AdamW optimiser
    \item Train the LoRA matrices and classifier head for 3 epochs on 3,000 samples (Emotion training set) using the AdamW optimiser.
    \item Evaluate on the full validation set (2,000 samples)
    % \item Return validation accuracy as fitness (maximisation objective) % Not needed as the lead-in sentence already establishes this.
\end{enumerate}

% \noindent All algorithms are initialised with a population of 20 with individuals being evaluated at the start. Offsprings and updates are generated from the starting population and undergo four generational updates, totalling for 100 evaluations. We have chosen these small values (relative to literature) for population and generations as the search space itself has been constrained due to the computational intensiveness of the task at hand. Three epochs are used during training for all algorithms to reinforce fairness, and limit evaluation time. P100 GPUs are used in training for their speed and availability. Finally, the top five configurations are taken from each algorithm and are trained on three unique seeds. The results shown on Table ~\ref{tab:shade_comparison} and Table ~\ref{tab:params} are from the best performing configuration of each algorithm.

\noindent The baseline Random Search was allocated a budget of 20 trials. The metaheuristic algorithms (RCGA, SHADE, PSO) were initialised with a population of 20 and evolved for 5 generations (100 evaluations total). This budget constraint mimics real-world scenarios where foundation model fine-tuning is computationally expensive. Finally, to verify robustness, the top performing configurations were re-evaluated using 3 different random seeds to report the mean accuracy and standard deviation. The results shown on Table ~\ref{tab:shade_comparison} and Table ~\ref{tab:params} are from the best performing configuration of each algorithm.

\begin{table}[htbp]
    \centering
    \caption{Algorithm Parameter Ranges}
    \label{tab:params}
    \begin{tabular}{ll}
    \hline
    \textbf{Algorithm} & \textbf{Parameters} \\
    \hline
    Random Search & Trials $= 20$ \\
    PSO & $w=0.7$, $c_1=c_2=1.5$ \\
    RCGA BLX-$\alpha$ & $\alpha=0.5$, mut. prob. $<10\%$ \\
    SHADE & $H=20$, A $=1.0$, $p_{\text{best}}=0.4$ \\
    \hline
    \end{tabular}
\end{table}

\noindent For PSO, $w$ controls velocity momentum while $c_1$ and $c_2$ are used for cognitive and social learning respectively. In RCGA BLX-$\alpha$, $\alpha$ defines the exploration range beyond parent bounds, with mutation applied to individual hyperparameters. SHADE adapts its parameters using a history of size $H$, maintains an archive of inferior solutions scaled by the archive rate, and an individual from the top $p_{\text{best}}$ fraction of the population is used to help create a mutant vector.

\section{Results and Key Findings}
\noindent The experimental set up comprises of 4 metaheuristic algorithms; The baseline 20-trial random search, the population based PSO, and two chosen algorithms, RCGA BLX-alpha and SHADE.

\begin{figure}[htbp]
    \centering
    \resizebox{0.5\textwidth}{!}{%
    \begin{tabular}{|c|c|c|c|c|c|}
    \hline
    \multicolumn{6}{|c|}{Top Configuration Three Seed Results}\\
    \hline
    Algorithm & Best Acc. & Mean Acc. & $\pm$Std. & Evaluations & Time (min)\\
    \hline
    Random Search & 0.878 & 0.877 & 0.003 & 20 & 21.04\\
    \hline
    PSO & 0.905 & 0.893 & 0.002 & 100 & 108.44\\
    \hline
    RCGA BLX-$\alpha$ & 0.906 & 0.896 & 0.004 & 100 & 108.55\\
    \hline
    SHADE & 0.900 & 0.897 & 0.003 & 100 & 72.90\\
    \hline
    \end{tabular}%
    }
    % \caption{Three seed performance of top LoRA configurations}
    \caption{Robustness Metrics (Top Configuration, 3 Seeds)}
    \label{tab:shade_comparison}
\end{figure}

\begin{figure}[htbp]
    \centering
    \resizebox{0.5\textwidth}{!}{%
    \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
    \multicolumn{7}{|c|}{Best Performing LoRA Configurations}\\
    \hline
    Algorithm & Learning Rate & Warm Up & Rank & Alpha & Dropout & Target Modules\\
    \hline
    % RS & 0.0002 & 0.06 & 4 & 64 & 0.1 & Attn+FFN\\
    % RS & 1.7e-4 & 0.06 & 4 & 64 & 0.1 & Attn+FFN\\
    RS & $1.7 \times 10^{-4}$ & 0.06 & 4 & 64 & 0.1 & Attn+FFN\\
    \hline
    % PSO & 0.0002 & 0.1 & 4 & 96 & 0.0 & Attn+FFN\\
    PSO & $2.0 \times 10^{-4}$ & 0.1 & 4 & 96 & 0.0 & Attn+FFN\\
    \hline
    % RCGA BLX-Alpha & 0.0002 & 0.0 & 16 & 96 & 0.0 & Attn+FFN\\
    % RCGA & 2.2e-4 & 0.0 & 16 & 96 & 0.0 & Attn+FFN\\
    RCGA & $2.2 \times 10^{-4}$& 0.0 & 16 & 96 & 0.0 & Attn+FFN\\
    \hline
    % SHADE & 0.0002 & 0.0 & 24 & 96 & 0.0 & Attn+FFN\\
    SHADE & $2.0 \times 10^{-4}$ & 0.0 & 24 & 96 & 0.0 & Attn+FFN\\
    \hline
    \end{tabular}%
    }
    \caption{Best LoRA hyperparameter configurations chosen by algorithms}
    \label{tab:shade_comparison}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{images/convergence_plot.png}
    \caption{Best Accuracy of Algorithm vs. Objective Evaluations}
    \label{fig:convergence_plot}
\end{figure}

\noindent Convergence Analysis:
All four algorithms converged to values for learning rate (0.0002) and target modules (attention + feedforward), suggesting that these represent ideal optima within the search space. Similarly, the meta-heuristic methods identified alpha=96 and dropout=0.0 as optimal, while Random Search resolved at suboptimal values (alpha=64, and dropout=0.1). This is likely due to its limited smaller evaluation budget of 20 evaluations, compared to 100 for the meta-heuristic algorithms. The most notable difference occurred in the rank parameter. SHADE and RCGA BLX-Alpha discovered higher rank values of 24 and 16 respectively, which could correlate with their marginally better performance. PSO converged to rank=4, suggesting that its velocity based updates were less effective at escaping the local region compared to the mutation based exploration of SHADE and RCGA. All metaheuristic methods rejected dropout regularisation. This could potentially be because the 3 epoch training loop for evaluations is too short for overfitting to occur; leading to the benefit of dropout regularisation not being able to manifest.  

The clearest connection from these results was that the more trainable parameters that are available, the better the model performs. This is shown in the two best metaheuristic algorithms converging to a higher rank, and all top configurations of the four algorithms selecting both attention heads and feed forward layers as the target modules for LoRA's application, greatly increasing the number of trainable parameters. Another interesting find is the scaling factor alpha being the highest possible value. This shows that the LoRA adapted weights having a very high influence compared to DistilBERTs pre-trained knowledge (weights), is crucial for greater performance. 

Convergence Speed and Search Space Analysis: Figure ~\ref{fig:convergence_plot} shows all the population achieving accuracy $>$0.85 within the 20 initial evaluations. This rapid convergence can be attributed to 2 factors: 
\begin{enumerate}
    \item The constrained discrete search space increasing the probability of sampling good configurations randomly.
    \item AdamW's robustness as the training optimiser pushing the accuracy ceiling higher, allowing many configurations to perform relatively better than if a worse optimiser like SGD was used for training.
\end{enumerate}
The level of convergence suggests that for similarly constrained LoRA fine-tuning problems, a smaller evaluation budget, with simply more random restarts may be a more cost effective approach than an extended metaheuristic search with several generational updates.

Computational Efficiency: SHADE achieved comparable RCGA BLX-Alpha; 0.897 vs. 0.896 in 33\% less time less time; 72.90 vs. 108.55 minutes. This makes our implementation of SHADE the most efficient metaheuristic method in this experiment. However, the 3-5$\times$ computational cost of any metaheuristic algorithm over Random Search yielded only a $\sim$2\% accuracy improvement. This raises questions about the cost-effectiveness of these algorithms. To scrutinise further, if we go back to Figure ~\ref{fig:convergence_plot}, for both SHADE and RCGA BLX-Alpha, the optimal parameters are found by sheer luck in the initial random population generation phase, further reinforcing the point that if for metaheuristic methods, random restarts with checkpoints of best solutions or simple Random Search with more trials may just be the better approach for problems with constrained search spaces like ours.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{images/final_performance.png}
    \caption{Comparison of Final Performances on Three Seeds}
    \label{fig:final_performance}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{images/training_cost.png}
    \caption{Training Time of All Algorithms}
    \label{fig:training_cost}
\end{figure}

\section{Multi-objective Extension}
\noindent In this section, we explore the use of multi-objective evolutionary algorithms to determine the optimal trade-off boundary between validation accuracy and model complexity (measured by trainable parameters) for the LoRA adaptation of DistilBERT. We opted for the Non-dominated Sorted Genetic Algorithm II (NSGA-II), proposed by Deb et al.~\cite{nsga2}, as it provides computational efficiency in bi-objective problems through its fast non-dominated sorting mechanism ($O(MN^2)$). Furthermore, its elitist strategy means that optimal hyperparameter configurations are not lost through stochastic selection processes, but are rather passed onto the next generation. This ensures monotonic improvement in the Pareto front.

Another Pareto front-based multi-objective algorithm worth discussing is SPEA2 (Strength Pareto Evolutionary Algorithm 2), which utilises a k-th nearest neighbour approach to guide the search process. Similarly, SPEA2 demonstrates elitism much like NSGA-II, but by employing a fixed-size external archive system. The archive is updated per generation by ensuring non-dominant solutions are kept to be used to create new offspring. While SPEA2 is a strong candidate for approaching the LoRA hyperparameter optimisation problem, it is more strongly suited for problems with 3 or more objectives--evident from Zitzler et al's paper when comparing SPEA2 and NSGA-II. As the result of the additional nearest neighbour calculations, that is, the euclidean distance calculations between each individual before sorting them, we face an increased computational complexity of $O(M^3)$ (or in some cases, $O(M^2logM)$) where
\begin{equation}
M=population\_size + archive\_size
\end{equation}.~\cite{spea2}

We use the same LoRA hyperparameter configurations as those in the other algorithms discussed in sections 3, with the evolutionary parameters also unchanged (population size $P=20$ and generations $G=5$), matching the exact search space and evaluations to ensure fairness. While the single-objective algorithms discussed in this paper focus on maximising validation accuracy, the complexity of the resultant neural architectures does not contribute to selection pressure. In other words, these algorithms favour models with higher accuracy, regardless of over-parametrisation. For example, one LoRA configuration could produce an accuracy of 85.0\% with $rank=4$ and $\#trainable\_params=600,000$, resulting in low training cost. Another configuration could produce an accuracy of 85.1\% with $rank=24$ and $\#trainable\_params=2,000,000$, in which the training cost has significantly increased, but this cost is insignificant to single-objective algorithms and it will only select the model configuration with higher accuracy (85.1\% $>$ 85.0\%). Therefore, we can justify the need to introduce parameter efficiency as a primary optimisation goal alongside accuracy.

Figure ~\ref{fig:pareto_front} presents the Pareto front procured from the NSGA-II optimisation process, plotting validation accuracy against the number of trainable parameters.

\begin{figure}[H]
    \includegraphics[width=0.5\textwidth]{images/pareto_front.png}
    \caption{NSGA-II: Pareto Front}
    \label{fig:pareto_front}
\end{figure}

\begin{figure}[htbp]
    \centering
    \resizebox{0.5\textwidth}{!}{%
    \begin{tabular}{|c|c|c|c|c|}
    \hline
    \multicolumn{5}{|c|}{NSGA-II Pareto Front Solutions}\\
    \hline
    Solution & Rank & Trainable Params & Accuracy & Target Modules\\
    \hline
    1 & 2 & 0.63M & 82.05\% & Attn\\
    \hline
    2 & 4 & 0.67M & 82.15\% & Attn\\
    \hline
    3 & 2 & 0.72M & 87.75\% & Attn+FFN\\
    \hline
    4 & 4 & 0.85M & 89.55\% & Attn+FFN\\
    \hline
    5 & 8 & 1.11M & 90.20\% & Attn+FFN\\
    \hline
    \end{tabular}%
    }
    \caption{Pareto optimal LoRA configurations from NSGA-II}
    \label{tab:pareto_solutions}
\end{figure}

\noindent The red dots represent Pareto optimal solutions also known as non-dominated solutions, which are characterised by having no further possibility of improving an objective without harming the other objectives~\cite{pareto_front} which, in this case, is the optimal trade-off between validation accuracy and number of trainable parameters. The grey dots on the other hand, represent the Pareto dominated solutions which were discovered during the optimisation process. These solutions are considered sub-optimal as they are overcome by non-dominated (Pareto front) solutions which achieve equal or higher accuracies with the same or fewer amount of trainable parameters.

The front path displays a steep vertical ascent as the number of trainable parameters increase, until the path halts at a validation accuracy 90.2\% with 1.1M trainable parameters. The steepest ascent occurs when the parameters are $<0.72$M, suggesting that the model is constrained by capacity. Evidently from the results, an increase of model size from 0.67M to 0.72M (8\% increase) training parameters provided a gain in accuracy of 82.15\% to 87.75\% (5.6\% increase). This "High-Gain, Low-Cost" behaviour demonstrates that LoRA configurations targeting only attention layers (shown in Table ~\ref{tab:pareto_solutions} [solution 1 \& 2]) underfit the emotion dataset regardless of rank. The jump from 82.15\% to 87.75\% when adding FFN modules, $solution_2 \xrightarrow{} solution_3$, suggests that adapting the feedforward layers is critical for this task.

The knee point becomes visible at approximately 0.72M parameters, representing the most efficient trade-off in the search space as it approaches the peak accuracy. At this point the model size is roughly a third smaller than the largest Pareto solution.

As the front path exceeds the knee point, the curve plateaus, converging to maximum accuracy. During this stage, it can be observed that achieving a final 2\% gain in accuracy requires a costly increase of nearly 53\% in trainable parameters. 

This analysis suggests that the LoRA hyperparameter optimisation problem benefits significantly from multi-objective optimisation algorithms. In contrast to single-objective optimisation algorithms, which have no mechanism to penalise model complexity and tend to select high-capacity configurations for marginal accuracy gains, multi-objective optimisation algorithms can penalise high model-complexity. This strategy ensures smooth convergence towards the 'knee point' of the Pareto front, eventually achieving optimal efficiency by avoiding the selection of over-parametrised LoRA configurations. Ultimately, treating the number of training parameters and validation accuracy as conflicting objectives, introduces a dimension of efficiency to the model selection process that would otherwise be ignored by single-objective algorithms.

\section{Conclusion and Future Work}
\noindent 
\bibliographystyle{IEEEtran}
\bibliography{references}
Due to the nature of the components of this coursework being inseparable, all the group members have agreed to contribute equally in all components. Therefore, each group member has had a significant impact on all graded sections of this work.
\end{document}

\subsection{Abbreviations and Acronyms}\label{AA}
Define abbreviations and acronyms the first time they are used in the text, 
even after they have been defined in the abstract. Abbreviations such as 
IEEE, SI, MKS, CGS, ac, dc, and rms do not have to be defined. Do not use 
abbreviations in the title or heads unless they are unavoidable.

\subsection{Units}
\begin{itemize}
\item Use either SI (MKS) or CGS as primary units. (SI units are encouraged.) English units may be used as secondary units (in parentheses). An exception would be the use of English units as identifiers in trade, such as ``3.5-inch disk drive''.
\item Avoid combining SI and CGS units, such as current in amperes and magnetic field in oersteds. This often leads to confusion because equations do not balance dimensionally. If you must use mixed units, clearly state the units for each quantity that you use in an equation.
\item Do not mix complete spellings and abbreviations of units: ``Wb/m\textsuperscript{2}'' or ``webers per square meter'', not ``webers/m\textsuperscript{2}''. Spell out units when they appear in text: ``. . . a few henries'', not ``. . . a few H''.
\item Use a zero before decimal points: ``0.25'', not ``.25''. Use ``cm\textsuperscript{3}'', not ``cc''.)
\end{itemize}

\subsection{Equations}
Number equations consecutively. To make your 
equations more compact, you may use the solidus (~/~), the exp function, or 
appropriate exponents. Italicize Roman symbols for quantities and variables, 
but not Greek symbols. Use a long dash rather than a hyphen for a minus 
sign. Punctuate equations with commas or periods when they are part of a 
sentence, as in:
\begin{equation}
a+b=\gamma\label{eq}
\end{equation}

Be sure that the 
symbols in your equation have been defined before or immediately following 
the equation. Use ``\eqref{eq}'', not ``Eq.~\eqref{eq}'' or ``equation \eqref{eq}'', except at 
the beginning of a sentence: ``Equation \eqref{eq} is . . .''

\subsection{\LaTeX-Specific Advice}

Please use ``soft'' (e.g., \verb|\eqref{Eq}|) cross references instead
of ``hard'' references (e.g., \verb|(1)|). That will make it possible
to combine sections, add equations, or change the order of figures or
citations without having to go through the file line by line.

Please don't use the \verb|{eqnarray}| equation environment. Use
\verb|{align}| or \verb|{IEEEeqnarray}| instead. The \verb|{eqnarray}|
environment leaves unsightly spaces around relation symbols.

Please note that the \verb|{subequations}| environment in {\LaTeX}
will increment the main equation counter even when there are no
equation numbers displayed. If you forget that, you might write an
article in which the equation numbers skip from (17) to (20), causing
the copy editors to wonder if you've discovered a new method of
counting.

{\BibTeX} does not work by magic. It doesn't get the bibliographic
data from thin air but from .bib files. If you use {\BibTeX} to produce a
bibliography you must send the .bib files. 

{\LaTeX} can't read your mind. If you assign the same label to a
subsubsection and a table, you might find that Table I has been cross
referenced as Table IV-B3. 

{\LaTeX} does not have precognitive abilities. If you put a
\verb|\label| command before the command that updates the counter it's
supposed to be using, the label will pick up the last counter to be
cross referenced instead. In particular, a \verb|\label| command
should not go before the caption of a figure or a table.

Do not use \verb|\nonumber| inside the \verb|{array}| environment. It
will not stop equation numbers inside \verb|{array}| (there won't be
any anyway) and it might stop a wanted equation number in the
surrounding equation.

\subsection{Some Common Mistakes}\label{SCM}
\begin{itemize}
\item The word ``data'' is plural, not singular.
\item The subscript for the permeability of vacuum $\mu_{0}$, and other common scientific constants, is zero with subscript formatting, not a lowercase letter ``o''.
\item In American English, commas, semicolons, periods, question and exclamation marks are located within quotation marks only when a complete thought or name is cited, such as a title or full quotation. When quotation marks are used, instead of a bold or italic typeface, to highlight a word or phrase, punctuation should appear outside of the quotation marks. A parenthetical phrase or statement at the end of a sentence is punctuated outside of the closing parenthesis (like this). (A parenthetical sentence is punctuated within the parentheses.)
\item A graph within a graph is an ``inset'', not an ``insert''. The word alternatively is preferred to the word ``alternately'' (unless you really mean something that alternates).
\item Do not use the word ``essentially'' to mean ``approximately'' or ``effectively''.
\item In your paper title, if the words ``that uses'' can accurately replace the word ``using'', capitalize the ``u''; if not, keep using lower-cased.
\item Be aware of the different meanings of the homophones ``affect'' and ``effect'', ``complement'' and ``compliment'', ``discreet'' and ``discrete'', ``principal'' and ``principle''.
\item Do not confuse ``imply'' and ``infer''.
\item The prefix ``non'' is not a word; it should be joined to the word it modifies, usually without a hyphen.
\item There is no period after the ``et'' in the Latin abbreviation ``et al.''.
\item The abbreviation ``i.e.'' means ``that is'', and the abbreviation ``e.g.'' means ``for example''.
\end{itemize}
An excellent style manual for science writers is \cite{b7}.

\subsection{Authors and Affiliations}
\textbf{The class file is designed for, but not limited to, six authors.} A 
minimum of one author is required for all conference articles. Author names 
should be listed starting from left to right and then moving down to the 
next line. This is the author sequence that will be used in future citations 
and by indexing services. Names should not be listed in columns nor group by 
affiliation. Please keep your affiliations as succinct as possible (for 
example, do not differentiate among departments of the same organization).

\subsection{Identify the Headings}
Headings, or heads, are organizational devices that guide the reader through 
your paper. There are two types: component heads and text heads.

Component heads identify the different components of your paper and are not 
topically subordinate to each other. Examples include Acknowledgments and 
References and, for these, the correct style to use is ``Heading 5''. Use 
``figure caption'' for your Figure captions, and ``table head'' for your 
table title. Run-in heads, such as ``Abstract'', will require you to apply a 
style (in this case, italic) in addition to the style provided by the drop 
down menu to differentiate the head from the text.

Text heads organize the topics on a relational, hierarchical basis. For 
example, the paper title is the primary text head because all subsequent 
material relates and elaborates on this one topic. If there are two or more 
sub-topics, the next level head (uppercase Roman numerals) should be used 
and, conversely, if there are not at least two sub-topics, then no subheads 
should be introduced.

\subsection{Figures and Tables}
\paragraph{Positioning Figures and Tables} Place figures and tables at the top and 
bottom of columns. Avoid placing them in the middle of columns. Large 
figures and tables may span across both columns. Figure captions should be 
below the figures; table heads should appear above the tables. Insert 
figures and tables after they are cited in the text. Use the abbreviation 
``Fig.~\ref{fig}'', even at the beginning of a sentence.

\begin{table}[htbp]
\caption{Table Type Styles}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
\cline{2-4} 
\textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
\hline
copy& More table copy$^{\mathrm{a}}$& &  \\
\hline
\multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
\end{tabular}
\label{tab1}
\end{center}
\end{table}

\begin{figure}[htbp]
\centerline{\includegraphics{fig1.png}}
\caption{Example of a figure caption.}
\label{fig}
\end{figure}

Figure Labels: Use 8 point Times New Roman for Figure labels. Use words 
rather than symbols or abbreviations when writing Figure axis labels to 
avoid confusing the reader. As an example, write the quantity 
``Magnetization'', or ``Magnetization, M'', not just ``M''. If including 
units in the label, present them within parentheses. Do not label axes only 
with units. In the example, write ``Magnetization (A/m)'' or ``Magnetization 
\{A[m(1)]\}'', not just ``A/m''. Do not label axes with a ratio of 
quantities and units. For example, write ``Temperature (K)'', not 
``Temperature/K''.

\section*{Acknowledgment}

The preferred spelling of the word ``acknowledgment'' in America is without 
an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
acknowledgments in the unnumbered footnote on the first page.

\section*{References}

Please number citations consecutively within brackets \cite{b1}. The 
sentence punctuation follows the bracket \cite{b2}. Refer simply to the reference 
number, as in \cite{b3}---do not use ``Ref. \cite{b3}'' or ``reference \cite{b3}'' except at 
the beginning of a sentence: ``Reference \cite{b3} was the first $\ldots$''

Number footnotes separately in superscripts. Place the actual footnote at 
the bottom of the column in which it was cited. Do not put footnotes in the 
abstract or reference list. Use letters for table footnotes.

Unless there are six authors or more give all authors' names; do not use 
``et al.''. Papers that have not been published, even if they have been 
submitted for publication, should be cited as ``unpublished'' \cite{b4}. Papers 
that have been accepted for publication should be cited as ``in press'' \cite{b5}. 
Capitalize only the first word in a paper title, except for proper nouns and 
element symbols.

For papers published in translation journals, please give the English 
citation first, followed by the original foreign-language citation \cite{b6}.

\begin{thebibliography}{00}
\bibitem{b1} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil. Trans. Roy. Soc. London, vol. A247, pp. 529--551, April 1955.
\bibitem{b2} J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68--73.
\bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
\bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
\bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
\bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
\bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.
\end{thebibliography}
\vspace{12pt}
\color{red}
IEEE conference templates contain guidance text for composing and formatting conference papers. Please ensure that all template text is removed from your conference paper prior to submission to the conference. Failure to remove the template text from your paper may result in your paper not being published.

\end{document}